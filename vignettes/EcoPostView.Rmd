---
title: "EcoPostView: Ecological Posterior View"
author: Willem (Wim) Kaijser
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{"EcoPostView: Ecological Posterior View"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE,}
options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Introduction

Our ecological data is scattered around and noisy. It is often 'case-study' specific and not representative for the totality if conditions encountered. However, as ecologist you are not interested int case specific situations, but making probability statements on general patterns, visualizing them and predicting with them.

If working with meta-data and uncovering patterns from literature, making predictions and probability statements are your goal, this R-package can help you make your life easier. The goal of this package is for you to use the benefits of the output of simple fitted Linear and Generalized Linear Models ((G)LMs) over multiple studies to combine using the strength of Bayesian Meta-analysis and the flexibility it offers to generalize a pooled (G)LM from multiple (G)LM outputs. 

You need to find this information in figures, tables, data sets or combinations between them. In simple words we need a data set on which we can fit a (G)LM (see Kaijser et al, ...). This however, will show that most data is very noisy, perhaps biased and the heterogeneity is large. This is an issue for a focus on error-control and causality statements. Both need very clearly a controlled environment with a well setup experimental conditions and exclusion of confounding variables. Hence, error-control and causality start a-priori (Fisher 1949, Pearl 2009, Mayo 2018). Of course this makes or information still useful for make probability statements or using the required information as a-priori for an error-control or causality study. However, posterior probability statements, visualization and prediction are still within the range of all this information.

To start with the examples and using this R-package both JAGS and devtools need to be installed. JAGS can be installed from https://sourceforge.net/projects/mcmc-jags/ and devtools can be install directly into R. The EcoPostView can be directly installed from GitHub for the most recent version. Of course any problems or possible improvements can be directed to me if nicely asked.

```{r setup}
#install.packages("devtools")
library(devtools)
#install.github("snwikaij/EcoPostView")
library(EcoPostView)
```

# 2. Obtaining data and fitting (G)LMs

To obtain data from the (G)LMs we need to fit models. This data can be obtained from figures, tables, data sets or combinations (See Kaijser et al. ...). What we need from these models are the so called estimations of the model parameters. This are more commonly known as the intercept or slope. The intercept and slope can be used to predict: link(response variable) = intercept + slope * predictor variable. In a GLM the response variable is linked to to the linear component with a link-function. The 'identity' link function applies not link function it means that the mean (expected value) is directly related to the linear component. However in a GLM with, log- or logit-link it is easier to talk about log-linear or logit-linear relation. The slope is not a slope anymore because there is not straight line anymore between. Therefore the term coefficient or regression coefficient refers to this model parameter. When fitting a model it is convenient to note the source (e.g., DOI), the type of predictor variable (e.g., conductivity), response type (e.g., benthic invertebrates), link-function and if the model parameters is the intercept b0 or a regression coefficient b1. If multiple predictor variables are fitted in a model all regression coefficients.

Heuristically, I prefer working often working with elasticity- or semi-elasticity coefficients (Woolridge, 2001). But there are enough reasons not to do so. The elasticity coefficient expresses the percentage change of y (response) as relative to x (predictor variable). Thus, 0.2 is 0.2% change in y given 1% in x. Hence, for a log-linear model Log(y)=b0+b1·log(x) and thus b1=Log(x)/Log(x). For the semi-elasticity coefficient (i.e., logit-linear) this only accounts partially and values closer 0 are better interpret able because Logit(y)=b0+b1·log(x) and thus b1=Logit(y)/log(x). This expressed the  change in the log-odds per 1% elasticity (Cramer 1991; Woolridge, 2001). This makes it possible to coarsely compare among different models and still keeps the interpretation of connected to the units needed for prediction. Hence, we can still predict with the model and interpret the expected change of y given an increase in 1 log(x).

```{r data}
data(example1)
head(example1)

```
In the example above the column est (estimate) indicates the model parameters and the column se the (standard error of the estimate). The column group can be an organism group or specific species (or anything you wish to group by), the column predictor the specific predictor, the parameter whether the estimate is the intercept (b0) or a regression coefficient (b1) and the column link  contains the link function. An additional an recommended option is to include the sample size "n" for adjusting for 'small-sample-effect' (Peters et al., 2006; Moreno et al., 2009).

# 3. The meta-function

## 3.1 Basic function

The meta function can include a random effect setting the argument RE=T (default), It has the option of placing a single or multipe random effect as a vector or matrix using the argument Random. It can adjust for the relation between the SE and model parameters using the inverse of the standard error 1/SE (method=1, Stanley and Doucouliagos, 2014) or inverse of the sample size 1/n (method=2, the latter option is performed below). Ofcourse if bias is considered minimal and neglect able non can be performed (method=1).

```{r meta1}
mod1 <- meta(estimate=example1$est,         #Model estimate
             stderr=example1$se,            #Standard error of the model estimate
             parameter=example1$parameter,  #Model parameter (b0 or b1)
             predictor=example1$predictor,  #Predictor variable  (independent variable)
             link_function=example1$link,   #Link function
             grouping=example1$group,       #Group
             Nsamp=example1$n,              #Sample size
             method=2)                      #Adjustment method (0=none, 1=Egger's (1/se), 2=Peters (1/n))

#remove model to keep the environment clean
rm(mod1)

```

The meta-function returns a warnings that the MCMC-chains are not properly mixing. This can be an issue due to various reasons and assessed by looking at the 'raw' JAGS model output. I will not display the full output, but can be displayed wit the code below.
```{r meta1 warning}
#mod1$model$JAGS_model
```
We can see that for "mu[3]" the effective sample size is 610. Most of these issues can be resolved by thinning the chains or increasing the number of chains. In this case n_chain=6 solves the issue. Increasing the number of chains can be an issue if the number of available cores is limited as each chain is run on a separate core. Other options are to set 'better' or 'stronger' priors or combinations between them. Moreover, if the issue is not an issue of the estimate 'mu' parameters in the raw JAGS output, the issue could be considered to be ignored. These choices are ultimately up to the user. An option to prevent warnings would be to set the warning level for Eff_warn lower i.e., Eff_warn = 500.
```{r meta2}
mod2 <-meta(estimate=example1$est,        
            stderr=example1$se,            
            parameter=example1$parameter,  
            predictor=example1$predictor,  
            link_function=example1$link,   
            grouping=example1$group,       
            Nsamp=example1$n,              
            method=2,
            n_chain = 6)                   #Increasing the number of chains from 2 to 6   

#remove model to keep the environment clean
rm(mod2)

```

## 3.2 Setting priors
A benefit of the the Bayesian approach is that we can included prior information. Thereby explicitly adding more weight to more plausible outcomes of the model parameter or less weight to less plausible outcomes. To set only single priors for each relation and parameter a specific structure is needed. By default the model parameters are set with a mean (prior_mu) of 0 and se (prior_mu_se) of 0.5. The prior for the standard deviation is uniform where only the maximum value (prior_sigma_max) can be set which is at default 5. As a strongly critical side note. The priors do not need to stay the same, because I 'heuristically' like to work with the elasticity- or semi-elasticity coefficients. THIS IS NOT A NECESSITY. Hence, not thinking about your prior even if not a lot of information is available to make an very strong prior (but at least informed) is often not doing a `Bayesian analysis` at all. 

A column with the 'level' names, priors for the mean (mu) and priors for the standard error (se). This structure can be show by setting the argument get_prior_only=T. On could create a table with the same names and order and solely changing mu to i.e., 0.2 or -0.2.

``` {r meta2 example priors}
only_priors <- meta(estimate=example1$est,        
            stderr=example1$se,            
            parameter=example1$parameter,  
            predictor=example1$predictor,  
            link_function=example1$link,   
            grouping=example1$group,       
            Nsamp=example1$n,            
            method=2,
            n_chain = 6,
            get_prior_only=TRUE) #Only show the structure of the priors

print(only_priors)

#remove data frame of priors to keep environment clean
rm(only_priors)
```

## 3.3 Setting multiple priors for Bayesian Model Averaging

A benefit of the the Bayesian approach is that we can included multiple priors to perform Bayesian Model Averaging (BMA;  Hoeting et al., 1999; Hinne et al., 2020). Thereby explicitly adding multiple possible scenarios that could have generated the observable data and aggregating this possibilities. To set multiple priors we need a similar dataset but also include prior weights. A simple solution to the prior weights to weight each prior as equally reasonable. In the example below there are three priors for each possible model. An the prior weights are set as 1/3.

The results are summarized in a table

``` {r meta3 BMA}
data("example2")
print(example2)

mod3 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu = example2[c(2,4,6)],          #Prior for the mean
                    prior_mu_se = example2[c(3,5,7)],       #Prior for the standard error of the mean
                    prior_weights = c(1/3, 1/3, 1/3),       #Prior weights
                    Nsamp=example1$n,            
                    method=2,
                    n_chain = 6)

#Display the summarized results
mod3$Summary

#remove data frame of priors to keep environment clean
rm(mod3)
```
This is the most difficult part where the data needs to be gathered, priors need to formulated and the model needs to be optimized to get a stable chains. The display of the results requires less time.

# References

Cramer, J. S. 1991. The Logit Model: An Introduction for Economists. London: Edward Arnold.

Hinne, Max, Quentin F. Gronau, Don Van Den Bergh, and Eric-Jan Wagenmakers. 2020. “A Conceptual Introduction to Bayesian Model Averaging.” Advances in Methods and Practices in Psychological Science 3(2):200–215. doi: 10.1177/2515245919898657.

Hoeting, Jennifer A., David Madigan, Adrian E. Raftery, and Chris T. Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science 14(4):382–417. doi: 10.1214/ss/1009212519.

Moreno, Santiago G., Alex J. Sutton, Ae Ades, Tom D. Stanley, Keith R. Abrams, Jaime L. Peters, and Nicola J. Cooper. 2009. “Assessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study.” BMC Medical Research Methodology 9(1):2. doi: 10.1186/1471-2288-9-2.

Peters, Jaime L., Alex J. Sutton, David R. Sones, Keith R. Abrams, and Lesley Rushton. 2006. “Comparison of Two Methods to Detect Publication Bias in Meta-Analysis.” JAMA 295(6):676. doi: 10.1001/jama.295.6.676.

Stanley, T. D., and Hristos Doucouliagos. 2014. “Meta-Regression Approximations to Reduce Publication Selection Bias.” Research Synthesis Methods 5(1):60–78. doi: 10.1002/jrsm.1095.

Woolridge, Jeffery M. 2001. Econometric Analysis of Cross Section and Panel Data. Cambridge, Massachusetts, London, England: The MIT press.


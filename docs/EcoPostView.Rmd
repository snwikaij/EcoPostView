---
title: "EcoPostView: Ecological Posterior View"
author: Willem (Wim) Kaijser
output: 
  rmarkdown::html_vignette:
    toc: true      # Enable table of contents
    toc_depth: 2   # Define the depth of the table of contents
vignette: >
  %\VignetteIndexEntry{"EcoPostView: Ecological Posterior View"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE,}
options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Introduction

Ecological data is scattered around literature in different formats and very noisy. It is often 'case-study' specific and not representative for the totality if conditions encountered. However, as ecologist you are not interested in case specific situations, but making probability statements on general patterns, visualizing and predicting with them.

If working with meta-data and uncovering patterns from literature, making predictions and probability statements are your goal, this R-package can help make your life easier. The goal of this package is for you combine the benefits of multiple fitted Linear and Generalized Linear Models ((G)LMs), using the flexibility and strength of Bayesian Meta-analysis to generalize to a pooled (G)LM. For the differences differences, strengths and weaknesses of both the classical (Frequent-ism) or Bayesian approach see https://snwikaij.shinyapps.io/shiny/.

The information you are interested in is available from figures, tables, data sets or combinations between them. In simple words, we need a data set on which we can fit a (G)LM (see Kaijser et al, ...). This, however, will show that most data is very noisy, perhaps biased and the heterogeneity amount studies is large. This is an issue for a focus on error-control and causality statements. Both need very clearly a controlled environment with a well setup experimental conditions and exclusion of confounding variables. Hence, error-control and causality start a-priori (Fisher 1949, Pearl 2009, Mayo 2018). Of course this makes or information still useful to make probability statements or using the information to a-priori setup an study focused on error-control or causality study. However, the focus lies here on making posterior probability statements, visualization and prediction possible this information.

To start usgin the R-package both JAGS and devtools need to be installed. JAGS can be installed from https://sourceforge.net/projects/mcmc-jags/ and devtools can be install in R. For the most recent version of the EcoPostView it can be installed from GitHub. Of course, if nicely asked, any problems or possible improvements can be directed to me.

```{r setup}
#install.packages("devtools")
library(devtools)

#install.github("snwikaij/EcoPostView")
library(EcoPostView)
```

# 2. Obtaining data and fitting (G)LMs

To obtain data from the (G)LMs we need to fit models. This data can be obtained from figures (e.g., using WebPlotDigitizer), tables, data sets or combinations (See Kaijser et al. ...). What is needed to combine these models are the so called estimations of the model parameters. These are more commonly known as the intercept or slope. The intercept (*b0* or *β0*) and slope (*b1* or *β1*) can be used to predict: *link(response variable) = b0 + b1 · predictor variable*. In a GLM the response variable is linked to to the linear component with a link-function often notated as *g(..)*. The 'identity' link function applies no link function. This indicate that the mean (expected value notated as *E(y)* or *(E(y|x))*) is directly related to the linear component. However, in a GLM with, log- or logit-link it is easier to talk about log-linear relation 
$$log(E(y \mid x)) = \beta_0 + \beta_1 \cdot x$$
or logit-linear relation
$$logit(E(y \mid x)) = \beta_0 + \beta_1 \cdot x$$
The slope is not a true slope anymore because there is not straight line anymore between the response and predictor variable. Therefore the term coefficient or regression coefficient refers to the model parameter (*β*). When fitting a model it is convenient to note the source (e.g., DOI), the type of predictor variable (e.g., conductivity), group of the response type (e.g., benthic-invertebrates), link-function and if the model parameters is the intercept *b0* or a regression coefficient *b1*. If multiple predictor variables are fitted in a model all regression coefficients as notated as *b1* to distinguish it from the intercept *b0*.

Heuristically, I prefer working often working with elasticity- or semi-elasticity coefficients (Woolridge, 2001). But there are enough reasons not to do so and is dependent on the question and many other factors. The elasticity coefficient expresses the percentage change of $y$ (response) as relative to $x$ (predictor variable). Thus, 0.2 is 0.2% change in $y$ given 1% in $x$. Hence, for a log-linear model $log(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$
and thus $\beta_1 = \frac{\log(y)}{\log(x)}$. For the semi-elasticity coefficient (i.e., logit-linear) this only accounts partially and values closer 0 are better interpret able because $logit(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$ and thus $\beta_1 = \frac{logit(y)}{\log(x)}$. This expressed the  change in the log-odds per 1% elasticity (Cramer 1991; Woolridge, 2001). This makes it possible to coarsely compare among different models and still keep the interpretation of to the units needed for prediction. Hence, we can still predict with the model and interpret the expected change of $y$ given an increase in $1·log(x)$, because the units of $x$ are retain under the $log$.

I will keep these 'unofficial expression', due to the reference in the R-package to *b0* and *b1* while a proper notation would of course be more complex expressed as: 
$$g(E(y \mid x_{ij})) = \sum_{j=1}^{v} \beta_j \cdot x_{ij}$$
Where $x_{ij}$ refers to the $j$ the predictor variable (e.g., salinity is $j$=1 and light is $j$=2) and $i$ is the $i$-th observation. Here, $v$ represents the number of predictors. I will reduce the text to the basics.

```{r data}
data(example1)
head(example1)

```
In the example above the column **est** (estimate) indicates the model parameters and the column **se** the (standard error of the estimate). The column **group** can be an organism group or specific species (or anything you wish to group by), the column **predictor** the specific predictor variable, the parameter whether the estimate is the intercept (*b0*) or a regression coefficient (*b1*) and the column **link*** contains the link function. An additional and recommended option is to include the sample size **n** for adjusting for 'small-sample-effect' (Peters et al., 2006; Moreno et al., 2009).

# 3. The meta-function

## 3.1 Basic function

The meta function can include a random effect, by setting the argument RE=TRUE (default is TRUE). The  structure is then $\beta= \beta_{\text{pooled}} + u_i$. It has the option of placing a single or multiple random effect as a vector or matrix using the argument "random" the structure then becomes then $\beta = \beta_{\text{pooled}} + u_i +r_i$ for a single random effect. It can adjust for the relation between the $se$ and model parameters using the inverse of the standard error $1/se$ (method=1, Stanley and Doucouliagos, 2014). The structure is then $\beta = \beta_{\text{pooled}} + u_i + \alpha \cdot \left(\frac{1}{se}\right)$ or inverse of the sample size $1/n$ (method=2, the latter option is performed below) with the structure $\beta = \beta_{\text{pooled}} + u_i + \alpha \cdot \left(\frac{1}{n}\right)$. Of course if bias is considered minimal and neglect able non can be performed (method=1). I still would like to include a third 4th option to utilize Robust Bayesian Model Averaging (RoBMA: Maier et al. 2023). But this sometimes adjust extremely when including $1/se$ and therefore I left this option open for now.

```{r meta1}
mod1 <- meta(estimate=example1$est,         #Model estimate
             stderr=example1$se,            #Standard error of the model estimate
             parameter=example1$parameter,  #Model parameter (b0 or b1)
             predictor=example1$predictor,  #Predictor variable  (independent variable)
             link_function=example1$link,   #Link function
             grouping=example1$group,       #Group
             Nsamp=example1$n,              #Sample size
             method=2)                      #Adjustment method (0=none, 1=Egger's (1/se), 2=Peters (1/n))

#remove model to keep the environment clean
rm(mod1)

```

The meta-function can returns a warnings that the MCMC-chains are not properly mixing. This can be an issue due to various reasons and assessed by looking at the 'raw' JAGS model output (mod1$model$JAGS_model). This could show that a parameter of interested "mu[.]" Has a a large Rhat or small effective sample size. Most of these issues can be resolved by thinning the chains or increasing the number of chains or setting a "better informed" or "stronger" prior. Moreover, if the issue is not an issue of the estimated "mu" parameter, the issue could be chosen to be ignored. These choices are ultimately up to the user. An option to prevent warnings would be to set the warning level for Eff_warn lower i.e., Eff_warn = 500.

## 3.2 Setting priors
A benefit of the the Bayesian approach is that we can included prior information. Thereby explicitly shifting the weight of the outcome to more plausible values of the pooled model parameter or less weight to less plausible values. To set only a single prior for each relation and parameter a specific structure is needed. By default the model parameters are set as Gaussian/normally distributed with a mean ($\mu$, prior_mu) of 0 and standard error ($se$, prior_mu_se) of 0.5. The prior for the standard deviation ($\sigma$) is uniform where only the maximum value (prior_sigma_max) can be set, which at default is 5. As a strongly critical side note. The priors do not need to stay the same, because I "heuristically" like to work with the elasticity- or semi-elasticity coefficients. THIS IS NOT A NECESSITY. Hence, not thinking about your prior even if not a lot of information is available to make an very strong prior (but at least informed) is often not doing a Bayesian analysis at all. 

To get a nice table overview which can be adjusted an filled in with prior information the argument 'get_prior_only' can be set to 'TRUE'. This will return a dataframe, with a column **level** names, priors for the **mu** ($\mu$) and for the standard error ($se$). The values in this can be set manually and re-included as a vector. How to do this will be explained in more detail in section 3.3.

``` {r meta2 example priors}
only_priors <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,       
                    Nsamp=example1$n,            
                    method=2,
                    get_prior_only=TRUE) #Only show the structure of the priors

print(only_priors)

#remove data frame of priors to keep environment clean
rm(only_priors)
```

## 3.3 Setting multiple priors for Bayesian Model Averaging

A benefit of the the Bayesian approach is that we can included multiple priors to perform Bayesian Model Averaging (BMA;  Hoeting et al., 1999; Hinne et al., 2020). Thereby explicitly adding multiple possible scenarios that could have generated the observable data and aggregating this possibilities. To set multiple priors we need a similar dataset but also include prior weights. A simple solution to the prior weights to weight each prior as equally reasonable. In the example below there are three priors for each possible model. An the prior weights are set as 1/3. Hence each column vector in example2 gets the weight 1/3. The priors have been set differently and a broader prior for the intercept that seems reasonable.

``` {r meta3 BMA}
data("example2")
print(example2)

mod2 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu=example2[c(2,4,6)],          #prior for the mean
                    prior_mu_se=example2[c(3,5,7)],       #prior for the standard error of the mean
                    prior_weights=c(1/3, 1/3, 1/3),       #prior weights
                    Nsamp=example1$n,            
                    method=2,
                    n_thin=10,                            #thinning the chains
                    n_chain=4)                            #changing the number of chains from 2 to 4

#Display the summarized results
mod2$Summary
```
The results of the meta-analys are summarized in a table describing Maximum A Priori values (MAP), the $\mu$, $se$ and the high density intervals at default (90%). Additionally the $I2$ heterogeneity is given.

This is the most difficult part where the data needs to be gathered and extracted, (G)LMs need to be fitted, priors need to formulated and the whole model needs to be optimized to get a stable results. The display of the results requires less time.

# 4. Display

## 4.1 pdplot-function
To display the posterior results one could display the results with a point and interval range. However, this interval displaying a strong boundary on a continues set of possibilities does not completely capture  the idea of the posterior probability distribution. Another option would be to display the Posterior Density Distribution (PDD) combined with the point and interval estimate. Hence, the PDD displays the possible values of the prior information where the pooled estimated might have originated from $P(\beta_{\text{pooled}} \mid \text{meta-data}, \text{ prior information})$ it is the inverse of the likelihood which informs us of the values of the meta-data given we elected a particular pooled estimate $P(\text{meta-data}\mid\beta_{\text{pooled}}  )$. This can be  performed with the pdplot function.

``` {r pdplot, fig.width=8, fig.height=4, echo=FALSE}
pdd <- pdplot(mod2, 
              label_size=4,      #setting the label size larger
              point_size=2)      #large point

#for the models with the log-link
print(pdd$posterior_density$log)             

#for the models with the logit-link
print(pdd$posterior_density$logit)

#summary belonging to the figures
print(pdd$summary)

```
for larger datasets with multiple groups and predictor variables the order of the PDDs can be changed by using the arguments "order_predictor" and "order_group" needing a character string with the names of the desired order.

## 4.2 hop-function
Hypothetical Outcome Plots are a useful method to visualize the possibilities the expected value could take on given a change in the predictor variable. These are so called HOPs (Kale et al. 2019). The hops lines are a simulation of possibilities from the posterior. Note that the function works under the idea of log transformed variables. This means that xlim (the limits of the figures) as given below are exp(3)=20 and exp(8)=2980 for conductivity. The y-axis is on the response scale. I hop soon to make it possible to use an argument that makes it possible to set the xlim on the raw. Below an example of the outcomes of the response of invertebrate taxonomic richness along  the conductivity gradient. 
```{r HOPs log salinity fig1, fig.width=6, fig.height=4, echo=FALSE}
log_sal1 <- hop(mod2,                                   #object from the meta function
    group="Invertebrates",                             #select group to out of Invertebrates and Fish
    predictor = "Salinity",                            #select group to out of Salinity and Oxygen
    xlab= expression(Ln(Conductivity ~ µS ~ cm^-1)),   #give x-axis a name
    link_function = "log",                             #which link function out of log and logit
    ylab="Invertebrate taxonomic richness",            #give y-axis a name
    xlim=c(3, 8),                                      #give y-axis a name
    ylim=c(0, 25),                                     #set  limits y-axis
    hop_lwd = 0.3)                                     #set width of the hop lines
    
print(log_sal1)
```
It is also possible to scale to display the exponentiate values on the x-axis using the argument
"exp_axis".
```{r HOPs log salinity fig2, fig.width=6, fig.height=4, echo=FALSE}
log_sal2 <- hop(mod2,                                   
    group="Invertebrates",                             
    predictor = "Salinity",                            
    xlab= expression(Ln(Conductivity ~ µS ~ cm^-1)),   
    link_function = "log",                            
    ylab="Invertebrate taxonomic richness",            
    xlim=c(3, 8),                                      
    ylim=c(0, 25),                                     
    hop_lwd = 0.3,                                    
    exp_axis = T,                                     #exponentiate the x-axis notations
    round_x_axis = 0)                                 #round the notation to full integers                                 
print(log_sal2)
```

## 4.3 Parital Dependency Plots (hop-function)

# References

Cramer, J. S. 1991. The Logit Model: An Introduction for Economists. London: Edward Arnold.

Hinne, Max, Quentin F. Gronau, Don Van Den Bergh, and Eric-Jan Wagenmakers. 2020. “A Conceptual Introduction to Bayesian Model Averaging.” Advances in Methods and Practices in Psychological Science 3(2):200–215. doi: 10.1177/2515245919898657.

Hoeting, Jennifer A., David Madigan, Adrian E. Raftery, and Chris T. Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science 14(4):382–417. doi: 10.1214/ss/1009212519.

Kale, Alex, Francis Nguyen, Matthew Kay, and Jessica Hullman. 2019. “Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data.” IEEE Transactions on Visualization and Computer Graphics 25(1): 892–902. doi: 10.1109/TVCG.2018.2864909.

Maier, Maximilian, František Bartoš, and Eric-Jan Wagenmakers. 2023. “Robust Bayesian Meta-Analysis: Addressing Publication Bias with Model-Averaging.” Psychological Methods 28(1): 107–22. doi: 10.1037/met0000405.

Moreno, Santiago G., Alex J. Sutton, Ae Ades, Tom D. Stanley, Keith R. Abrams, Jaime L. Peters, and Nicola J. Cooper. 2009. “Assessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study.” BMC Medical Research Methodology 9(1):2. doi: 10.1186/1471-2288-9-2.

Peters, Jaime L., Alex J. Sutton, David R. Sones, Keith R. Abrams, and Lesley Rushton. 2006. “Comparison of Two Methods to Detect Publication Bias in Meta-Analysis.” JAMA 295(6):676. doi: 10.1001/jama.295.6.676.

Stanley, T. D., and Hristos Doucouliagos. 2014. “Meta-Regression Approximations to Reduce Publication Selection Bias.” Research Synthesis Methods 5(1):60–78. doi: 10.1002/jrsm.1095.

Woolridge, Jeffery M. 2001. Econometric Analysis of Cross Section and Panel Data. Cambridge, Massachusetts, London, England: The MIT press.


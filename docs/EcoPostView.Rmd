---
title: "EcoPostView: Ecological Posterior View"
author: Willem (Wim) Kaijser
output: 
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true 
    highlight: tango
    css: style.css
---

# Overview

This site reads like an incomplete book or essay. It serves as a way to organize my thoughts and store information. The project initially began around the EcoPostView R package but has since expanded into multiple chapters covering broader ideas.

On the left, you will find these chapters in the drop down menu. They start with incomplete explorations of reasoning, and  Statistics (and analytical and emperical Bayes), and end with a chapter focused on the EcoPostView R package. While this structure reflects a logical progression ‚Äî from abstract foundations to practical application ‚Äî many readers will find it more accessible to jump straight into using the R functions. That is entirely reasonable, and the material is designed to support both entry points.

In hindsight, this structure is in the correct order, as I believe all principles of what we *believe* about the natural world and ecology can ultimately be traced back to metaphysics and language. Reasoning and logic are not tools to find *truth*, but to try to contain and entail it in our propositions ‚Äî perhaps not by assuming the propositions are *true*, but by entertaining them as being *true*.

Ideally, one would begin with reasoning and logic, use that foundation to frame the statistical concepts of analytical and empirical Bayes, and only then proceed to practical application. But exploring these paths in reverse ‚Äî from application back to one's *beliefs* about *reality* and *being* - might even be more beneficial. These *themes* may seem like existential, rather than scientific, questions, but the boundary between the two remains debated and unresolved. 

I hope the structure is not too confusing if you prefer to go directly to the EcoPostView package. This site remains a work in progress ‚Äî a continuously evolving project, far from complete.

# Reasoning

## Introduction

Following the full path from reasoning and logic to application and *belief*, one may recognize that many ‚Äî if not most ‚Äî of our *beliefs* are not *justified* under particular lines of reasoning. This leads to deeper questions (1.) How do we *justify* our *beliefs*? (2.) Which propositions should we *believe* (3.) Why do we try to *justify* them? And perhaps more drastic (4.) How should we behave in light of this information?

It is argued that to some extend reasoning is based on the principles of logic. Although some argue otherwise (e.g., Harman, 1986). I will use formal expressions that are not all at full first-order-language and predicate logic, but mostly include some form of meta-language.
For this I will use the basic principle of deductive logic based. I believe they are useful to lay out underlying believes and their connection between them. 

To understand arguments, validity, soundness, cogency and the formalization of arguments and introduction to logic is needed. Although this will not be an full introduction into logic it will address some principles of it and for a more formal and extended introduction see (Barwise et al. 2002; Lee 2017; Smith 2021). Therefore I will start with an introduction to classical syllogism.

## Syllogism

A simple syllogistic argument consisting of two premises.

$$
Major\ premise \ 1) \ all \ insects \ have \ six \ legs,\\
Minor \ premise \ 2) \ x \ is \ an \ insect,\\
Conclusion) \ therefore \ x \ has \ six \ legs
$$
The argument above is called "modus ponens", which follows a specific logical structure. In this form, the conclusion is entailed by the premises ‚Äî that is, the truth of the premises guarantees the truth of the conclusion. Entailment means that in all cases where the premises are true, the conclusion cannot be false. If an argument has this feature, it is considered logically valid.

This argument can be further condensed by abstracting away the content and focusing only on its structure. In this way, the validity of the argument depends solely on the form of the entailment, not on the specific subject matter.

**Modens Ponens** 
$$
Major\ premise \ 1) \ all \ P \ are \ Q,\\
Minor \ premise \ 2) \ x \ is \ P,\\
Conclusion) \ therefore \ x \ is \ Q
$$
In an argument like the one above, all premises are declarative statements, also known as propositions ‚Äî sentences that are either True or False, but not both. Each premise typically consists of a subject (or antecedent), a copula (such as is, are, will be, etc.), and a predicate (or consequent). These components express a logical relationship between concepts.

An argument is said to be **valid** if, assuming all the premises are true, the conclusion cannot be false. In other words, the conclusion is logically entailed by the premises.
An argument is **sound** if it is both valid and all of its premises are in factually true. Therefore, soundness implies both logical structure and factual accuracy. 

For example: P1: All unicorns fly when eating carrots, P2: my unicorn eat a carrot, C: therefore my unicorn will fly. This is a valid argument because if the premises were true, the conclusion would necessarily follow. However, it is not sound because unicorns do not exist (as far as I know), so No consider the following argument: P1: most grass is green P2: the sky is blue C: you are reading this text. Even if all three propositions happen to be true, this argument is not valid because the conclusion is not entailed by the premises ‚Äî there is no logical connection between them. An argument where the conclusion does not follow from the premises is considered invalid, regardless of whether its statements are true.

In propositional logic, the argument as given before can be formally expressed as below. In this formal expression $\rightarrow$ indicates and implication (as "implies") and the three dots $\therefore$ indicates therefore. Similar $\vee$ means "or", $\wedge$ means "and" and $\neg$ is the negation symbol. Thus for example $\neg P$ means not-P. Other operators include $\vee$ indicates "or" (disjunction) and $\wedge$ indicates "and" (conjunction). Using these basic operators, many more valid argument forms can be constructed‚Äîsuch as disjunctive syllogism, hypothetical syllogism, modus tollens, and more. These forms serve as the foundation for formal reasoning in logic.
$$P \rightarrow Q, P \therefore Q$$

**Modens Tollens**
$$P \rightarrow Q, \neg Q \therefore \neg P$$

**Hypothetical Syllogism**
$$P \rightarrow Q, Q \rightarrow Z, P \therefore Z $$

**Disjunctive Syllogism**
$$Either \ P \vee Q, \neg P \therefore Q$$

**Semantic equivalent structures**
$$
\text{Modens Ponens:} for \ all \ x \ that \ are \ P \ it \ implies \ they \ are \ Q, x \ is \ P \ therefore \ x \ is \ Q\\
\text{Modens Tollens:} for \ all \ x \ that \ are \ P \ it \ implies \ they \ are \ Q, x \ is \ \neg Q \ therefore \ x \ is \ \neg P \\
\text{Hypothetical Syllogism:} for \ all \ x \ that \ are \ P \ it \ implies \ they \ are \ Q, for \ all \ x \ that \ are \ Q \ it \ implies \ they \ are \ Z, \ x \ is \ P \ therefore \ x \ is \ Z \\
\text{Disjunctive Syllogism:} either \ x \ is \ P \ or \ x \ is \ Q, x \ is \neg P \ therefore \ x \ is \ Q
$$

## Semantical validity of an argument

An valid argument - in the semantic interpretation of validity - in case $ùìü_n$ $ùìü_i$ is true in all interpretations in which $ùìü_1, ..., ùìü_{n-1} ,ùìü_n$ are true. Simplistically, if all propositions are true the conclusion cannot be false. Such a valid argument can be notated as
$$ùìü_1, ‚Ä¶, ùìü_{n-1} \models ùìü_n$$ (see also Haack, 1978)

## Predicate logic 

In predicate logic modens ponens is expressed as $\forall(P(x)\rightarrow Q(x))$ where ‚Äò$\forall$‚Äô indicates ‚Äòfor all‚Äô and ‚Äò$\models$‚Äô therefore. Each proposition referred to as $ùìü_i$ consisting of P and Q called properties and x and element from a set $x\in X$. The benefit of predicate logic (or approximating languages such as deontic-logic or a meta-language)is its possibility to quantify over different elements form a set $x \in X$ can be assessed.

**Predicated equivalent structures**
$$
\text{Modens Ponens:} \forall x(P(x)\rightarrow Q(x)), P(x) \vdash Q(x) \\
\text{Modens Tollens:} \forall x(P(x)\rightarrow Q(x)), \neg Q(x) \vdash \neg P(x) \\
\text{Hypothetical Syllogism:} \forall x(P(x)\rightarrow Q(x)) \wedge \forall x(Q(x) \rightarrow Z(x)), P(x) \rightarrow Z(x) \\ 
\text{Disjunctive Syllogism:} P(x) \vee Q(x), \neg P(x) \vdash Q(x) \\
$$

## Deduction, induction and abuction

Deductive reasoning moves from general principles to specific conclusions. It begins with premises that are assumed to be universally true and applies them to individual cases. If the premises are true and the reasoning is valid, the conclusion cannot be false. For example: P1: All humans are mortal, P2: I am a human C: Therefore I am mortal. 

In contrast, inductive reasoning moves from specific observations to broader generalizations. It attempts to infer a universal rule or principle from limited data, and thus does not guarantee the truth of the conclusion ‚Äî even if all observed instances support it. Induction tries to generals from the instances (x) we observe to all.

If I observe twenty white swans therefore all swans are white.

This reasoning is not valid. The observation of one swan with any other colour would negate the whole argument. This issue is known as the problem of induction. Hence, how can I justify my conclusion to be universally true based on a limited set of instances? This means inductive arguments might not be valid or sound.

## Cogency of arguments

In empirical sciences the universal quantifier ‚Äò$\forall$‚Äô (largely?) unsupported and therefore $\exists x(P(x) \rightarrow Q(x))$ where ‚Äò$\exists$‚Äô some. This indicates that propositions are merely acceptable. And in most cases ‚àÉ can be ignored and as it would be wises to explicitly assume it. Meaning that $ùìü_i: (P(x)\rightarrow Q(x))$ or $P \rightarrow Q$ and referred to as a conjecture. A conjecture is a proposition which is accepted as true via a valuation function $v(ùìü_i)=T$ (True) or false $v(ùìü_i)=F$ (False). 

Empirical science developing theories built upon conjectures. Where a conjecture has to be supported by either party in an argument. If a conjecture cannot  be accepted by one of the parties, discussion is meaningless and the source of the rejection of one of the parties should be resolved (Popper 1968). 

Arguments are either weakly or strongly cogent (weak or strong arguments) as the conjectures cannot be guaranteed in empirical sciences. Strong arguments are then arguments where most conjectures are acceptable (e.g., temperature is 0 degrees when the  thermometer works and indicates 0 degrees).

The arguments proposed are indicated as $ùíú$. And $ùíú: ùìü_1, ‚Ä¶, ùìü_{n-1} ,ùìü_n \models ùìü_n$ should entail the truth of the conjectures within the derived conclusion. And when this is the cases such an argument is then valid $valid(ùíú)$ when all parties agree on the truthood $v(ùìü_i)=T$ (True) of the conjectures involved.

The problem is that the empirical sciences has no axiomatic foundation from which the proof of an argument can be logically deduced. In the sense that I believe the the conclusion ($ùìü_3$) of the following argument. $ùìü_1$: All humans die, $ùìü_2$: I am human, conclusion ($ùìü_1 \therefore  ùìü_2$), therefore I die ($ùìü_1‚Üíùìü_2, ùìü_1 \therefore ùìü_2$). The conclusion forming a conjecture in my belief system and that of others. However, it does not ensure the soundness of the argument, where soundness refers to the factuality of all propositions. Yet, having not known all humans ever lived or going to live the argument is not sound. Moreover, it is questionable if reasoning deals with valid conclusions. Where one better can speak about cogency of the argument.

The agreement among parties - rather informally - forming a belief system that would be a function of acceptable (cogent) conjectures (Us√≥-Dom√©nech and Nescolarde-Selva 2016). The conjectures tied together for each individual moral agent (Carthwright et al. 1996; van Fraassen 1980, Closely representing the nodes and edges in a network). 

The moral agent then entertains the relations between propositions and arguments. Counter intuitively, the moral agents also appoints a property to an element x. Hence, a colour blind person would not see the specific colours a person that is not color blind would see, therefore not appoint the same colour property. Or, if a person does not know the function of a wrench, and therefore does not appoint the property wrench to it. If a property is not appointed to an element then the cogency (or validity) of an argument does not follow because without a property on x, the element x remains inert and undefined. Therefore,$v(ùìü_i)=T$ or $v(ùìü_i)=F$ remains undecided because $(x \rightarrow Q(x)), x \models Q(x)$ does not follow. Hence, each element is appointed a property, each proposition is valued, woven into an a cogent or not cogent (strong or weak) and formulated as a belief system.

## Skepticism

Understanding skepticism made most parts of why I am dissatisfied with most argument fall in its place. It perhaps plays a central but indirect part in this chapter. I will try to (meta-logical) formalize the Agrippa's trilemma and other skeptical "tools" that should lead to the realization that most of our justifications are not as strong and each of them could be undermined. 

Any conclusion $ùìü_n$ under some condition $c \in C$ in an argument $ùìü_1, ‚Ä¶, ùìü_{n-1} \models ùìü_n$. Can be suspended based on any other premise that is not certain/conclusive and therefore $ùìü_n$ does not follow. 

More formally, for any claim from a subject ($s \in S$) there exists at least some propositions of disagreement over the claim under particular conditions so that the disagreement can be exploited in the mode of hypothesis (MH), Mode of Circularity (MC) and mode of infinite regress (MI).
$$
\forall s \forall ùìü \exists (Claim(s, ùìü_n) \rightarrow Disagreement(Claim(s, ùìü_n), c) \rightarrow \{MH \vee MC \vee MI\}
\\
MH: Pr(ùìü_n, s)/Pr(\negùìü_n, s) \neq \{0, \infty\} \rightarrow \neg Valid(ùìü_n) = Undecided
\\
MC: Valid(s, ùìü_1, ‚Ä¶, ùìü_{n-1} \models ùìü_n) \ ùìü_n \in \{ ùìü_1, ‚Ä¶, ùìü_{n-1} \}) \rightarrow \neg Valid(ùìü_n) = Circulair
\\
MI: \forallùìü_m \existsùìü_n (Valid(s, ùìü_m \models ùìü_n) \wedge ùìü_m <  ùìü_n ) \rightarrow \neg Valid(ùìü_m)=  Non-foundational 
$$

# Statistics (analytical and emperical Bayes)

In most cases the theory and background is often not provided in many cookbooks, but makes it impossible to interpreted, criticize our results. This section introduces some theory. However, if you are already familiar with it or find it too technical, feel free to skip it. Since this R-package primarily uses Bayesian inference, an introduction to Bayes Theorem and methods used may be of interest. While it is not necessary needed for the utilization of the R-package itself.

## Classical statistics and estimation

All statistics focuses on estimating the parameter of interest $\theta$ which in most (G)LMs is denoted as the parameter $\mu$ or $\beta$. For consistency I will use the parameter of interest $\mu$. The population parameter of interest $\mu$ is fixed and unknown. To investigate potential values for $\mu$ we take measurements/samples. The observed measurements or samples ($x$) are realizations of an unknown random variable $X$, where both x and X represents sets $x = \{x_i, \dots, x_n \mid x \in \mathbb{R}\}$ and $X = \{X_i, \dots, X_N \mid X \in \mathbb{R}\}$. Here, $x$ is a realization and $X$ denotes the random variable. It is then assumed that $x$ is originating as randomly identically and independent distributed random variable (iid) from X as in $x\sim X$ or $x \stackrel{\text{iid}}{\sim} N(\mu, \sigma^2)$. Since we do not have $X$ but only a set of realizations we need an estimator for it which it the sample mean $\hat{x}(x)$. Hence, the mean of the random variable $X$ would be $\mu(X)=\frac{X_i,\cdots,X_N}{N}$ and if x are indeed iid $\hat{x}(x)=\frac{x_i,\cdots,x_n}{n}$ would be an estimator for $\mu$. Therefore, the sample mean has certain properties $\mathbb{E}[\hat{x}]$ and $Var(\hat{x})=\frac{\sigma^2}{n}$. Then the weak law of large numbers suggest that the probability of deviation from the population parameter decreases when sample size $n$ increases $\lim_{n\to\infty} P\left(|\hat{X}_n-\mu|\geq \epsilon\right)=0$. This means that according to the central limit theorem $Z_n=\frac{\hat{x}-mu}{\sigma/\sqrt(n)}$ converges to the standard normal under $mu=0$. The probability of observing $Z$ under a long-run of repetitions $P(-1.96\lesssim Z \lesssim 1.96)=0.95$. Similair, the probability of the intervals of $\overline{x}$ to cover $\mu$ in a long-run of repeated experiments at 95% is $1  - c = P(\overline{x} > \mu - 1.96 \cdot \frac{\sigma}{\sqrt{n}}) ~and~  P(\overline{x} < \mu - 1.96 \cdot \frac{\sigma}{\sqrt{n}})$. I visually explain this in this shiny app https://snwikaij.shinyapps.io/shiny/. Furthermore, it is clear that statistics does do nothing with causality in the first place and the focus on error-control and causality starts by satisfying theoretical conditions needed the arrive at believes in these concepts. Hence, error-control and causality start a-priori (Fisher 1949, Pearl 2009, Mayo 2018). Yet, such a focus and framework would, for example be extremely useful if objectivity over repetitions are the goal and favorable. While classical statistics focuses on fixed but unknown parameters, error-control and objectivity of information, Bayesian methods extend this perspective by introducing prior information and viewing parameters as random variables. This shift opens the door to more flexible and informative inference, as explained in the next section.

## Bayes Theorem and probablistic estimation

### Bayes Theorem

Informally Bayes Theorem would be notated $\text{Posterior}\, \text{probability} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Evidence}}$. More formally Bayes theorem is often notated with A and B where P indicates probability and '|' given or conditional on. $P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$.Other expression such as $P(\theta|Data, Info) = \frac{P(Data|\theta) \cdot P(\theta|info)}{P(Data)}$ are to highlight that the posterior describes the information of that conditional on the prior information that is given in there.

The derivation of Bayes theorem relies on the axioms probability theory.

**Premise 1)**

$$
P(A | B) = \frac{P(A \cap B)}{P(B)}
$$
similarly

$$
P(B |A) = \frac{P(B \cap A)}{P(B)}
$$
**Premise 2)**

Also, the joint probability, expressed as a set-theoretic relationship on $z$, indicates that element of both sets are the same.

$$
z = \{x : x \in A \cap B : x \in B \cap A\}
$$
thus

$$
P(A \cap B) = P(B \cap A)
$$
**Premise 3)**

In accordance with the previous

$$
P(A| B) \cdot P(A) = P(A \cap B)
$$
and

$$
P(B | A) \cdot P(B) = P(B \cap A)
$$
**Conclusion)**

Therefore

$$
P(A | B) \cdot P(A) = P(B | A) \cdot P(B)
$$
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

### Bayes Theorem in use

The previous expression can help us with answer simple questions. Assume that there is a likelihood of 0.7 $P(Species|<Threshold)$ that a species if found below a certain threshold. Furthermore, we also know that that the environment will only be found 0.3 or 30% of the time as $P(<Threshold)$.How probable would it then be we are below the threshold if we observe the species $P(<Threshold|Species)$? 

$$
P(Species|<Threshold) = 0.7\\
P(<Threshold) = 0.3\\
P(<Threshold|Species) = ?\\
$$

Expressing this in Bayes theorem would result in

$$
P(<Threshold|Species)=\frac{P(<Threshold|Species)\cdot P(Species) }{P(<Threshold)}
$$

The only thing still required is $P(Species)$ often called the 'evidence'. Yet, this evidence is simply the total probability of observing a species, below and above the threshold. For this we can assume that this is the reverse of the $P(Species|<Threshold)$ and $P(<Threshold)$

$$
P(Species)=[P(<Threshold|Species)\cdot P(Species)] + [(1-P(<Threshold|Species))\cdot(1-P(Species))]\\
0.42=[0.7\cdot0.3]+[0.3\cdot0.7]
$$

Then it is simply filling in the blanks

$$
P(<Threshold|Species)=\frac{P(<Threshold|Species)\cdot P(<Threshold) }{P(Species)}=\frac{0.7\cdot0.3}{0.42}=0.5
$$

The answer not very satisfying as the probability is simply a 'coin toss'. This could be improved if we would introduce more species with the same indicative potential.

$$
P(<Threshold|Species)=\frac{P(<Threshold|Species)\cdot P(<Threshold) }{P(Species)}=\frac{(0.7^2*0.3)}{[(0.7^2*0.3)+(1-0.7)^2*(1-0.3)]}=0.7
$$
We would need around five species to get an indicative potential of >0.95 (it would be 0.97).

### Approximate Bayesian Computation with rejection sampling

Approximate Bayesian Computation with rejection sampling (ABC-rejection) is a computationally expensive method for approximating the posterior distribution. However, when the number of parameters is relatively small, the posterior can still be approximated quite well. ABC-rejection is especially useful when the likelihood function cannot be computed or approximated accurately.

One example is the use of ABC to explore potential bias in the EcoPostView package. In a simplified case, assuming both the prior and the data-generating model are normally distributed, the ABC-rejection algorithm begins by simulating a parameter from the prior distribution.

$$
\mu_{i}^*\sim N(\mu_{prior},\sigma_{prior}^2) \\
\sigma_{i}^{2*}\sim Exp(rate)
$$

The asterisk ($^*$) denotes that these parameters are temporary, and this will become important later.Next, a data-generating model is used to simulate data based on these temporary parameters. We assume the observed data is approximately normally distributed, though any model could be used. For each simulation, we generate $n_{data}$ values.

$$
x_{i}\sim N(\mu^*, \sigma^{2*})
$$

Depending on the parameter of interest (e.g., $\mu$, $\sigma$, mode, or median), a summary statistic is computed from the simulated data. In this example, we focus on estimating $\mu$.

$$
\hat{x}_{sim, i}=\frac{\sum_{i=1}^n(x_i, ..., x_n)}{n_{data}}
$$

Each simulated mean $\hat{x}_{sim, i}$ (typically out of 100,000 simulations) is compared to the observed mean $\hat{x}_{data}$ using the Euclidean distance.

$$
E_{i}=\sqrt{(\hat{x}_{sim, i} - \hat{x}_{data})^2}
$$

A tolerance threshold is then selected to determine which simulated values are accepted. Simulations with $E_i > tolerance$ are rejected, while those with $E_i \leq tolerance$ are retained. While a tolerance of zero would yield the most accurate posterior, it would typically result in rejecting all simulations. On the other hand, setting the tolerance too high would allow in too many poor matches.

Each accepted simulation corresponds to an accepted pair of simulated parameters $\mu_{i}^*, \sigma_{i}^{2}*$. Since all $\mu_{i}^*$ were originally drawn from the prior, the subset of accepted values approximates the posterior distribution of $\mu$.

### Bayes theorem and conjugate priors

The previous example works for simpler approximations yet if we want to derive an interval for a particular parameter $\theta$ then we can approach this analytically using conjugate priors. Where a prior is conjugate to a likelihood if the resulting posterior is in the same family as the prior.

As introduced, in statistics and estimation is about finding out the value for $\theta$ which is assumed be $\mu$. Where in the frequentist framework this is considered fixed and unknown, this is in the Bayesian framework considered to be random 'and approximately' known. Of course also in the Bayesian framework samples $x$ are taken. Assume that we already know something about $\mu$ then it is possible to restrict to exclude unreasonable values or for the information we have on $\mu$ to more acceptable values.

$$
P(\mu|Data) = \frac{P(Data|\mu) \cdot P(\mu)}{P(Data)}
$$

For a simple mean and variance an analytical approach can be used to derive the posterior given the likelihood and prior via the following equations.

$$\mu_{posterior} =\frac{\frac{\mu_{prior} }{\sigma_{prior}^2} + \frac{\hat{x}_{data} }{\sigma_{data}^2}}{
\frac{1}{\sigma_{prior}^2} + \frac{1}{\sigma_{data}^2}}
\\
\sigma_{posterior}=\sqrt{\frac{1}{\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{data}^2}}}$$

**Proof:**

**Premise 1)**

Bayes rule can be simplified to
$$P(\mu|Data) \propto P(Data|\mu) \cdot P(\mu)
\\
N(\mu_{posterior}, \sigma_{posterior}^2)=N(\mu_{sample}, \sigma_{sample}^2)\cdot N(\mu_{prior}, \sigma_{prior}^2)$$

**Premise 2)**

The PDF for the normal distribution is
$$f(x)=\frac{1}{2\cdot \sqrt{\sigma \pi}}\cdot exp(-\frac{1}{2}(\frac{x-\mu}{\sigma})^2)$$

**Premise 3)**

$$Prior: P(\mu_{prior})=\frac{1}{2\cdot \sqrt{\sigma_{prior} \pi}}\cdot exp(-\frac{1}{2}(\frac{\theta-\mu_{prior}}{\sigma_{prior}})^2)
\\
Likelihood: P(\mu_{sample})=\frac{1}{2\cdot \sqrt{\sigma_{sample} \pi}}\cdot exp(-\frac{1}{2}(\frac{\mu_{sample}-\theta}{\sigma_{sample}})^2)
$$

**Premise 4)**

Both $\frac{1}{2\cdot \sqrt{\sigma_{prior} \pi}}$ and $\frac{1}{2\cdot \sqrt{\sigma_{sample} \pi}}$ are scalars and can be left out of the equation.

**Premise 5)**

Since both exponent have the same base we can add the exponent $$(a^2+b^2=a^{2+2})$$ 
resulting in

$$exp(-\frac{1}{2}\cdot[(\frac{\theta-\mu_{prior}}{\sigma_{prior}})^2+(\frac{\mu_{sample}-\theta}{\sigma_{sample}})^2]$$

After which brackets can be moved
$$exp(-\frac{1}{2}\cdot[\frac{(\theta-\mu_{prior})^2}{\sigma_{prior}^2}+\frac{(\mu_{sample}-\theta)^2}{\sigma_{sample}^2}])$$

**Premise 6)**

Expanding the brackets terms

$$(a^2+b^2)=(a-b)\cdot(a-b)=a^2-ab-ab+b^2=a^2-2ab+b^2$$
This means 
$$(\theta-\mu_{prior})^2=\theta^2-2\theta\mu_{prior}+\mu_{prior}^2$$ 
and
$$(\mu_{sample}-\theta)^2=\mu_{sample}^2-2\mu_{sample}\theta+\mu_{sample}^2$$ 
which can be replaced in premise 5
$$exp(-\frac{1}{2}\cdot[\frac{\theta^2-2\theta\mu_{prior}+\mu_{prior}^2}{\sigma_{prior}^2}+\frac{\mu_{sample}^2-2\mu_{sample}\theta+\mu_{sample}^2}{\sigma_{sample}^2}])$$

**Premise 7)**

Separating each term by dividing by $\sigma_{prior}^2$ and $\sigma_{sample}^2$

$$exp(-\frac{1}{2}\cdot\frac{\theta^2}{\sigma_{prior}^2}+\frac{-2\theta\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{prior}^2}{\sigma_{prior}^2}+
\frac{\mu_{sample}^2}{\sigma_{sample}^2}+\frac{-2\mu_{sample}\theta}{\sigma_{sample}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2})$$

**Premise 8)**

Group each term by the nominator

$$\frac{\theta^2}{\sigma_{prior}^2}+\frac{-2\theta\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{prior}^2}{\sigma_{prior}^2}+
\frac{\mu_{sample}^2}{\sigma_{sample}^2}+\frac{-2\mu_{sample}\theta}{\sigma_{sample}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2}=
\\
\theta^2(\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2})+
-2\theta(\frac{\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{sample}}{\sigma_{sample}^2})
+(\frac{\mu_{prior}^2}{\sigma_{prior}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2})
$$
Since the last group is not dependent on $\theta$ it is not in our focus
$$
exp(-\frac{1}{2}\cdot[\frac{\theta^2}{\sigma_{prior}^2}+\frac{-2\theta\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{prior}^2}{\sigma_{prior}^2}+
\frac{\mu_{sample}^2}{\sigma_{sample}^2}+\frac{-2\mu_{sample}\theta}{\sigma_{sample}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2}=
\\
exp(-\frac{1}{2}\cdot\theta^2(\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2})+
-2\theta(\frac{\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{sample}}{\sigma_{sample}^2})
+not\ dependent\ on\ \theta])
$$

**Premise 9)**

The goal is to derive $P(\mu|Data)$ from $P(\mu|Data) \propto P(Data|\mu) \cdot P(\mu)$ An the general exponential form of the normal distribution is given in Premise 2 and the premises 6, 7 and 9 lead to
$$\frac{1}{2}\cdot \theta^2 (\frac{1}{\sigma^2})+\theta(\frac{\mu}{\sigma^2})+C=
\\
\frac{1}{2}\cdot\theta^2A+\theta B+C$$
the general exponential form for the normal distribution is always $\frac{1}{2}\cdot\theta^2A+\theta B+C$ meaning that $A=\frac{1}{\sigma^2}$ and $B=\frac{\mu}{\sigma^2}$ and to obtain the standard deviation $A$ needs to be re-arranged to $\sigma = \sqrt{\frac{1}{A}}$ and to obtain the mean $\mu=\frac{B}{A}=\frac{\frac{\mu}{\sigma^2}}{\frac{1}{\sigma^2}}$

**Conclusion)**

In Premise 8
$$
exp(-\frac{1}{2}\cdot\theta^2(\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2})+
-2\theta(\frac{\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{sample}}{\sigma_{sample}^2})+C)
$$
In Premise 9
$$
\sigma = \sqrt{\frac{1}{A}}, A=\frac{1}{\sigma^2}\\
\mu=\frac{B}{A}=\frac{\frac{\mu}{\sigma^2}}{\frac{1}{\sigma^2}}
$$
Which implies that
$$
\sigma_{posterior}=\sqrt{\frac{1}{\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2}}}\\
\mu_{posterior}=\frac{\frac{\mu_{prior}}{\sigma_{prior}^2} + \frac{\mu_{sample}}{\sigma_{sample}^2}}{\frac{1}{\sigma_{prior}^2} + \frac{1}{\sigma_{sample}^2}}
$$
Another way to obtain the posterior mean would be via
$$\mu_{posterior}=\frac{\frac{\mu_{prior}}{\sigma_{prior}^2}+\hat{x}_{data}*\frac{n}{\sigma_{data}^2}}
{\frac{1}{\sigma_{prior}^2}+\frac{n}{\sigma_{data}^2}}$$

As might be clear this is less computational heavy than MCMC methods. For more then two parameter such an analytically approach becomes more cumbersome. And, if conjugacy is not satisfied no closed form solution is available. In this regards, Laplacian approximation is also computational easy. Yet, the equation clearly formulate the idea what happens in Bayes theorem. A simple example below highlights how this this can be useful.

## Salinity example

For example, we want to know the probable relation between insects and salinity over a whole gradient from 0 till 5000 $\mu S \cdot cm ^{-1}$. To do so we generate a simple log-linear model $log(insect ~ diversity)=\beta_1*conductivity~+~\beta_0$. We know that insect diversity declines with an increasing salinity therefore we could exclude  positive values and shift the weight to more negative values. In the frequentist framework this is not performed and there is worked under the principle of indifference. This means all values are apriori equally likely. This is from an consistent error-control perspective preferred, but from a probabilistic perspective unreasonable. Hence, it would be beneficial to leverage this information into the model.
$$
P(\beta|Data, info) = \frac{P(Data|\beta) \cdot P(\beta|info)}{P(Data)}
$$
Under different context we can leverage more information over the likelihood using Bayesian Model Averaging (BMA) that does not use a single prior, but multiple priors.

## Introduction to Bayesian Model Averaging (BMA)

Instead of $P$ the function '$f$' are used this to highlight that the probability is a mapping function. A mapping function being a 'rule' that maps $x$ to $y$ and so $y=f(x)$.
$$
f(\beta \mid Data, Info) = 
\frac{f(Data \mid \beta) \cdot f(\beta \mid Info)}
{\int f(Data \mid \beta) \cdot f(\beta \mid Info)}
$$
The integral in the denominator is used to scale the posterior probability to one. This expression is sometimes simplified to
$$f(\beta \mid Data, Info) = f(Data \mid \beta) \propto f(\beta \mid Info)$$
Where the $\propto$ symbol indicates 'proportional to' highlighting the idea of exchangeability. Therefore, the posterior is nothing more than a function that describes the probability $y$ as a function of $\beta$ conditional on $Data$ and $Info$ ($y=f(\beta \mid Data, Info)$). This cannot be solely conditional on the $Data$ as the $Data$ is not uncertain our information/believe is uncertain about a none existing object $\beta$ (unless Platonism is true). 

In the previous part a single prior model was used. Bayesian Model Averaging (BMA) has the advantages that it allows multiple ($k$) functions to be utilized as prior. I specifically choose the use of $f$ so multiple priors as $f_k$ in the equation below can be seen nothing more as multiple functions (or models). This in my opinion makes it easier to see that there is only optimized between multiple functions. It sound weird to say to optimize between probabilities.
Hence, multiple possible scenarios that could have been responsible for $\beta$ can be introduced as below.
$$
f(\beta \mid Data,Info) = \frac{f(Data \mid \beta) \cdot f_k(\beta \mid Info)}{\int \left( \sum_{k=1}^{k} f(Data \mid \beta) \cdot f_k(\beta \mid Info) \right)}
$$
Now it should be clear that each $\beta$ contained within $g(E(y \mid x_{ij})) = \sum_{j=1}^{v} \beta_j \cdot x_{ij}$ is being restricted by the prior models. While in frequentism it is unrestricted and 'complete indifference' towards the  possibility of $\beta$. All these methods can be used in a meta-analysis.

## Meta-analysis

A standard meta-analysis uses a measure of location (mean) and scale (precision) to estimate a pooled value based on all parameters. For a fixed meta-analysis the pooled parameter is derived via the following equation.
$$\theta_{pooled} = \frac{\sum_{i=1}^{k}(\theta_i\cdot w_i)}{\sum_{k=1}^kw_i}$$
$\theta_i$ is the extracted effect-size for a study $i$. The $w_i$ is the weight per study $i$ for allk $k$ studies, derived from the precision $1/se_i^2$ via the equation below.
$$w_i = \frac{1}{se_i^2}$$
The standard error for the pooled effect-size can then be derived via the formula given below.

$$se(\theta_{pooled})=\frac{1}{\sqrt\sum_{i=1}^{k}(w_i)}$$
For a random-effect meta-analysis the variance between studies is separately modeled. In the metafor package REML or (Restricted Maximum Likelihood) is used to estimate this between study variance. However it is also possible using the DerSimonian and Laird method.
$$
\tau^2=max(0, \frac{Q-(k-1)}{\sum_{i=1}^{k}\frac{1}{w_i}-\frac{\sum_{i=1}^{k}1/w_i^2}{\sum_{i=1}^{k}1/w_i}})\
\\
w^*_i=\frac{1}{(\frac{1}{w_i}+\tau^2)}
\\
\theta_{pooled} = \frac{\sum_{i=1}^{k}(\theta_i\cdot w^*_i)}{\sum_{i=1}^{k}(w^*_i)}
\\
se(\theta_{pooled})=\frac{1}{\sqrt(\sum_{i=1}^{k}w^*_i)}
$$
If we now go back to how we analytically derived the posterior we can devise a function that can analytically perform a fixed effect meta-analysis with ease. I have placed this in a function called 'abmeta'. In in simple cases it approximates the results of metafor and the meta function inf EcoPostView relatively well. Of course the variance component slightly differs with that from metafor and the 'meta' function due to the different method of estimation.

## BMA and meta-analysis

In a meta-analysis we do not talk about $\beta$ but about a set of estimates $\beta=\{\beta_{i}, ..., \beta_{n}\}$ meaning that $f(Meta-data\mid\{\beta_{i}, ..., \beta_{n}\})$. Hereby the flexibility allows that these estimates are either likelihood estimates ($\hat{\beta}$) or  posterior estimates ($\beta$). and we end up with an expression that should capture the inference to an underlying pooled model parameter.
$$
f(\beta_{poolded} \mid Meta-data,Info) = \frac{f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\}) \cdot f_k(\beta_{pooled} \mid Info)}{\int \left( \sum_{k=1}^{m} f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\}) \cdot f_k(\beta_{pooled} \mid Info) \right)}
$$

## Sequential updating

Bayesian sequential updating refers to the practice of re-using the derived posterior of a previous model as the prior for the new model. For this the assumption of conditional independence between the the datasets is assumed. The parameter of interest is $\theta$ based on a dataset $Data_1$ and we derive the posterior. 
$$P(\theta|Data_1) = \frac{P(Data_1|\theta) \cdot P(\theta)}{P(Data_1)}$$
The next would be 
$$P(\theta|Data_1, Data_2) = \frac{P(Data_2|\theta) \cdot P(\theta|Data_1)}{P(Data_2)}$$ 
till $$P(\theta|Data_n) = \frac{P(Data_n|\theta) \cdot P(\theta|Data_1,\cdots,Data_{n-1})}{P(Data_n)}$$

For example, we would like to know what $\mu$ from a population of interest. Our example population has $\mu=0.5$, $\sigma=5$ and each study would have an error of $\alpha = 40\%$ when when we assume $\alpha=5\%$ (meaning that our heterogeneity is larger than expected). Our first prior starts with $N(0, 5)$ after which the posterior of previous is sequentially re-used visually represented in Fig. 2a below. Where more studies increase the precision of the estimated posterior. 

If the focus lies on objectivity and the error control over the different studies and assume iid then the curve between studies would follow that of Fig. 2b below. 

In a less formal way is the Bayesian framework more focused on transfer of information an precision. On the other hand the frequentist framework is more interested in objectivity, consistency and error among studies.

```{r sequential updating,  fig.width=8, fig.height=4, echo=F, warning=FALSE}

library(ggplot2)

postvals <- function(mu_data, sigma_data, mu_prior, sigma_prior){
  
  post_mu <- (mu_prior / sigma_prior^2 + mu_data /sigma_data^2) / (1 / sigma_prior^2 + 1 / sigma_data^2)
  post_sigma <- sqrt(1/(1 / sigma_prior^2 + 1 / sigma_data^2))
  
  c(mu=post_mu, sigma=post_sigma)}

nsim <- 100
cint <- 0.05
errint <- 0.4
theta  <- 0.5
postset <- array(NA, dim=c(nsim, 5))
freqset <- array(NA, dim=c(nsim, 5))

z_adj <- abs(qnorm(errint/2))/abs(qnorm(cint/2))

set.seed(3)
for(i in 1:nrow(postset)){
xsamp <- rnorm(25, theta, 5)
if(i==1){
post  <- postvals(mean(xsamp), sd(xsamp)/sqrt(length(xsamp)), 0, 5)
}else{
post  <- postvals(mean(xsamp), sd(xsamp)/sqrt(length(xsamp)), post[1], post[2])}
postset[i,] <- c(i, post, post[1]-post[2]*abs(qnorm(cint/2))*z_adj, post[1]+post[2]*abs(qnorm(cint/2))*z_adj)

fse <- sd(xsamp)/sqrt(length(xsamp))
freqset[i,] <- c(i, mean(xsamp), fse,  mean(xsamp)-fse*abs(qnorm(cint/2))*z_adj,  mean(xsamp)+fse*abs(qnorm(cint/2))*z_adj)}

postset <- setNames(as.data.frame(postset), c("iter", "mu", "se", "ll", "ul"))

p1 <- ggplot(postset, aes(iter, mu))+
  geom_line()+ylab("¬µ")+xlab("Study")+
  geom_hline(yintercept = 0.5, col="tomato3", lty=2, lwd=0.2)+
  geom_ribbon(aes(ymin = ll, ymax = ul), fill = "grey70", alpha=0.2)+
  theme_classic()

freqset <- setNames(as.data.frame(freqset), c("iter", "mu", "se", "ll", "ul"))

p2 <- ggplot(freqset, aes(iter, mu))+
  geom_line()+ylab("")+xlab("Study")+
  geom_hline(yintercept = 0.5, col="tomato3", lty=2, lwd=0.2)+
  geom_ribbon(aes(ymin = ll, ymax = ul), fill = "grey70", alpha=0.2)+
  theme_classic()

sequpdate <- cowplot::plot_grid(p1, p2, ncol=2)
print(sequpdate)
rm(sequpdate)
```

*Figure 2: Sequential updating with credibility intervals on the left panel and a long-run of means with confidence intervals on the right The left panel.*

## A short reflection on uncertainty

I do not believe statistics reflects uncertainty about events; rather, it reflects the information in the data under a particular model or the uncertainty about our belief in a parameter ($\theta, \beta, \mu$, etc.). The later concept is often vague and confusing because, if one assumes the parameter does not exist independently of the mind, then what exactly is uncertain - our belief? The claim to 'objective probability' is already compromised by the assumption that the parameter is objective. However, if the parameter does not exist outside the mind, the meaning of 'objective' in this context becomes questionable.

When people refer to objectivity, they often mean that the data itself is the most 'objective' part of the process. However, if some conditions are not met, such as (1) the data is not randomly sampled from a population of interest, (2) the model is not pre-selected in advance, (3) a sufficiently large sample size is not chosen based on the model, and (4) confounding variables are present, then even the data cannot be considered truly objective unless these limitations are explicitly acknowledged. Moreover, model selection procedures further contaminate the objectivity of the data, meaning that the estimated model parameters no longer fully reflect the objectivity of the data which is often implied in our conclusions (Gelman and Loken, 2013; Tong, 2019).

In Bayesian updating, the prior reflects the extent to which we want to sacrifice over the objectivity of the likelihood by using information which cannot be formalized into the likelihood. This is captured by the relationship $f(\theta \mid Data, Info) = f(Data \mid \theta) \propto f(\theta \mid Info)$

The posterior, therefore, is merely the weighted combination of the prior and likelihood. It represents the relationship (e.g., $0.25$ as $0.5 \cdot 0.5$) between the prior and the likelihood. There is no invalidity in a logical argument such as:(Premise 1.) All unicorns are orange. (Premise 2.) I have a unicorn. (Conclusion) Therefore, my unicorn is orange.

While this argument may be unsound ‚Äî because unicorns do not exist ‚Äî the reasoning itself is not flawed. The issue lies with the premises, not the structure of the argument. Hence, Uncertainty does not exist in the 'real' world; it resides solely in our minds. We cannot be 'wrong' or 'correct' about $f(\beta \mid \text{Data, Info})$ because it does not exist as a tangible entity. Even if it did, its existence would have no impact on reality because uncertainty is unrelated to the way reality operates. In the real world, events either occur or they do not. If my unicorn does not exist, I will never see it, and it was never orange in the first place.

We should also avoid treating models as a definitive representation of reality. Models are tools that convey information and serve as pragmatic instruments. The the model itself is not the result, the strength of the results relies on the argument, and how well the premises within the argument are clarified and supported by the model.

# EcoPostView

## Introduction

Ecological data is scattered throughout the literature in varying formats. It is often case-specific and not representative of the full range of ecological conditions encountered in the real world. As a result, a large amount of noise is introduced, limiting the generalizability of results. Moreover, there is no coherent framework to re-use, generalize, and integrate large amount of ecological data into understandable and predictive models. This lack of structure hampers ecological understanding and leads to a loss of valuable information.

As ecologists, we are not only interested in isolated, case-specific explanations or post-hoc rationalizations. We aim to build cumulative knowledge and apply it across diverse contexts.
Working with logic, metadata, and stochastic processes can help solidify our understanding of ecological relationships. These tools enable us to make probability based statements and generate predictions. This R package is designed to assist with exactly that. Its goal is to facilitate the integration of multiple sources of information, combining their strengths to generate broader insights.

The core function of the package utilizes outputs from multiple fitted Linear and Generalized Linear Models ((G)LMs), leveraging Bayesian methods to generalize their effects into a single, meta-analytic (G)LM. This synthesized model can then be visualized (see Fig. 1 below) and used to make predictions on new data. The ultimate aim is to provide a robust, generalized understanding of ecological relationships by drawing from all available sources of information.

```{r intro-plot,  fig.width=8, fig.height=4, echo=F, warning=FALSE}
library(EcoPostView)
library(ggplot2)
library(cowplot)
data("example1")

mod_intro <- meta(estimate=example1$est,         
                  stderr=example1$se,            
                  parameter=example1$parameter,  
                  predictor=example1$predictor,  
                  link_function=example1$link,   
                  grouping=example1$group,       
                  Nsamp=example1$n,
                  n_chain=6,
                  n_thin=10,
                  method=2)

pl1 <- hop(mod_intro ,                                              
           group = "Invertebrates",                            
           predictor = c("Salinity", "Sediment"),                   
           xlab = expression(Conductivity ~ ¬µS ~ cm^-1),                          
           ylab = "Fine sediment fraction",                   
           pdp_resolution = 100,                                   
           link_function = "logit",
           exp_disp = T, 
           round_x_axis = 0,
           round_y_axis = 2,
           xlim=c(2.99, 6.215), 
           ylim=c(-4.61, -0.92))
pl1 <- pl1+ggplot2::theme(legend.position = "none")

pl2 <- hop(mod_intro ,                                              
           group = "Invertebrates",                            
           predictor = c("Salinity", "Sediment"),                   
           xlab = expression(Conductivity ~ ¬µS ~ cm^-1),                          
           ylab = "Fine sediment fraction",                   
           gradient_title = "Evenness \naquatic \ninvertebrates",
           pdp_resolution = 100,                                   
           link_function = "logit",
           exp_axis = T, 
           round_x_axis = 0,
           round_y_axis = 2,
           xlim=c(2.99, 6.215), 
           ylim=c(-4.61, -0.92))
pl2 <- pl2+ggplot2::theme(axis.title.y = element_blank())

intro_plot <- cowplot::plot_grid(pl1, pl2, rel_widths = c(0.43, 0.57))
print(intro_plot)
rm(mod_intro)
```

*Figure 1: Expected evenness of aquatic invertebrates as a function of conductivity and fine sediment fraction within a river reach. The left panel shows the relationship on the response scale, while the right panel presents it on the log scale for improved visualization.*

This package offers tools for ecological statistical modeling, with a strong emphasis on Bayesian methods. While a basic understanding of R is sufficient to use the core functions, a more in-depth theoretical background is provided in chapter 2 for those interested in exploring the underlying concepts. Please note that this is an actively developed R package, and improvements or updates may be introduced over time.

## Data and models

The information required for a meta-analytic approach can often be extracted from figures, tables, datasets, or combinations of these sources. However, such information is rarely used in a consistent or standardized way. Put simply, multiple datasets are needed on which (G)LMs can be fitted (see Kaijser et al., ...). This process often reveals that ecological data is noisy, potentially biased, and exhibits considerable heterogeneity across studies.

These challenges pose difficulties for drawing causal inferences or controlling statistical error. Both error control and causal conclusions require controlled environments, well-designed experiments, and the identification or modeling of confounding variables. While it may not be possible to impose such controls retrospectively (i.e., a posteriori), the existing information is still highly valuable.

This data can be used to make probabilistic statements, generate predictions, and estimate the a priori power required to design future studies that do focus on error control or causal inference. The purpose of this package is to enable such posterior analysis‚Äîallowing users to generalize ecological effects from the literature, visualize emerging patterns, and make informed predictions.

## (Generalized) Linear Models (G)LMs and extracting data

To use this package effectively, data is required, and (G)LMs need to be fitted to that data. This (meta-)data can be obtained from figures (e.g., using tools like WebPlotDigitizer), tables (e.g., by converting PDFs to Excel files), datasets, or combinations of these sources (see Kaijser et al., ...).

The so-called "effects" we refer to are, more precisely, model parameters, commonly known as the intercept and slope - typically denoted as (*b0* or *Œ≤0*) and (*b1* or *Œ≤1*). These parameters define the equation:

**response variable = b0 + b1 ¬∑ predictor variable**

This package is built on the underlying philosophy that if we accept a reported parameter (e.g., (*b1*) to represent an "effect" of the predictor, then such an effect should ideally be generalizable. For instance, the relationship between chlorophyll-a and total phosphorus is widely considered generalizable across aquatic systems.

From here on, the term model parameter will be used instead of "effect." By collecting estimates of (*b0*) and (*b1*) from various studies, we can build a pooled model that predicts responses for one or more new values (*xi* or $x_i$). This approach allows us to understand the magnitude of the relationship, assess its variability, and make informed predictions.

In the context of Generalized Linear Models (GLMs), the response variable is linked to the linear predictor through a link function, commonly denoted as *g(...)*.For example, when using the identity link function, no transformation is applied. In this case, the expected value of y - written as *E(y)* or *(E(y|x))* - is directly related to the linear component, just as in a standard LM.
$$g(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$ 
However, in a GLM with, log- or logit-link it is easier to talk about log-linear relations 
$$log(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$ 
or logit-linear relations
$$logit(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$
In a Generalized Linear Model (GLM), the slope is not a "true" slope in the geometric sense, since the relationship between the response variable (*y*) and predictor variable (*x*) is no longer a straight line.However, the model is still considered linear because the parameters are incorporated linearly in the linear predictor. As such, the terms coefficient or regression coefficient typically refer to these model parameters, denoted as ($\beta$). 

In practice, I often prefer to work with elasticity or semi-elasticity coefficients (Wooldridge, 2001), which can offer an interpretable measures in log-linear or logit-linear models. That said, their use is context-dependent and may not always be appropriate. The elasticity coefficient quantifies the percentage change in $y$ associated with a 1% change in $x$. For example, an elasticity of 0.2 implies a 0.2% increase in $y$ for every 1% increase in $x$. In a log-log model: $y$ given 1% in $x$. Hence, for a log-linear model $log(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$
and thus $\beta_1 = \frac{\log(y)}{\log(x)}$. For the semi-elasticity coefficient (i.e., logit-linear) this only accounts partially and values closer 0 are better interpretable because $logit(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$ and thus $\beta_1 = \frac{logit(y)}{\log(x)}$. This expressed the  change in the log-odds per 1% elasticity 

(Cramer, 1991; Wooldridge, 2001). These coefficients allow comparison across models and predictors while maintaining interpretable units for prediction. Since $x$ is log-transformed, its original units are preserved ‚Äî unlike in standardized coefficients, where this interpretability is lost.

As an example, consider a decline in benthic invertebrate species richness from 100 to 30 as conductivity increases from 50 till 5000 $\mu S¬∑cm^-1$ The elasticity is: 
$$\beta_{elasticity}= (log(100)-log(30))/(log(50)-log(5000))=-0.26$$. 
This decrease is the same for a decline from 10 till 3 over the same range $$\beta_{elasticity}=(log(10)-log(3))/(log(50)-log(5000))=-0.26$$ 
Although the model‚Äôs intercept $\beta_0$would differ, this does not affect the interpretation of the regression coefficient $\beta_1$, nor its uncertainty or visualization.

I use the 'unofficial expressions' for *b0* and *b1*, due to the reference in the R-package to The expression from the models above would be more more formally expressed: 
$$g(E(y_{i} \mid x_{ij})) = \sum_{j=1}^{j} \beta_j \cdot x_{ij}$$
Where $x_{ij}$ refers to the $j$ the predictor variable (e.g., salinity is $j$=1 and light is $j$=2) and $i$ is the $i$-th observation. This expression will later be utilized in the explanation of the visualization.

## Data and R-package

To start using this R-package, both JAGS and devtools must be installed. JAGS can be installed from https://sourceforge.net/projects/mcmc-jags/ and devtools can be installed in R via CRAN. The most recent version of the EcoPostView it can be installed from GitHub. Of course, any problems, questions or possible improvements can be directed to me.

```{r setup}
#install.packages("devtools")
library(devtools)

#install.github("snwikaij/EcoPostView")
library(EcoPostView)
```

At this stage, we assume that multiple (G)LMs have been fitted. From these models, the parameter estimates and their standard errors have been extracted and compiled into a dataset. For each estimate, it is useful to record relevant metadata including the source (e.g., DOI), the type of predictor variable (e.g., conductivity), group of the response type (e.g., benthic-invertebrates), link-function and if the model parameters is the intercept *b0* or a regression coefficient *b1*. When models include multiple predictor variables, all corresponding regression coefficients are denoted as *b1*, distinguishing them from the intercept *b0*. The example below in R demonstrates the expected structure of this data frame.

```{r data}
data(example1)
head(example1)

```

In the example above, the **est** column contains the estimated model parameters, while the **se** column holds the standard error of those estimates. The **group** column can represent an organism group, specific species, or taxon (or any other category you wish to use for grouping). The **predictor** column denotes the specific predictor variable, and the **parameter** column indicates whether the estimate corresponds to the intercept (*b1*) or a regression coefficient (*b1*). The link column specifies the link function used in the model. Additionally, it is recommended to include the sample size (**n**) in your dataset to adjust for 'small-sample effects' if needed (Peters et al., 2006; Moreno et al., 2009).

## Basic model structures for meta-analysis

The meta function can include a random effect, by setting the argument RE=TRUE (default is TRUE). The  structure is then 
$$\{\beta_{i}, ..., \beta_{n}\}= \beta_{\text{pooled}} + u_i$$
It has the option of placing a single or multiple random effect as a vector or matrix using the argument 'random' the structure then becomes then 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i +r_i$$
for a single random effect. It can adjust for the relation between the $se$ and model parameters using the the squared standard error $se^2$ often refered to as Precision-Effect Estimate with Standard Errors or short PEESE (method=1, Stanley and Doucouliagos, 2014). The structure is then 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i + \alpha_{i} \cdot se^2$$ 
or inverse of the sample size $1/n$ (method=2, the latter option is performed below) with the structure 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i + \alpha_{i} \cdot \left(\frac{1}{n}\right)$$ 
Of course if bias is considered neglect non can be performed (method=0). I still would like to include a third 4th option to utilize Robust Bayesian Model Averaging (RoBMA: Maier et al. 2023). But this sometimes adjust extremely when including $se^2$ and therefore I left this option open for now.

## The meta-function
The meta-function can be applied over the example data via the following argument.

```{r meta1}
mod1 <- meta(estimate=example1$est,         #Model estimate
             stderr=example1$se,            #Standard error of the model estimate
             parameter=example1$parameter,  #Model parameter (b0 or b1)
             predictor=example1$predictor,  #Predictor variable  (independent variable)
             link_function=example1$link,   #Link function
             grouping=example1$group,       #Group
             Nsamp=example1$n,              #Sample size (optional, for adjustment 2=Peters (1/n)),
             method=2)                      #Adjustment method (0=none, 1=Egger's (1/se), 2=Peters (1/n))
```

The meta-function can return a warning that the MCMC-chains are not properly mixing. This can be an issue due to various reasons. Where this warning originates from can be assessed by looking at the 'raw' JAGS model output (`mod1$model$JAGS_model`). This could show that a parameter of interested 'mu[.]' Has a a large Rhat or small effective sample size. Most of these issues can be resolved by thinning the chains, increasing the number of chains or setting more informed or stronger priors. Moreover, if the issue is not an issue of the estimated 'mu' parameter, it could be decided to ignore it. These choices are ultimately up to the user. An option to prevent warnings would be to set the warning level for Eff_warn lower i.e., Eff_warn = 500.

The meta-function may return a warning indicating that the MCMC chains are not mixing properly. This issue can arise for various reasons. To diagnose the source of the warning, examine the raw JAGS model output (`mod1$model$JAGS_model`). Specifically, look for cases where a parameter of interest, such as `mu[.]`, has a large Rhat value or a small effective sample size.

Most of these issues can be addressed by thinning the chains, increasing the number of chains, or specifying more informed or stronger priors. If the problem is not related to the mu parameter, you may choose to disregard the warning. Ultimately, the decision on how to address these issues lies with the user.

To prevent the warning from being raised, you can lower the threshold for the Eff_warn parameter (e.g., Eff_warn = 500).

## Standard meta-analysis

Meta-analysis is often performed using standardized effect sizes (SES). While I do not endorse this practice, I believe it offers limited benefits for field ecology, applied ecology, and the generalization of real-world ecological relations (Baguley, 2009; Tukey, 1969). Therefore, I will provide a brief introduction to meta-analysis and demonstrate how bias correction methods perform. To illustrate this, I will compare the results with those obtained using my preferred metafor package in R.

```{r, bias adjustment and metafor}
#First run and check standard output for metafor
library(metafor)
data("example3")

#Run metafor
standard_metafor <- metafor::rma(yi=example3$est, sei=example3$se)

#Run EcoPostView
standard_meta    <- meta(estimate=example3$est, stderr=example3$se)

#Results metafor
print(standard_metafor)

#Results EcoPostView
print(standard_meta$Summary)

```

Both the metafor and EcoPostView packages yield similar means (-0.22) and standard errors. This suggests that, in many cases, not specifying priors may be uninformative. However, we can further assess the models performance by examining the bias through the residuals.

```{r plot metafor and EcoPostView, fig.width=8, fig.height=4}
par(mfrow=c(1,2))
plot(1/example3$se, resid(standard_metafor), 
     xlab = "1/se", ylab="Residuals", main="metafor")
abline(a=0, b=0, col="red", lty=2)

plot(1/example3$se, standard_meta$Residuals, 
     xlab = "1/se", ylab="Posterior mean residuals", main="EcoPostView")
abline(a=0, b=0, col="red", lty=2)
```

Both functions reveal a clear diagonal pattern, which is nearly identical in both cases. However, the posterior means are pulled closer to the overall mean (a phenomenon known as shrinkage) for estimates with weaker standard errors. This bias can also be assessed using the rescheck function from EcoPostView

```{r plot rescheck function, fig.width=8, warning=FALSE, fig.height=4}
#Use the residual check function within EcoPostView
res_bias <- rescheck(standard_meta)

print(res_bias$bias_se)
```

This bias is clearly a result of excluding or selectively retaining 'significant' results, often through practices such as manually dropping 'non-significant' variables or using stepwise model selection methods like forward/backward AIC or BIC (Gelman & Loken, 2013). These practices can lead to a significant overestimation of the parameter (or 'effect size'). This bias can be corrected using Method 1 presented by Stanley and Doucouliagos (2014).

```{r adjusting bias via Eggers approach, fig.width=7.8, fig.height=4, warning=FALSE}
#Run EcoPostView (increased the chain thinning interval and number of iterations to improve mixing)
adjusted_meta    <- meta(estimate=example3$est, stderr=example3$se, method = 1, 
                         n_thin = 5, 
                         n_iter = 30000)

print(adjusted_meta$Summary)

res_bias2 <- rescheck(adjusted_meta)
print(res_bias2$bias_se)
```

In the method outlined above, the bias has been adjusted (not removed), resulting in a much lower pooled estimate. This adjustment allows for a clearer assessment of the relationship between the standard errors by examining the residuals. While this is commonly done using funnel plots, it can also be done by directly checking the residuals.

However, the adjustment should only be applied when clear patterns of bias are present, as it can lead to over-corrections even when no bias is evident. That said, it is highly effective when a bias is present in the data. In the future, I aim to incorporate additional methods to better assess the strength of any bias.

The bias displayed in this example is extreme, and in such cases, it may be beneficial to further explore the nature of the bias to determine if a 'publication gap' exists. This gap can be highlighted by examining the z-distribution derived from the p-value. Normally, the p-value is derived from the z-value, but when a clear gap is visible (as observed in Zwet & Cator, 2021), we should be able to model the absolute z-value as a mixture of two half-normal distributions - one truncated at 0 and the other at 1.96. Since the likelihood of such a mixture is challenging to estimate, I employ an Approximate Bayesian Computation (ABC) algorithm based on rejection sampling (ABC-rejection). This method is further described in Csill√©ry et al. (2010) and Hartig et al. (2011), and the model is formally presented in the theoretical section.

```{r exploring bias with ABC-rejection, fig.width=7.8, fig.height=4, warning=FALSE}
#From the dataset calculate the p-value from the effect-sizes and standard errors
pvalues <- ptoz(estimate=example3$est, stderr=example3$se)

#run the ABC-rejection model
result_abc  <- abctoz(p=pvalues$data$p, nsim = 250000)

#Extract the information from the results based on a selected threshold
extract_abc <- extrabc(result_abc, dist_threshold = 0.052)

#Plot the histogram of the z-values with the simulated density lines of the posterior
plot(extract_abc$hist)
```

Based on the distribution of z-values, we can clearly observe that values where |z|>1.96 are more frequently published than those with lower absolute values. This would be reasonable if no publication gap existed; however, the sharp boundary at |z| > 1.96 suggests otherwise. The z-values can be reasonably modeled, and the model appears to fit the data well, with an R-squared typically above 0.6‚Äì0.7, which is considered acceptable. Additionally, the density curves align well with the histogram, further supporting the model's fit.

The proportion of observations explained by the censored component of the model is 0.76 (76%). This does not imply that 76% of the data is censored, but rather that the model's censored component captures a substantial portion of the observed pattern. The goal here is to determine whether the model provides a good fit to the data under the assumption of selective reporting. If the residuals suggest severe bias, this model fit offers additional information of a selection process at play.

To ensure the robustness of the results, one should verify that the model fit remains adequate and that the number of accepted simulations is heuristically sufficient (typically > 100).

### Setting priors

A key advantage of the Bayesian approach is the ability to incorporate prior information, thereby explicitly shifting the posterior estimates toward more plausible values for the pooled model parameter. To define a single prior for each relation and parameter, a specific structure is required. By default, model parameters are assumed to follow a normal (Gaussian) distribution with a mean ($\mu$, `prior_mu`) of 0 and a standard deviation ($\sigma$, `prior_mu_se`) of 0.5. At present, the prior distribution for model parameters is limited to the normal distribution. The prior for the residual standard deviation ($\sigma$) is defined as a uniform distribution, with the upper bound (`prior_sigma_max`) set to 5 by default.

As discussed in Section, I often prefer to work heuristically with elasticity or semi-elasticity coefficients. However, this is not required, and the choice of prior should reflect your modeling preferences and domain knowledge. In fact, failing to think carefully about the priors ‚Äî even when even limited prior information is available ‚Äî means the analysis is not truly Bayesian in nature.

Users can specify their own prior values for both the mean and standard deviation. To obtain a structured overview of the required prior inputs, set `get_prior_only = TRUE`. This will return a data frame containing a level column, as well as columns for the prior mean ($\mu$) and standard deviation ($se$). These values can then be tailored to the specific context of the analysis using available prior information. Details on how to incorporate this prior data frame into your model are provided later.

``` {r meta2 example priors}
only_priors <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,       
                    Nsamp=example1$n,            
                    method=2,
                    get_prior_only=TRUE) #Only show the structure of the priors

print(only_priors)
```

### Setting multiple priors for Bayesian Model Averaging

An important advantage of the Bayesian framework is the ability to incorporate multiple prior distributions ($k$), enabling Bayesian Model Averaging (BMA; Hoeting et al., 1999; Hinne et al., 2020). This approach allows one to represent multiple plausible scenarios that could have explained the observed data, and to average over these competing models based on their relative credibility.

To implement BMA, a dataset similar in structure to the single-prior setup is required, but extended to include multiple prior specifications. Each prior distribution typically includes a mean ($\mu$) and standard error ($se$), just as before.

In many cases, prior weights are assigned to reflect how strongly each prior contributes to the model. These weights range between 0 and 1 and ideally sum to 1 (or 100%). For simplicity, especially when no strong preference among priors exists, equal weighting can be used (e.g., with three priors, each receives a weight of 1/3). Alternatively, when the weights themselves are uncertain, they can be treated as random variables and modeled using a Dirichlet distribution: $weight \sim Dir(\alpha_i)$ where $\alpha_i = 1$ for each prior ($i$), yielding a uniform Dirichlet distribution.

In the example below, I illustrate this approach using priors with varying values of $\mu$ and $se$. For intercept parameters, a broader prior such as $N(\mu = 0, se = 10)$ is often reasonable, reflecting higher uncertainty.

``` {r meta3 BMA}
data("example2")
print(example2)

mod2 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu=example2[c(2,4,6)],          #prior for the mean
                    prior_mu_se=example2[c(3,5,7)],       #prior for the standard error of the mean
                    Nsamp=example1$n,            
                    method=2,
                    n_thin=10,                            #thinning the chains
                    n_chain=4)                            #changing the number of chains from 2 to 4

#Display the summarized results
mod2$Summary
```

The results of the meta-analysis are summarized in a table that includes the Maximum A Posteriori (MAP) estimates, the posterior mean ($\mu$), standard error ($se$), and the High Density Interval (HDI), which by default is set to 90%. Additionally, the heterogeneity among studies is quantified using the $I^2$ statistic.

The prior for the between-study variance ($\tau^2$) is, by default, specified as a uniform distribution ranging from 0 to 5. This choice has the benefit of producing wider intervals, which can be conservative‚Äîparticularly useful when dealing with smaller sample sizes. However, this conservatism can also be a drawback in cases where more precision is desired.

To offer users flexibility, the argument `prior_var_fam` can be set to `"exp"` to use an exponential distribution instead of the default `"unif"` (uniform distribution). When using`"unif"`, the variance prior is specified as $Unif(0, \text{prior_study_var})$. When "exp" is selected, the prior variance becomes $Exponential(\frac{1}{\text{prior_study_var})}$

In general, for complex meta-analyses with many predictors and responses (e.g., Kaijser et al. ...), the uniform prior is recommended, provided that convergence is achieved. In contrast, for more focused analyses with fewer predictors and responses - and particularly with small datasets (e.g., $n < 10$) ‚Äî an exponential prior (e.g., with a mean of 1000) may be more appropriate.

These recommendations are heuristic ‚Äî they are grounded in practical experience and prior applications, but should not be treated as universally optimal. Users are strongly encouraged to conduct sensitivity analyses to assess how prior assumptions influence the results.

### rescheck-function and bias

After analyzing the meta-data, it is essential to check for bias, which can arise from multiple sources. This check should ideally be part of a sensitivity analysis, employing various methods such as: Display of the z-distribution, Egger's test, Peters tests and/or funnel plots. Bias is nearly always present to some extent, but its magnitude may vary depending on the dataset.

A straightforward first step is to visually assess the relationship between the residuals and the inverse of the standard error ($1/se$). If sample sizes are available, one can also assess the relationship with $1/n$.

If a clear diagonal pattern between $\beta$ to $1/se$ can indicate the selection larger effects with broader intervals, p-hacking, HARKing, data dredging, noise in the data, etc. A relation with $1/n$ often occurs when small sample sizes an noise result in so called 'small-study-effects'. 

The residuals can be assessed across the total dataset, per group or per predictor. 

Below the residuals per group in relation to $1/se$.

```{r, residual check, fig.width=8, fig.height=4, warning=FALSE}
res_mod2 <- rescheck(mod2)

print(res_mod2$bias_se_group)
```

And the residuals per predictor in relation to $1/se$.

```{r, residuals predictor, fig.width=8, fig.height=4, warning=FALSE}
print(res_mod2$bias_se_predictor)
```

The dotted red line should approximately overlay the solid blue line, which represents the expected relationship with an intercept and slope of 0. However, small sample sizes can substantially influence the slope of the red line, potentially leading to misleading inferences. When clear bias is detected, it is advisable to either apply a bias correction method or specify stronger priors to mitigate the consequences.

## senscheck-function and prior sensitivity

Sensitivity checks play an important role in assessing the robustness of model results. For simpler models with highly informative data, these checks may not always be necessary. However, one cannot assume that identical results would be obtained in a subsequent study under different conditions. Consequently, drawing strong conclusions based solely on whether an interval includes zero is arbitrary and often misleading.

A more informative approach involves directly inspecting the posterior distribution, along with the estimate, its uncertainty, and a visualization of the variance in patterns revealed by the data. This is particularly important in ecology, where data tend to be noisy and often exploratory in nature‚Äîmeaning the posterior distribution can vary substantially between studies. Still, sensitivity checks can serve as a useful reality check.

In this framework, a sensitivity check evaluates the difference between a fully specified model with informed priors (mod1) and a baseline model with vague or weak priors (mod0). This comparison is made by computing the posterior odds ratio: $Log(P(Mod1|Data, Info)/P(Mod0|Data, Info))$ assuming that all other model hyper parameters are held constant except for the priors.

An alternative approach is to assess the extent to which prior information in mod1 versus mod0 contributes to a shift in the posterior away from zero: $Log(P(Mod1>0|Data, Info)/P(Mod0>0|Data, Info))$ This can be transformed into a probability between 0 and 1, where 0.5 indicates no net influence of the prior on the posterior, 0 indicates complete negative influence, and 1 complete positive influence. However, I am not the biggest fan of this way of assessing sensitivity, as this again treats the posterior as a form of dichotomous hypothesis test.

In the earlier example, mod2 was treated as mod1. A corresponding weakly informed mod0 model can be created by setting all prior means ($\mu$) to 0 and their standard errors ($se$) to 100.

```{r, sensitivity check, fig.width=8, fig.height=4, warning=FALSE}
#Create a model with minimal prior information
mod0 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu=0,                           #prior for the mean
                    prior_mu_se=100,                      #prior for the standard error of the mean
                    Nsamp=example1$n,            
                    method=2,
                    n_thin=10,
                    n_chain=4)

#Perform the sensitivity check
sens_check <- senscheck(mod2, mod0)

#Plot the posterior odds
print(sens_check$posterior_odds)

```

The vertical black line in the plot represents the threshold where there is no difference between the models, i.e., where: $0=Log(P(Mod1|Data, Info)/P(Mod0|Data, Info))$ This corresponds to equal support for both mod1 and mod0. Notably, the results show that for log-linear models, the fish‚Äìoxygen relationship, and for logit-linear models, the salinity‚Äìsediment relationship, the inclusion of prior information in mod1 shifts the posterior distributions toward more negative values.

To quantify the strength and direction of this shift, the inverse logit of the Maximum A Posteriori (MAP) value can be taken: To quantify the strength and direction of this shift, the inverse logit of the Maximum A Posteriori (MAP) value can be taken: $logit^{-1}(MAP)$ This transformation expresses the shift as a probability where smaller then 0.5 indicates negative shift, bigger than 0.5 a positive shift and 0.5 none.

```{r, sensitivity quantif, warning=FALSE}
#Select only predictor and link function
inv_df <- sens_check$table[c(3:4)]

#Calculate the probability
inv_df$prob <-plogis(sens_check$table$mu[sens_check$table$group=="Fish"])

#Print the table
print(inv_df)
```

The table shows that for sediment in a logit-linear model related to fish, the posterior probability shifts from 0.38 in mod0 to 0.50 in mod1. This corresponds to a 12% increase, i.e., 
0.50‚àí0.38=0.12. This indicates that prior information contributes additional support for a negative relationship between fine sediment and lotic fish species ‚Äî an relation is not fully supported by the data alone.

This is not a limitation but rather reflects what occurs in practice: prior information often drawn from empirical studies or domain expertise tends to be more directional. This highlights the value of incorporating prior information from the literature when building and refining Bayesian models.

Admittedly, this is the most demanding phase of the workflow: gathering and extracting data, fitting multiple (G)LMs, defining and implementing priors, assessing potential biases, and optimizing the model to ensure stable and interpretable results. Once complete, presenting the results or making predictions from the fitted models is considerably more straightforward.

## pdplot-function

To visualize posterior results, a common approach is to display point estimates along with credible intervals. However, this method imposes sharp boundaries on a continuous distribution of uncertainty, which may not fully reflect the nature of a posterior probability distribution.

An alternative‚Äîand often more informative‚Äîapproach is to plot the Posterior Density Distribution (PDD). This plot combines the point estimate, interval range, and the full shape of the posterior, offering a richer picture of the uncertainty and possible parameter values.

The PDD represents the distribution of the pooled parameter estimate conditional on the meta-data and prior information: $f(\beta_{\text{pooled}} \mid Meta-data, Info)$. This is conceptually the inverse of the likelihood, which tells us how likely the observed data are given a set of parameter values: $f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\})$. In practice, this visualization can be generated using the `pdplot()` function, which overlays the posterior density curve with interval and point estimates, allowing for intuitive interpretation of the central tendency and uncertainty of the pooled estimate.

```{r pdplot}
pdd <- pdplot(mod2, 
              label_size=4,      #setting the label size larger
              point_size=2)      #large point
```

The object contains the figures generated for both the log

```{r pddplot log, fig.width=8, fig.height=4}
#For the models with the log-link
print(pdd$posterior_density$log)             
```

and logit functions.

```{r pddplot logit, fig.width=8, fig.height=4}
#For the models with the logit-link
print(pdd$posterior_density$logit)
```

And, a summary belonging to the figures.

```{r summary figures}
#summary belonging to the figures
print(pdd$summary)
```

For larger datasets with multiple groups and predictor variables, it may be useful to adjust the order of the Posterior Density Distributions (PDDs) for better clarity and comparison. You can control the ordering of predictors and groups by using the arguments order_predictor and order_group. These arguments expect a character vector containing the names of the predictors or groups in the desired order.

By adjusting these arguments, you can organize the PDDs in a way that facilitates easier interpretation, especially when working with complex models involving numerous predictors and groupings.

## hop-function (Hypothetical Outcome Plots)

Hypothetical Outcome Plots (HOPs) are a valuable tool for visualizing how the expected value of a response variable might change in response to variations in a predictor variable, while keeping all other variables constant (Kale et al., 2019).

In a HOP, each line represents the marginal change in the expected value of the response variable as the predictor variable changes. This allows for a clear understanding of how the relationship between the predictor and response behaves, without the influence of other variables.

$$
g(E(y_{i} \mid x_{ij)})) = \beta_{pooled,j=0,m} + \beta_{pooled,j=1,m} \cdot x_{i,j=1}+ \sum_{j=1}^{j} \beta_{pooled,j} \cdot \hat{x}_{j}
$$

The hypothetical prediction is generated using the posterior estimates, denoted as $f(\beta_{pooled}|Data, Info)$. This parameter reflects the influence of a one-unit change in the predictor on the response variable.

To display this change, a set of sequential values for the predictor $x_{i,j=1}$ can be generated where $x_{i,j=1}=\{i, ..., n\}$ represents realistic values for the observed gradient of the predictor. The function $f(\beta_{pooled,j}|Data, Info)$ provides the most plausible values for the pooled regression coefficients $\beta_{pooled,j}$. By drawing a sample $m$ from this distribution, $\beta_{pooled,j,m}\sim f(\beta_{pooled,j}|Data, Info)$ and repeating this process multiple times, a range of hypothetical outcomes can be generated.

For this process, all other parameters are held constant at their estimated values, $\sum_{j=1}^{j} \beta_{pooled,j}$, while the predictor $x_j$ is varied. Therefore, the HOP lines represent simulations of possible marginal changes in the response from the posterior distribution.

A slight difference from classical HOPs is that, in the case of meta-analysis, $\hat{x}=1$ is assumed. This approach still illustrates the marginal change in $y_i$ given a change in $x_{i,j}$
 , but only under the condition where all other predictors are set to their estimated values 
$\hat{x}_j=1$.

$$
g(E(y_{i} \mid x_{ij)})) = \beta_{pooled,j=0,m} + \beta_{pooled,j=1,m} \cdot x_{i,j=1}+ \sum_{j=1}^{j} \beta_{pooled,j} \cdot 1
$$

Note that the function operates under the assumption of log-transformed variables. This implies that the xlim argument, which defines the limits of the x-axis in the plot, is given in log-transformed units. For example, for the fraction of fine sediment, the log-transformation of the limits is exp(‚àí4.6)=0.01 and exp(0)=1. These values correspond to the range of the fraction of fine sediment on a logarithmic scale.

The y-axis, on the other hand, is presented on the response scale, meaning that the values are not transformed, and they represent the actual predicted responses (e.g., taxonomic richness in this case).

In the future, I plan to implement an argument that allows users to set the xlim on the response scale directly, without needing to manually convert to log-transformed values.

Below is an example of the outcomes showing the response of invertebrate taxonomic richness along the gradient of fine sediment.

```{r HOPs log salinity fig1, fig.width=8, fig.height=4,  warning=FALSE}
log_sal1 <- hop(mod2,                                  #Object from the meta function
    group="Invertebrates",                             #Select group to out of Invertebrates and Fish
    predictor = "Sediment",                            #Select group to out of Salinity and Oxygen
    xlab= "Fine sediment (%)",                         #Give x-axis a name
    link_function = "log",                             #Which link function out of log and logit
    ylab="Invertebrate taxonomic richness",            #Give y-axis a name
    xlim=c(-4.6, 0),                                   #Give y-axis a name
    ylim=c(0, 80),                                     #Set  limits y-axis
    hop_lwd = 0.3)                                     #Set width of the hop lines

#Display the HOPs
print(log_sal1)
```

It is also possible to scale the x-axis to display the exponentiated values using the argument `exp_axis`. This option will transform the values on the x-axis back from their log-transformed scale to their original scale, providing a more intuitive representation of the predictor variable.

Additionally, you may notice that the intercept could appear unusually high. This might be due to the fact that the intercept represents the average intercept across all studies, with all other variables held constant at a value of 1. If you wish to adjust the position of the intercept, you can use the `shift_b0` argument. This allows you to shift the intercept value for a more accurate representation based on the specific context of your analysis.

```{r HOPs log salinity fig2, fig.width=8, fig.height=4,  warning=FALSE,}
log_sal2 <- hop(mod2,                                   
    group="Invertebrates",                             
    predictor = "Sediment",                            
    xlab="Fine sediment (%)",   
    link_function = "log",                            
    ylab="Invertebrate taxonomic richness",            
    xlim=c(-4.6, 0),                                      
    ylim=c(0, 50),                                     
    hop_lwd = 0.3,                                    
    exp_axis = T,                                     #Exponentiate the x-axis notations
    round_x_axis = 2,                                 #Round the notation to full integers 
    shift_b0 = -1)                                    #Shift the intercept by -1     

#Display the HOPs
print(log_sal2)
```

## hop function (and Partial Dependency Plots)

When multiple predictors are included in the dataset, it is possible to visualize the effect of a change in one predictor while holding the other predictors constant. This can be achieved by creating Partial Dependency Plots (PDPs), which show how the predicted response variable changes as a particular predictor varies, keeping other predictors fixed.

The Partial Dependency Plot is a powerful tool for understanding the marginal effect of each predictor on the response variable. In the context of a meta-analysis, it helps illustrate how the relationship between a specific predictor and the response is shaped by the collective data from multiple studies, while controlling for the influence of other variables.

```{r HOPs log salinity fig3, fig.width=8, fig.height=4,  warning=FALSE,}
log_sal3 <- hop(mod2,                                              
    group="Invertebrates",                            
    predictor = c("Sediment", "Oxygen"),                     #Select both Sediment and Oxygen
    xlab= "Fine sediment fraction",                          #Give x-axis a name
    ylab= expression(Oxygen ~ mg ~ L^-1),                    #Give y-axis a name
    gradient_title = "MAP Invertebrate \ntaxonomic richness",#Give the y-axis gradient a name
    pdp_resolution = 100,                                    #Set resolution of the grid
    link_function = "log",
    exp_axis = T, 
    round_x_axis = 2,
    round_y_axis = 0,
    xlim=c(-4.61, -0.92),
    ylim=c(1.61, 2.77)) 

#Display the PDP
print(log_sal3)
```

## Prediction

This part is still under construction.


# References

Baguley, Thom. 2009. ‚ÄúStandardized or Simple Effect Size: What Should Be Reported?‚Äù British Journal of Psychology 100(3): 603‚Äì17. doi: 10.1348/000712608X377117.

Barwise, Jon, John Etchemendy, Gerard Allwein, Dave Barker-Plummer, and Albert Liu. 2002. Language, Proof, and Logic. Stanford, Calif: CSLI Publications.

Carthwright, Nancy, Jordi Cat, Lola Fleck, and E. Thomas Uebel. 1996. Otto Neurath: Philosophy between Science and Politics. Cambridge: Cambridge university press.

Cramer, J. S. 1991. The Logit Model: An Introduction for Economists. London: Edward Arnold.

Csill√©ry, Katalin, Michael G.B. Blum, Oscar E. Gaggiotti, and Olivier Fran√ßois. 2010. ‚ÄúApproximate Bayesian Computation (ABC) in Practice.‚Äù Trends in Ecology & Evolution 25(7): 410‚Äì18. doi: 10.1016/j.tree.2010.04.001.

Fisher, R. A. (1949). The Design of Experiments (5th ed.). Oliver and Boyd.

Gelman, Andrew, and Eric Loken. 2013. ‚ÄúThe Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‚ÄòFIshing Expedition‚Äô or ‚Äòp-Hacking‚Äô and the Research Hypothesis Was Posited Ahead of Time.‚Äù 348(3): 1‚Äì17. doi: 10.1007/978-3-658-12153-2_7.

Haack, Susan. 1978. Philosophy of Logics. 1st ed. Cambridge University Press. doi: 10.1017/CBO9780511812866.

Harman, Gilbert. 1986. Change in View: Principles of Reasoning. A Bradford Book, The MIT press Cambridge, Massachusetts, London England.

Hartig, Florian, Justin M. Calabrese, Bj√∂rn Reineking, Thorsten Wiegand, and Andreas Huth. 2011. ‚ÄúStatistical Inference for Stochastic Simulation Models - Theory and Application: Inference for Stochastic Simulation Models.‚Äù Ecology Letters 14(8): 816‚Äì27. doi: 10.1111/j.1461-0248.2011.01640.x.

Hinne, Max, Quentin F. Gronau, Don Van Den Bergh, and Eric-Jan Wagenmakers. 2020. ‚ÄúA Conceptual Introduction to Bayesian Model Averaging.‚Äù Advances in Methods and Practices in Psychological Science 3(2):200‚Äì215. doi: 10.1177/2515245919898657.

Hoeting, Jennifer A., David Madigan, Adrian E. Raftery, and Chris T. Volinsky. 1999. ‚ÄúBayesian Model Averaging: A Tutorial.‚Äù Statistical Science 14(4):382‚Äì417. doi: 10.1214/ss/1009212519.

Kale, Alex, Francis Nguyen, Matthew Kay, and Jessica Hullman. 2019. ‚ÄúHypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data.‚Äù IEEE Transactions on Visualization and Computer Graphics 25(1): 892‚Äì902. doi: 10.1109/TVCG.2018.2864909.

Lee, Siu-Fan. 2017. Logic: A Complete Introduction. Great Britain: Hodder & Stoughton.

Maier, Maximilian, Franti≈°ek Barto≈°, and Eric-Jan Wagenmakers. 2023. ‚ÄúRobust Bayesian Meta-Analysis: Addressing Publication Bias with Model-Averaging.‚Äù Psychological Methods 28(1): 107‚Äì22. doi: 10.1037/met0000405.

Mayo, D. G. (2018). Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. Cambridge University Press.

Moreno, Santiago G., Alex J. Sutton, Ae Ades, Tom D. Stanley, Keith R. Abrams, Jaime L. Peters, and Nicola J. Cooper. 2009. ‚ÄúAssessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study.‚Äù BMC Medical Research Methodology 9(1):2. doi: 10.1186/1471-2288-9-2.

Pearl, J. (2009). Causality. Cambridge university press.

Peters, Jaime L., Alex J. Sutton, David R. Sones, Keith R. Abrams, and Lesley Rushton. 2006. ‚ÄúComparison of Two Methods to Detect Publication Bias in Meta-Analysis.‚Äù JAMA 295(6):676. doi: 10.1001/jama.295.6.676.

Popper, Karl Raimund. 1968. The Logic of Scientific Discovery. Basic Books.

Smith, Peter. 2021. An Introduction to Formal Logic. Second edition, reprinted with corrections. Monee, IL: Logic Matters.

Stanley, T. D., and Hristos Doucouliagos. 2014. ‚ÄúMeta-Regression Approximations to Reduce Publication Selection Bias.‚Äù Research Synthesis Methods 5(1):60‚Äì78. doi: 10.1002/jrsm.1095.

Tukey, John W. 1969. ‚ÄúAnalysing Data: Sanctification or Detective Work?‚Äù American Psychologist 24(2): 83‚Äì91. doi: 10.1037/h0027108.

Tong, Christopher. 2019. ‚ÄúStatistical Inference Enables Bad Science; Statistical Thinking Enables Good Science.‚Äù The American Statistician 73(sup1): 246‚Äì61. doi: 10.1080/00031305.2018.1518264.

Us√≥-Dom√©nech, J. L., and J. Nescolarde-Selva. 2016. ‚ÄúWhat Are Belief Systems?‚Äù Foundations of Science 21(1): 147‚Äì52. doi: 10.1007/s10699-015-9409-z

Van Fraassen, Bas C. 1980. The Scientific Image. Oxford University Press.

Van Zwet, E.W., Cator, E.A., 2021. "The significance filter, the winner‚Äôs curse and the need to shrink." Stat. Neerlandica 75, 437‚Äì452. doi: 10.1111/stan.12241

Woolridge, Jeffery M. 2001. Econometric Analysis of Cross Section and Panel Data. Cambridge, Massachusetts, London, England: The MIT press.


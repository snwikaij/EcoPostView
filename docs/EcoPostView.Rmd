---
title: "EcoPostView: Ecological Posterior View"
author: Willem (Wim) Kaijser
output: 
  rmarkdown::html_vignette:
    toc: true      # Enable table of contents
    toc_depth: 2   # Define the depth of the table of contents
vignette: >
  %\VignetteIndexEntry{"EcoPostView: Ecological Posterior View"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE,}
options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Introduction

Ecological data is scattered around literature in different formats and very noisy. It is often 'case-study' specific and not representative for the totality if conditions encountered. However, as ecologist you are not interested in case specific situations, but making probability statements on general patterns, visualizing and predicting with them.

If working with meta-data and uncovering patterns from literature, making predictions and probability statements are your goal, this R-package can help make your life easier. The goal of this package is for you combine the benefits of multiple fitted Linear and Generalized Linear Models ((G)LMs), using the flexibility and strength of Bayesian Meta-analysis to generalize to a pooled (G)LM. For the differences strengths and weaknesses of both classical (Frequent-ism) or Bayesian approach see https://snwikaij.shinyapps.io/shiny/.

The information you are interested in is available from figures, tables, data sets or combinations between them. In simple words, we need a data set on which we can fit a (G)LM (see Kaijser et al, ...). This, however, will show that most data is very noisy, perhaps biased and the heterogeneity amount studies is large. This is an issue for a focus on error-control and causality statements. Both need very clearly a controlled environment with a well setup experimental conditions and exclusion of confounding variables. Hence, error-control and causality start a-priori (Fisher 1949, Pearl 2009, Mayo 2018). Of course this makes or information still useful to make probability statements or using the information to a-priori setup an study focused on error-control or causality study. However, the focus lies here on making posterior probability statements, visualization and prediction possible this information.

To start usgin the R-package both JAGS and devtools need to be installed. JAGS can be installed from https://sourceforge.net/projects/mcmc-jags/ and devtools can be install in R. For the most recent version of the EcoPostView it can be installed from GitHub. Of course, if nicely asked, any problems or possible improvements can be directed to me.

```{r setup}
#install.packages("devtools")
library(devtools)

#install.github("snwikaij/EcoPostView")
library(EcoPostView)
```

# 2. Obtaining data and fitting (G)LMs

To obtain data from the (G)LMs we need to fit models. This data can be obtained from figures (e.g., using WebPlotDigitizer), tables, data sets or combinations (See Kaijser et al. ...). What is needed to combine these models are the so called estimations of the model parameters. These are more commonly known as the intercept or slope. The intercept (*b0* or *β0*) and slope (*b1* or *β1*) can be used to predict: *link(response variable) = b0 + b1 · predictor variable*. In a GLM the response variable is linked to to the linear component with a link-function often notated as *g(..)*. The 'identity' link function applies no link function. This indicate that the mean (expected value notated as *E(y)* or *(E(y|x))*) is directly related to the linear component. However, in a GLM with, log- or logit-link it is easier to talk about log-linear relation 
$$log(E(y \mid x)) = \beta_0 + \beta_1 \cdot x$$
or logit-linear relation
$$logit(E(y \mid x)) = \beta_0 + \beta_1 \cdot x$$
The slope is not a true slope anymore because there is not straight line anymore between the response and predictor variable. Therefore the term coefficient or regression coefficient refers to the model parameter (*β*). When fitting a model it is convenient to note the source (e.g., DOI), the type of predictor variable (e.g., conductivity), group of the response type (e.g., benthic-invertebrates), link-function and if the model parameters is the intercept *b0* or a regression coefficient *b1*. If multiple predictor variables are fitted in a model all regression coefficients as notated as *b1* to distinguish it from the intercept *b0*.

Heuristically, I prefer working often working with elasticity- or semi-elasticity coefficients (Woolridge, 2001). But there are enough reasons not to do so and is dependent on the question and many other factors. The elasticity coefficient expresses the percentage change of $y$ (response) as relative to $x$ (predictor variable). Thus, 0.2 is 0.2% change in $y$ given 1% in $x$. Hence, for a log-linear model $log(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$
and thus $\beta_1 = \frac{\log(y)}{\log(x)}$. For the semi-elasticity coefficient (i.e., logit-linear) this only accounts partially and values closer 0 are better interpret able because $logit(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$ and thus $\beta_1 = \frac{logit(y)}{\log(x)}$. This expressed the  change in the log-odds per 1% elasticity (Cramer 1991; Woolridge, 2001). This makes it possible to coarsely compare among different models and still keep the interpretation of to the units needed for prediction. Hence, we can still predict with the model and interpret the expected change of $y$ given an increase in $1·log(x)$, because the units of $x$ are retain under the $log$.

I will keep these 'unofficial expression', due to the reference in the R-package to *b0* and *b1* while a proper notation would of course be more complex expressed as: 
$$g(E(y \mid x_{ij})) = \sum_{j=1}^{v} \beta_j \cdot x_{ij}$$
Where $x_{ij}$ refers to the $j$ the predictor variable (e.g., salinity is $j$=1 and light is $j$=2) and $i$ is the $i$-th observation. Here, $v$ represents the number of predictors. I will reduce the text to the basics.

```{r data}
data(example1)
head(example1)

```
In the example above the column **est** (estimate) indicates the model parameters and the column **se** the (standard error of the estimate). The column **group** can be an organism group or specific species (or anything you wish to group by), the column **predictor** the specific predictor variable, the parameter whether the estimate is the intercept (*b0*) or a regression coefficient (*b1*) and the column **link*** contains the link function. An additional and recommended option is to include the sample size **n** for adjusting for 'small-sample-effect' (Peters et al., 2006; Moreno et al., 2009).

# 3. The meta-function

## 3.1 Basic function

The meta function can include a random effect, by setting the argument RE=TRUE (default is TRUE). The  structure is then $\beta= \beta_{\text{pooled}} + u_i$. It has the option of placing a single or multiple random effect as a vector or matrix using the argument "random" the structure then becomes then $\beta = \beta_{\text{pooled}} + u_i +r_i$ for a single random effect. It can adjust for the relation between the $se$ and model parameters using the inverse of the standard error $1/se$ (method=1, Stanley and Doucouliagos, 2014). The structure is then $\beta = \beta_{\text{pooled}} + u_i + \alpha \cdot \left(\frac{1}{se}\right)$ or inverse of the sample size $1/n$ (method=2, the latter option is performed below) with the structure $\beta = \beta_{\text{pooled}} + u_i + \alpha \cdot \left(\frac{1}{n}\right)$. Of course if bias is considered minimal and neglect able non can be performed (method=1). I still would like to include a third 4th option to utilize Robust Bayesian Model Averaging (RoBMA: Maier et al. 2023). But this sometimes adjust extremely when including $1/se$ and therefore I left this option open for now.

```{r meta1}
mod1 <- meta(estimate=example1$est,         #Model estimate
             stderr=example1$se,            #Standard error of the model estimate
             parameter=example1$parameter,  #Model parameter (b0 or b1)
             predictor=example1$predictor,  #Predictor variable  (independent variable)
             link_function=example1$link,   #Link function
             grouping=example1$group,       #Group
             Nsamp=example1$n,              #Sample size
             method=2)                      #Adjustment method (0=none, 1=Egger's (1/se), 2=Peters (1/n))

#remove model to keep the environment clean
rm(mod1)

```

The meta-function returns a warnings that the MCMC-chains are not properly mixing. This can be an issue due to various reasons and assessed by looking at the 'raw' JAGS model output. I will not display the full output, but can be displayed wit the code below.
```{r meta1 warning}
#mod1$model$JAGS_model
```
We can see that for "mu[3]" the effective sample size is 610. Most of these issues can be resolved by thinning the chains or increasing the number of chains. In this case n_chain=6 solves the issue. Increasing the number of chains can be an issue if the number of available cores is limited as each chain is run on a separate core. Other options are to set "better informed" or "stronger" priors or combinations between them. Moreover, if the issue is not an issue of the estimate "mu" parameter in the raw JAGS output, the issue could be considered to be ignored. These choices are ultimately up to the user. An option to prevent warnings would be to set the warning level for Eff_warn lower i.e., Eff_warn = 500.
```{r meta2}
mod2 <-meta(estimate=example1$est,        
            stderr=example1$se,            
            parameter=example1$parameter,  
            predictor=example1$predictor,  
            link_function=example1$link,   
            grouping=example1$group,       
            Nsamp=example1$n,              
            method=2,
            n_chain=6)                   #Increasing the number of chains from 2 to 6   

#remove model to keep the environment clean
rm(mod2)

```

## 3.2 Setting priors
A benefit of the the Bayesian approach is that we can included prior information. Thereby explicitly shifting the weight of the outcome to more plausible values of the pooled model parameter or less weight to less plausible values. To set only a single prior for each relation and parameter a specific structure is needed. By default the model parameters are set as Gaussian/normally distributed with a mean ($\mu$, prior_mu) of 0 and $se$ (prior_mu_se) of 0.5. The prior for the standard deviation is uniform where only the maximum value (prior_sigma_max) can be set, which at default is 5. As a strongly critical side note. The priors do not need to stay the same, because I "heuristically" like to work with the elasticity- or semi-elasticity coefficients. THIS IS NOT A NECESSITY. Hence, not thinking about your prior even if not a lot of information is available to make an very strong prior (but at least informed) is often not doing a Bayesian analysis at all. 

To get a nice table overview which can be adjusted an filled in with prior information the argument 'get_prior_only' can be set to 'TRUE'. This will return a dataframe, with a column **level** names, priors for the **mu** ($\mu$) and for the standard error ($se$). The values in this can be set manually and re-included as a vector. How to do this will be explained in more detail in section 3.3.

``` {r meta2 example priors}
only_priors <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,       
                    Nsamp=example1$n,            
                    method=2,
                    n_chain=6,
                    get_prior_only=TRUE) #Only show the structure of the priors

print(only_priors)

#remove data frame of priors to keep environment clean
rm(only_priors)
```

## 3.3 Setting multiple priors for Bayesian Model Averaging

A benefit of the the Bayesian approach is that we can included multiple priors to perform Bayesian Model Averaging (BMA;  Hoeting et al., 1999; Hinne et al., 2020). Thereby explicitly adding multiple possible scenarios that could have generated the observable data and aggregating this possibilities. To set multiple priors we need a similar dataset but also include prior weights. A simple solution to the prior weights to weight each prior as equally reasonable. In the example below there are three priors for each possible model. An the prior weights are set as 1/3.

The results are summarized in a table describing Maximum A Priori values (MAP), the $\mu$, $se$ and the high density intervals at default (93.1%). Additionally the $I2$ heterogeneity is given.

``` {r meta3 BMA}
data("example2")
print(example2)

mod3 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu = example2[c(2,4,6)],          #Prior for the mean
                    prior_mu_se = example2[c(3,5,7)],       #Prior for the standard error of the mean
                    prior_weights = c(1/3, 1/3, 1/3),       #Prior weights
                    Nsamp=example1$n,            
                    method=2,
                    n_chain = 6)

#Display the summarized results
mod3$Summary

#remove data frame of priors to keep environment clean
rm(mod3)
```
This is the most difficult part where the data needs to be gathered and extracted, (G)LMs need to be fitted, priors need to formulated and the whole model needs to be optimized to get a stable results. The display of the results requires less time.

# References

Cramer, J. S. 1991. The Logit Model: An Introduction for Economists. London: Edward Arnold.

Hinne, Max, Quentin F. Gronau, Don Van Den Bergh, and Eric-Jan Wagenmakers. 2020. “A Conceptual Introduction to Bayesian Model Averaging.” Advances in Methods and Practices in Psychological Science 3(2):200–215. doi: 10.1177/2515245919898657.

Hoeting, Jennifer A., David Madigan, Adrian E. Raftery, and Chris T. Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science 14(4):382–417. doi: 10.1214/ss/1009212519.

Maier, Maximilian, František Bartoš, and Eric-Jan Wagenmakers. 2023. “Robust Bayesian Meta-Analysis: Addressing Publication Bias with Model-Averaging.” Psychological Methods 28(1): 107–22. doi:10.1037/met0000405.

Moreno, Santiago G., Alex J. Sutton, Ae Ades, Tom D. Stanley, Keith R. Abrams, Jaime L. Peters, and Nicola J. Cooper. 2009. “Assessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study.” BMC Medical Research Methodology 9(1):2. doi: 10.1186/1471-2288-9-2.

Peters, Jaime L., Alex J. Sutton, David R. Sones, Keith R. Abrams, and Lesley Rushton. 2006. “Comparison of Two Methods to Detect Publication Bias in Meta-Analysis.” JAMA 295(6):676. doi: 10.1001/jama.295.6.676.

Stanley, T. D., and Hristos Doucouliagos. 2014. “Meta-Regression Approximations to Reduce Publication Selection Bias.” Research Synthesis Methods 5(1):60–78. doi: 10.1002/jrsm.1095.

Woolridge, Jeffery M. 2001. Econometric Analysis of Cross Section and Panel Data. Cambridge, Massachusetts, London, England: The MIT press.


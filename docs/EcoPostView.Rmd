---
title: "EcoPostView: Ecological Posterior View"
author: Willem (Wim) Kaijser
output: 
  rmarkdown::html_vignette:
    toc: true      
    toc_depth: 2
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{"EcoPostView: Ecological Posterior View"}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, include = FALSE,}
options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Introduction

Ecological data is scattered around literature in different formats and very noisy. It is often 'case-study' specific and not representative for the totality if conditions encountered. However, as ecologist we are not interested in case specific situations, but making probability statements on general effects, visualizing and predicting with them.

If working with meta-data and uncovering effects from literature, making probability statements and predictions are the goal, this R-package can help make your life easier. The goal of this package is to combine the benefits of multiple fitted Linear and Generalized Linear Models ((G)LMs), using the flexibility and strength of Bayesian meta-analysis to generalized the effects to one pooled (G)LM. This meta-analytically derived (G)LM can be visualized (see the Fig. 1 below) and used to predict on new data.

```{r intro-plot,  fig.width=7.1, fig.height=4, echo=F}
library(EcoPostView)
library(ggplot2)
library(cowplot)
data("example1")


mod_intro <- meta(estimate=example1$est,         
                  stderr=example1$se,            
                  parameter=example1$parameter,  
                  predictor=example1$predictor,  
                  link_function=example1$link,   
                  grouping=example1$group,       
                  Nsamp=example1$n,
                  n_chain=6,
                  n_thin=10,
                  method=2)

pl1 <- hop(mod_intro ,                                              
           group = "Invertebrates",                            
           predictor = c("Salinity", "Sediment"),                   
           xlab = expression(Conductivity ~ µS ~ cm^-1),                          
           ylab = "Fine sediment fraction",                   
           pdp_resolution = 100,                                   
           link_function = "logit",
           exp_disp = T, 
           round_x_axis = 0,
           round_y_axis = 2,
           xlim=c(2.99, 6.215), 
           ylim=c(-4.61, -0.92))
pl1 <- pl1+ggplot2::theme(legend.position = "none")

pl2 <- hop(mod_intro ,                                              
           group = "Invertebrates",                            
           predictor = c("Salinity", "Sediment"),                   
           xlab = expression(Conductivity ~ µS ~ cm^-1),                          
           ylab = "Fine sediment fraction",                   
           gradient_title = "Evenness \naquatic \ninvertebrates",
           pdp_resolution = 100,                                   
           link_function = "logit",
           exp_axis = T, 
           round_x_axis = 0,
           round_y_axis = 2,
           xlim=c(2.99, 6.215), 
           ylim=c(-4.61, -0.92))
pl2 <- pl2+ggplot2::theme(axis.title.y = element_blank())

intro_plot <- cowplot::plot_grid(pl1, pl2, rel_widths = c(0.43, 0.57))
print(intro_plot)
rm(mod_intro)
```
*Figure 1: Expected evenness of aquatic invertebrates as function of conductivity and fine sediment fraction with a river reach. Left figure display the relation on the response scale while the right figure displays it on the log scale, which gives a better view.*

A large amount of information is available from figures, tables, data sets or combinations between them. In simple words, multiple datasets are needed on which (G)LMs can be fitted (see Kaijser et al, ...). This, however, will show that most data is very noisy, perhaps biased and the heterogeneity among different studies is large. This is an issue if we focus on error-control and causality statements. Both error-control and causality need controlled environment with a well setup experimental conditions and exclusion or modeling of confounding variables. Hence, error-control and causality start a-priori (Fisher 1949, Pearl 2009, Mayo 2018). For an explanation on the topic of error-control connected to the classical (Frequentism) approach see https://snwikaij.shinyapps.io/shiny/. Yet, while error-control a posterior is not possible anymore, this makes the information still useful. It can be used to make probability statements or used to setup a new experiment, focused on error-control or causality. This packages focuses on making posterior probability statements, visualization and prediction from information available from literature. Hence, the goal is to generalize ecological effects from literature, displaying its patterns and predict with it.

# 2. Data and models

## 2.1 (Generalized) Linear Models (G)LMs and extracting data

To use this package data is needed and on this data (G)LMs we need to fitted. This (meta-)data can be obtained from figures (e.g., using WebPlotDigitizer), tables, data sets or combinations between them (See Kaijser et al. ...). The effects we are talking of are not really effects but model parameters. These parameters are more commonly known as the intercept or slope. The intercept (*b0* or *β0*) and slope (*b1* or *β1*) can be used to predict: *response variable = b0 + b1 · predictor variable*. By knowing *b0* and *b1* we can give the model a single or multiple new observations (*i*) values for *x* and make a prediction. In a GLM the response variable is linked to to the linear component with a link-function often notated as *g(..)*. The 'identity' link function applies no link function. This indicates that the mean (expected value of *y* notated as *E(y)* or *(E(y|x))*) is directly related to the linear component.
$$g(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$ 
However, in a GLM with, log- or logit-link it is easier to talk about log-linear relation 
$$log(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$ 
or logit-linear relation
$$logit(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$
The slope in a GLM not a true slope anymore because there is not straight line anymore that links the response (*y*) and predictor variable (*x*). Therefore the term coefficient or regression coefficient refers to the model parameter ($\beta$). 

Heuristically, I prefer working often working with elasticity- or semi-elasticity coefficients (Woolridge, 2001). In this example I also use the elasticity- or semi-elasticity coefficients. But, there are enough reasons not to do so and is dependent on the question, data and many other factors. 

The elasticity coefficient expresses the percentage change of $y$ (response) as relative to $x$ (predictor variable). Thus, 0.2 is 0.2% change in $y$ given 1% in $x$. Hence, for a log-linear model $log(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$
and thus $\beta_1 = \frac{\log(y)}{\log(x)}$. For the semi-elasticity coefficient (i.e., logit-linear) this only accounts partially and values closer 0 are better interpret able because $logit(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$ and thus $\beta_1 = \frac{logit(y)}{\log(x)}$. This expressed the  change in the log-odds per 1% elasticity (Cramer 1991; Woolridge, 2001). This makes it possible to coarsely compare among different models and still keep the interpretation of to the units needed for prediction. Hence, we can still predict with the model and interpret the expected change of $y$ given an increase in $1·log(x)$, because the units of $x$ are retain under the $log$.

Keep these 'unofficial expressions' in mind, due to the reference in the R-package to *b0* and *b1* while a proper notation would of course be more formally expressed as: 
$$g(E(y_{i} \mid x_{ij})) = \sum_{j=1}^{j} \beta_j \cdot x_{ij}$$
Where $x_{ij}$ refers to the $j$ the predictor variable (e.g., salinity is $j$=1 and light is $j$=2) and $i$ is the $i$-th observation.

## 2.2. Bayes Theorem

Since this R-package is mostly using Bayesian inference an introduction to Bayes Theorem might be of interested. While it is not necessary need for the utilization of the R-package. If this is not of interest one could skip this part. Perhaps it is more to formulate my own "idea" of it. Informally Bayes Theorem would be notated $\text{Posterior}\, \text{probability} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Evidence}}$. More formally Bayes theorem is often notated with A and B where P indicates probability and '|' given or conditional on.$P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}$. However, to connect it to models $A=\beta$ and $B=Data$. Where $\beta$ denotes the model parameter and $Data$ the available data.
The derivation of Bayes theorem relies on the exchangeability of probabilities, while classically 'A' and 'B' are used for reference to the text where A = $\beta$ and B = $Data$.

**Premise 1)**
$$
P(\beta | \text{Data}) = \frac{P(\beta \cap \text{Data})}{P(\text{Data})}
$$
similarly
$$
P(\text{Data} | \beta) = \frac{P(\text{Data} \cap \beta)}{P(\beta)}
$$
**Premise 2)**
Also, the joint probability is expressed as a set-theoretic relationship indicating that element of both sets is the same.
$$
z = \{x : x \in \beta \cap Data : x \in Data \cap \beta\}
$$
thus
$$
P(\beta \cap Data) = P(Data \cap \beta)
$$
**Premise 3)**
Thus:
$$
P(\beta | Data) \cdot P(\beta) = P(\beta \cap Data)
$$
and
$$
P(Data | \beta) \cdot P(Data) = P(Data \cap \beta)
$$
**Conclusion)**
Therefore:
$$
P(\beta | Data) \cdot P(\beta) = P(Data | \beta) \cdot P(Data)
$$
$$
P(\beta | Data) = \frac{P(Data | \beta) \cdot P(\beta)}{P(Data)}
$$

The issue is that here the subjectivity of the prior and the function '$f$' of the probability distribution are not that transparent in my honest opinion. where a function is nothing more than a 'rule' that maps $x$ to $y$ and so $y=f(x)$. Hence, probability is a mapping function, mapping an expectation of $x$ to $y$. The probability can be any function (although some might disagree). In this case the prior probability is a function of both the $Data$ and prior information ($Info$). 
$$
f(\beta \mid Data, Info) = 
\frac{f(Data \mid \beta) \cdot f(\beta \mid Info)}
{\int f(Data \mid \beta) \cdot f(\beta \mid Info)}
$$
This expression is sometimes simplified to
$$f(\beta \mid Data, Info) = f(Data \mid \beta) \propto f(\beta \mid Info)$$
Where the $\propto$ symbol indicates 'proportional to' highlighting the idea of exchangeability. Therefore, the posterior nothing more than a function that describes the probability $y$ as a function of $\beta$ conditional on $Data$ and $Info$ ($y=f(\beta \mid Data, Info)$). This cannot be solely conditional on the $Data$ as the $Data$ is not uncertain our information/believe is uncertain about a none existing object $\beta$ (unless Platonism is true). Hence, even if we were wrong or correct about $f(\beta \mid Data, Info)$ nothing would change in reality at all. Hence, uncertainty has nothing to do with reality at all. It becomes now clear that only our own error/information is to be optimized as contrary to frequentism where this error is that of the long-run. Hence, the frequentist cannot say something about the prior (hypothesis), it cannot reject hypothesis (which seem to mean that it is false) and it cannot state something about the probability of the parameter (effect) or observations "by chance alone". The strength of the frequentism lies in its objectivity and solidarity (in the long-run), which is, however, not used.
Unfortunately the later is difficult in a world where mistakes are not allowed and "excellence" is the goal that each study has to portray (not a repeated long-run of studies). Hence, models are only useful if their current errors and failures in forecasting future phenomena are openly displayed, allowing us to optimize our understanding of predictive uncertainty until it becomes *acceptable*.

Bayesian Model averaging has the advantages that it allows multiple ($k$) functions to be utilized as prior. Hence, multiple possible scenarios that could have been responsible for $\beta$ can be introduced as below.
$$
f(\beta \mid Data,Info) = \frac{f(Data \mid \beta) \cdot f_k(\beta \mid Info)}{\int \left( \sum_{k=1}^{k} f(Data \mid \beta) \cdot f_k(\beta \mid Info) \right)}
$$
Now it should be clear that each $\beta$ contained within $g(E(y \mid x_{ij})) = \sum_{j=1}^{v} \beta_j \cdot x_{ij}$ is being restricted by the prior models. While in frequentism it is unrestricted and 'complete indifference' towards the  possibility of $\beta$. 

In a meta-analysis we do not talk about $\beta$ but about a set of estimates $\beta=\{\beta_{i}, ..., \beta_{n}\}$ meaning that $f(Meta-data\mid\{\beta_{i}, ..., \beta_{n}\})$. Hereby the flexibility allows that these estimates are either likelihood estimates ($\hat{\beta}$) or  posterior estimates ($\beta$). and we end up with an expression that should capture the inference to an underlying pooled model parameter.
$$
f(\beta_{poolded} \mid Meta-data,Info) = \frac{f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\}) \cdot f_k(\beta_{pooled} \mid Info)}{\int \left( \sum_{k=1}^{k} f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\}) \cdot f_k(\beta_{pooled} \mid Info) \right)}
$$

## 2.3 (G)LMs and Bayes Theorem 

Perhaps seeing a (G)LM or Bayes Theorem does not immediately lead to the understanding of how this connects. In simplicity Bayes Theorem is used to find the posterior value of the model parameters. In this example $\beta_{0}$ and $\beta_{1}$.
$$
\begin{align*}
&g(E(y_{i} \mid x_{i})) \hspace{20cm} \\
&=  \\
&f(\beta_{0} \mid Data, Info) \sim f(Data \mid \beta_{0}) \propto f(\beta_{0} \mid Info) \\ 
&+ \\
&f(\beta_{1} \mid Data, Info) \sim f(Data \mid \beta_{1}) \propto f(\beta_{1} \mid Info) \\
&\cdot \\
&x_{i} 
&\end{align*}
$$
Hence, each parameter has its likelihood and prior to optimize between. More complex hierarchical models can be created where $\beta$ can be conditional on other parameters and even $x$ can be modeled scholastically. For now this suffices. 

# 3. The meta-function

## 3.1 Data
To start using this R-package both JAGS and devtools need to be installed. JAGS can be installed from https://sourceforge.net/projects/mcmc-jags/ and devtools can be install in R. For the most recent version of the EcoPostView it can be installed from GitHub. Of course, any problems, questions or possible improvements can be directed to me.

```{r setup}
#install.packages("devtools")
library(devtools)

#install.github("snwikaij/EcoPostView")
library(EcoPostView)
```

From here I assume multiple (G)LMs were fitted and from these fitted models the parameter estimates and the standard error were gathered and stored in a dataset. In connection to these parameter estimates it is convenient to note the source (e.g., DOI), the type of predictor variable (e.g., conductivity), group of the response type (e.g., benthic-invertebrates), link-function and if the model parameters is the intercept *b0* or a regression coefficient *b1*. If multiple predictor variables are fitted in a model all regression coefficients as notated as *b1* to distinguish it from the intercept *b0*. The example in R shows the structure of the data frame.

```{r data}
data(example1)
head(example1)

```
In the example above the column **est** (estimate) indicates the estimated model parameters and the column **se** the standard error of the estimate. The column **group** can be an organism group or specific species/taxon (or anything you wish to group by), the column **predictor** the specific predictor variable, the parameter whether the estimate is the intercept (*b0*) or a regression coefficient (*b1*) and the column **link** contains the link function. An additional and recommended option is to include the sample size **n** for adjusting for 'small-sample-effects' (Peters et al., 2006; Moreno et al., 2009).

## 3.2 Basic model structure

The meta function can include a random effect, by setting the argument RE=TRUE (default is TRUE). The  structure is then 
$$\{\beta_{i}, ..., \beta_{n}\}= \beta_{\text{pooled}} + u_i$$
It has the option of placing a single or multiple random effect as a vector or matrix using the argument 'random' the structure then becomes then 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i +r_i$$
for a single random effect. It can adjust for the relation between the $se$ and model parameters using the inverse of the standard error $1/se$ (method=1, Stanley and Doucouliagos, 2014). The structure is then 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i + \alpha_{i} \cdot \left(\frac{1}{se}\right)$$ 
or inverse of the sample size $1/n$ (method=2, the latter option is performed below) with the structure 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i + \alpha_{i} \cdot \left(\frac{1}{n}\right)$$ 
Of course if bias is considered neglect non can be performed (method=0). I still would like to include a third 4th option to utilize Robust Bayesian Model Averaging (RoBMA: Maier et al. 2023). But this sometimes adjust extremely when including $1/se$ and therefore I left this option open for now.

```{r meta1}
mod1 <- meta(estimate=example1$est,         #Model estimate
             stderr=example1$se,            #Standard error of the model estimate
             parameter=example1$parameter,  #Model parameter (b0 or b1)
             predictor=example1$predictor,  #Predictor variable  (independent variable)
             link_function=example1$link,   #Link function
             grouping=example1$group,       #Group
             Nsamp=example1$n,              #Sample size
             method=2)                      #Adjustment method (0=none, 1=Egger's (1/se), 2=Peters (1/n))

#remove model to keep the environment clean
rm(mod1)

```

The meta-function can return a warning that the MCMC-chains are not properly mixing. This can be an issue due to various reasons. Where this warning originates from can be assessed by looking at the 'raw' JAGS model output (`mod1$model$JAGS_model`). This could show that a parameter of interested 'mu[.]' Has a a large Rhat or small effective sample size. Most of these issues can be resolved by thinning the chains, increasing the number of chains or setting more informed or stronger priors. Moreover, if the issue is not an issue of the estimated 'mu' parameter, it could be decided to ignore it. These choices are ultimately up to the user. An option to prevent warnings would be to set the warning level for Eff_warn lower i.e., Eff_warn = 500.

## 3.3 Setting priors

A benefit of the the Bayesian approach is that we can included prior information. Thereby explicitly shifting the weight of the outcome to more plausible values of the pooled model parameter. To set only a single prior for each relation and parameter a specific structure is needed. By default the model parameters are set as Gaussian/normally distributed with a mean ($\mu$, prior_mu) of 0 and standard error ($se$, prior_mu_se) of 0.5. The prior distribution is at the moment restricted to the normal distribution. The prior for the standard deviation ($\sigma$) is uniform where only the maximum value (prior_sigma_max) can be set, which at default is 5. 

As a strongly critical side note. The priors do not need to stay the same, because I "heuristically" like to work with the elasticity- or semi-elasticity coefficients. THIS IS NOT A NECESSITY! Hence, not thinking about your prior even if not a lot of information is available to make an very strong prior (but at least informed) is often not doing a Bayesian analysis at all. 

To get a nice table overview of the structure need for the priors can be obtained by setting the 'get_prior_only' to 'TRUE'. This will return a dataframe, with a column **level**, priors for the **mu** ($\mu$) and for the standard error ($se$). The numbers can adjusted to the specific topic based on prior information available. How to included this dataframe will be explained in more detail in section 3.3.

``` {r meta2 example priors}
only_priors <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,       
                    Nsamp=example1$n,            
                    method=2,
                    get_prior_only=TRUE) #Only show the structure of the priors

print(only_priors)

#remove data frame of priors to keep environment clean
rm(only_priors)
```

## 3.4 Setting multiple priors for Bayesian Model Averaging

A benefit of the the Bayesian approach is that we can included multiple ($k$) priors to perform Bayesian Model Averaging (BMA;  Hoeting et al., 1999; Hinne et al., 2020). Thereby explicitly adding multiple possible scenarios that could have generated the observable data and averaging over these plausible explanation. To set multiple priors we need a similar dataset as shown above, but also include prior weights.These prior weights determine how heavy we think this prior model is reasonable. These prior weights could be given any number between 0 (0%) and 1 (100%) so that it preferably adds up to 100%. In the example below there are three priors for each possible model each set as 1/3 (0.33 is also possible). Hence each column vector in example2 is weighted 1/3. The different priors have been set differently, and a broader prior for the intercept is reasonable.

``` {r meta3 BMA}
data("example2")
print(example2)

mod2 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu=example2[c(2,4,6)],          #prior for the mean
                    prior_mu_se=example2[c(3,5,7)],       #prior for the standard error of the mean
                    prior_weights=c(1/3, 1/3, 1/3),       #prior weights
                    Nsamp=example1$n,            
                    method=2,
                    n_thin=10,                            #thinning the chains
                    n_chain=4)                            #changing the number of chains from 2 to 4

#Display the summarized results
mod2$Summary
```
The results of the meta-analysis are summarized in a table describing Maximum A Priori values (MAP), the $\mu$, $se$ and the high density intervals at default (90%). Additionally the $I2$ heterogeneity is given.

This is the most difficult part where the data needs to be gathered and extracted, (G)LMs need to be fitted, priors need to formulated and the whole model needs to be optimized to get a stable results. To display or predict with the models requires less time.

# 4. Display

## 4.1 pdplot-function

To display the posterior results one could display the results with a point and interval range. However, this interval displaying a strong boundary on a continues set of possibilities does not completely capture  the idea of the posterior probability distribution. Another option would be to display the Posterior Density Distribution (PDD) combined with the point and interval estimate. Hence, the PDD displays the possible values of the prior information where the pooled estimated might have originated from $f(\beta_{\text{pooled}} \mid Meta-data, Info)$ it is the inverse of the likelihood which informs us of the values of the meta-data given we selected a particular pooled estimate $f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\})$. This can be  performed with the pdplot function.

```{r pdplot}
pdd <- pdplot(mod2, 
              label_size=4,      #setting the label size larger
              point_size=2)      #large point
```

The object contains the figures generated for both the log

```{r pddplot log, fig.width=8, fig.height=4, echo=FALSE}
#for the models with the log-link
print(pdd$posterior_density$log)             
```

and logit functions

```{r pddplot logit, fig.width=8, fig.height=4, echo=FALSE}
#for the models with the logit-link
print(pdd$posterior_density$logit)
```

and a summary belonging to the figures

```{r summary figures}
#summary belonging to the figures
print(pdd$summary)
```

For larger datasets with multiple groups and predictor variables the order of the PDDs can be changed by using the arguments 'order_predictor' and 'order_group"' needing a character string with the names of the desired order.

## 4.2 hop-function (Hypothetical Outcome Plots)

Hypothetical Outcome Plots (HOPs) are a powerful method to visualize the possibilities the expected value could take on given a change in the predictor variable (Kale et al. 2019). The HOPs lines are a simulation of possibilities from the posterior. Note that the function works under the idea of log transformed variables. This means that xlim (the limits of the figures) as given below are exp(3)=20 and exp(8)=2980 for conductivity. The y-axis is on the response scale. I hop soon to make it possible to use an argument that makes it possible to set the xlim on the raw. Below an example of the outcomes of the response of invertebrate taxonomic richness along  the conductivity gradient. 

```{r HOPs log salinity fig1}
log_sal1 <- hop(mod2,                                  #object from the meta function
    group="Invertebrates",                             #select group to out of Invertebrates and Fish
    predictor = "Salinity",                            #select group to out of Salinity and Oxygen
    xlab= expression(Ln(Conductivity ~ µS ~ cm^-1)),   #give x-axis a name
    link_function = "log",                             #which link function out of log and logit
    ylab="Invertebrate taxonomic richness",            #give y-axis a name
    xlim=c(3, 8),                                      #give y-axis a name
    ylim=c(0, 25),                                     #set  limits y-axis
    hop_lwd = 0.3)                                     #set width of the hop lines
```

```{r fig log_sal1, fig.width=7.1, fig.height=4, echo=FALSE}
print(log_sal1)
```

It is also possible to scale to display the exponentiate values on the x-axis using the argument
'exp_axis'. The observant reader might notice the taxonomic richness is ~15 under lower conductivity. This is correct, because the majority of the literature publish does not focus on species richness, it often focuses on order or family richness.

```{r HOPs log salinity fig2}
log_sal2 <- hop(mod2,                                   
    group="Invertebrates",                             
    predictor = "Salinity",                            
    xlab= expression(Conductivity ~ µS ~ cm^-1),   
    link_function = "log",                            
    ylab="Invertebrate taxonomic richness",            
    xlim=c(3, 8),                                      
    ylim=c(0, 25),                                     
    hop_lwd = 0.3,                                    
    exp_axis = T,                                     #exponentiate the x-axis notations
    round_x_axis = 0)                                 #round the notation to full integers        
```

```{r fig log_sal2, fig.width=7.1, fig.height=4, echo=FALSE}
print(log_sal2)
```

## 4.3 hop function (and Partial Dependency Plots)

There are multiple predictors is the dataset. Therefore it possible to display the MAP predicted value given a change in one of the two predictors. This can be performed by display in Partial Dependency Plots.

```{r HOPs log salinity fig3}
log_sal3 <- hop(mod2,                                              
    group="Invertebrates",                            
    predictor = c("Sediment", "Oxygen"),                     #select both Sediment and Oxygen
    xlab= "Fine sediment fraction",                          #give x-axis a name
    ylab= expression(Oxygen ~ mg ~ L^-1),                    #give y-axis a name
    gradient_title = "MAP Invertebrate \ntaxonomic richness",#give the y-axis gradient a name
    pdp_resolution = 100,                                    #set resolution of the grid
    link_function = "log",
    exp_axis = T, 
    round_x_axis = 2,
    round_y_axis = 0,
    xlim=c(-4.61, -0.92),
    ylim=c(1.61, 2.89)) 
```

```{r fig log_sal3, fig.width=7.1, fig.height=4, echo=FALSE}
print(log_sal3)
```

# 5. Prediction

This part is still under construction.

# References

Cramer, J. S. 1991. The Logit Model: An Introduction for Economists. London: Edward Arnold.

Fisher, R. A. (1949). The Design of Experiments (5th ed.). Oliver and Boyd.

Hinne, Max, Quentin F. Gronau, Don Van Den Bergh, and Eric-Jan Wagenmakers. 2020. “A Conceptual Introduction to Bayesian Model Averaging.” Advances in Methods and Practices in Psychological Science 3(2):200–215. doi: 10.1177/2515245919898657.

Hoeting, Jennifer A., David Madigan, Adrian E. Raftery, and Chris T. Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science 14(4):382–417. doi: 10.1214/ss/1009212519.

Kale, Alex, Francis Nguyen, Matthew Kay, and Jessica Hullman. 2019. “Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data.” IEEE Transactions on Visualization and Computer Graphics 25(1): 892–902. doi: 10.1109/TVCG.2018.2864909.

Maier, Maximilian, František Bartoš, and Eric-Jan Wagenmakers. 2023. “Robust Bayesian Meta-Analysis: Addressing Publication Bias with Model-Averaging.” Psychological Methods 28(1): 107–22. doi: 10.1037/met0000405.

Mayo, D. G. (2018). Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. Cambridge University Press.

Moreno, Santiago G., Alex J. Sutton, Ae Ades, Tom D. Stanley, Keith R. Abrams, Jaime L. Peters, and Nicola J. Cooper. 2009. “Assessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study.” BMC Medical Research Methodology 9(1):2. doi: 10.1186/1471-2288-9-2.

Pearl, J. (2009). Causality. Cambridge university press.

Peters, Jaime L., Alex J. Sutton, David R. Sones, Keith R. Abrams, and Lesley Rushton. 2006. “Comparison of Two Methods to Detect Publication Bias in Meta-Analysis.” JAMA 295(6):676. doi: 10.1001/jama.295.6.676.

Stanley, T. D., and Hristos Doucouliagos. 2014. “Meta-Regression Approximations to Reduce Publication Selection Bias.” Research Synthesis Methods 5(1):60–78. doi: 10.1002/jrsm.1095.

Woolridge, Jeffery M. 2001. Econometric Analysis of Cross Section and Panel Data. Cambridge, Massachusetts, London, England: The MIT press.


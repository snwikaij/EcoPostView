---
title: "EcoPostView: Ecological Posterior View"
author: Willem (Wim) Kaijser
output: 
  rmarkdown::html_vignette:
    toc: true      
    toc_depth: 2
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{"EcoPostView: Ecological Posterior View"}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, include = FALSE}
options(rmarkdown.html_vignette.check_title = FALSE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Introduction

Ecological data is scattered around literature in different formats. It is often 'case-study' specific and not representative for the totality if conditions encountered. Therefore, a lot of noise is introduced in the data. Furthermore, there exist no framework to re-use, generalize and fuse large amounts data into an understandable and predictive models hampering ecological knowledge by wasting valuable information. Hence, as ecologist we are not interested in only case specific situations and explaining away after the event happens. We want our information to cumulatively add to a larger body of knowledge and apply it.

Working with logic, meta-data and stochastic process can solidify our understanding of ecological relations. It can be used to make probability statements and predictions. This R-package can help make your life easier to do so. The goal of this package is to provide help to do so combine the benefits of multiple source of information. The current main function uses the information of multiple fitted Linear and Generalized Linear Models ((G)LMs) and the flexibility and strength of Bayesian methods to generalized the absolute effects of multiple (G)LMs to pool it in to one meta-analytic (G)LM. This meta-analytically derived (G)LM can be visualized (see the Fig. 1 below) and used to predict on new data. Hence, it is meant to get a reasonable generalization picture of ecological relation from using all possible sources of information. 

```{r intro-plot,  fig.width=7.1, fig.height=4, echo=F, warning=FALSE}
library(EcoPostView)
library(ggplot2)
library(cowplot)
data("example1")

mod_intro <- meta(estimate=example1$est,         
                  stderr=example1$se,            
                  parameter=example1$parameter,  
                  predictor=example1$predictor,  
                  link_function=example1$link,   
                  grouping=example1$group,       
                  Nsamp=example1$n,
                  n_chain=6,
                  n_thin=10,
                  method=2)

pl1 <- hop(mod_intro ,                                              
           group = "Invertebrates",                            
           predictor = c("Salinity", "Sediment"),                   
           xlab = expression(Conductivity ~ µS ~ cm^-1),                          
           ylab = "Fine sediment fraction",                   
           pdp_resolution = 100,                                   
           link_function = "logit",
           exp_disp = T, 
           round_x_axis = 0,
           round_y_axis = 2,
           xlim=c(2.99, 6.215), 
           ylim=c(-4.61, -0.92))
pl1 <- pl1+ggplot2::theme(legend.position = "none")

pl2 <- hop(mod_intro ,                                              
           group = "Invertebrates",                            
           predictor = c("Salinity", "Sediment"),                   
           xlab = expression(Conductivity ~ µS ~ cm^-1),                          
           ylab = "Fine sediment fraction",                   
           gradient_title = "Evenness \naquatic \ninvertebrates",
           pdp_resolution = 100,                                   
           link_function = "logit",
           exp_axis = T, 
           round_x_axis = 0,
           round_y_axis = 2,
           xlim=c(2.99, 6.215), 
           ylim=c(-4.61, -0.92))
pl2 <- pl2+ggplot2::theme(axis.title.y = element_blank())

intro_plot <- cowplot::plot_grid(pl1, pl2, rel_widths = c(0.43, 0.57))
print(intro_plot)
rm(mod_intro)
```
*Figure 1: Expected evenness of aquatic invertebrates as a function of conductivity and fine sediment fraction within a river reach. The left panel shows the relationship on the response scale, while the right panel presents it on the log scale for improved visualization.*

This package provides tools for ecological statistical modeling, largely using Bayesian approaches. While a basic understanding of R is sufficient to use the package, a detailed theoretical background is available in Section 6 at the end for those interested in exploring the concepts further. Please note that this is a continuously developing R-package, and updates or improvements may be made over time.

# 2. Data and models

The information needed for such a meta-analytic approach is available from figures, tables, data sets or combinations between them. However, these are hardly utilized in a consistent fashion. In simple words, multiple datasets are needed on which (G)LMs can be fitted (see Kaijser et al, ...). This, however, will show that most data is very noisy, perhaps biased and the heterogeneity among different studies is large. This is an issue if we focus on error-control and causality statements. Both error-control and causality need controlled environment with a well setup experimental conditions and exclusion or modeling of confounding variables. Yet, while error-control a posterior is not possible anymore, this makes the information still incredibly useful. It can be used to make probability statements, predictions and used to calculated the apriori needed power to setup a new experiment where the focus lies on error-control or causality. This package focuses on making posterior probability statements, visualization and prediction from information. Hence, the goal is to generalize ecological effects from literature, displaying its patterns and predict with it.

## 2.1 (Generalized) Linear Models (G)LMs and extracting data

To use this package data is needed and on this data (G)LMs need to be fitted. This (meta-)data can be obtained from figures (e.g., using WebPlotDigitizer), tables (i.e., extrected by converting pdf to Excel files), data sets and combinations between them (See Kaijser et al. ...). The effects we are talking of are not really 'effects' but model parameters more commonly known as the intercept or slope. The intercept (*b0* or *β0*) and slope (*b1* or *β1*) can be used to predict using the following equation: *response variable = b0 + b1 · predictor variable*. Hence, this package starts with the underlying philosophy that if we choose to believe claims as 'an effect' (*b1*) of the *predictor variable* was observed' then this should be clearly generalizable. For example the relation between chlorophyll-a and total phosphorus is generalizable. From here on I will use the term model 'parameter' instead of 'effect'. Hence, by gathering information on the parameters *b0* and *b1* we can give a pooled model a single or multiple new values (*i*) for *x*. By doing so we can understand the magnitude of the relation, variability of it and make a predictions. 

In a GLM the response variable is linked to to the linear component with a link-function often notated as *g(...)*. The 'identity' link function applies no link function. This indicates that the mean (expected value of *y* notated as *E(y)* or *(E(y|x))*) is directly related to the linear component.
$$g(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$ 
However, in a GLM with, log- or logit-link it is easier to talk about log-linear relations 
$$log(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$ 
or logit-linear relations
$$logit(E(y_{i} \mid x_{i})) = \beta_0 + \beta_1 \cdot x_{i}$$
The slope in a GLM not a true slope anymore because there is not straight line anymore that links the response (*y*) and predictor variable (*x*). It is however still a linear model because the model parameters are still constructed  as in a linear equation. Therefore the term coefficient or regression coefficient refers to the model parameter ($\beta$). 

Heuristically, I prefer working often working with elasticity- or semi-elasticity coefficients (Woolridge, 2001). In this example I use the elasticity- or semi-elasticity coefficients. But, there are enough reasons not to do so and is dependent on the question, data and many other factors. The elasticity coefficient expresses the percentage change of $y$ (response) as relative to $x$ (predictor variable). Thus, 0.2 is 0.2% change in $y$ given 1% in $x$. Hence, for a log-linear model $log(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$
and thus $\beta_1 = \frac{\log(y)}{\log(x)}$. For the semi-elasticity coefficient (i.e., logit-linear) this only accounts partially and values closer 0 are better interpretable because $logit(E(y \mid x)) = \beta_0 + \beta_1 \cdot log(x)$ and thus $\beta_1 = \frac{logit(y)}{\log(x)}$. This expressed the  change in the log-odds per 1% elasticity (Cramer 1991; Woolridge, 2001). This makes it possible to compare among different models and predictors and still keep the interpretation of to the units needed for prediction. Hence, we can still predict with the model and interpret the expected change of $y$ given an increase in $1·log(x)$, because the units of $x$ are retain under the $log$. This is not possible anymore if we would 'standardize' the regression coefficients.

An example of the properties of the elasticity coefficient would be that decrease of 100 till 30 species of benthic-invertebrates from 50 till 5000 $\mu S·cm^-1$ would be $\beta_{elasticity}= (log(100)-log(30))/(log(50)-log(5000))=-0.26$. This decrease is the same if the decline occurred  from 10 till 3 over the same range $\beta_{elasticity}=(log(10)-log(3))/(log(50)-log(5000))=-0.26$ What does change is the intercept *b0* of the model. Yet, this is not an issue for understanding the magnitude, the uncertainty connected and visualization. 

Sometimes these 'unofficial expressions' for *b0* and *b1* are used, due to the reference in the R-package to the expressions $\beta_0$ and $\beta_1$. The expression from the models above would be more more formally expressed: 
$$g(E(y_{i} \mid x_{ij})) = \sum_{j=1}^{j} \beta_j \cdot x_{ij}$$
Where $x_{ij}$ refers to the $j$ the predictor variable (e.g., salinity is $j$=1 and light is $j$=2) and $i$ is the $i$-th observation. This expression will later be utilized in the explanation of the visualization.

# 3. The meta-function

## 3.1 Data

To start using this R-package both JAGS and devtools need to be installed. JAGS can be installed from https://sourceforge.net/projects/mcmc-jags/ and devtools can be install in R. For the most recent version of the EcoPostView it can be installed from GitHub. Of course, any problems, questions or possible improvements can be directed to me.

```{r setup}
#install.packages("devtools")
library(devtools)

#install.github("snwikaij/EcoPostView")
library(EcoPostView)
```

From here I assume multiple (G)LMs were fitted and from these fitted models the parameter estimates and the standard error were gathered and stored in a dataset. In connection to these parameter estimates it is convenient to note the source (e.g., DOI), the type of predictor variable (e.g., conductivity), group of the response type (e.g., benthic-invertebrates), link-function and if the model parameters is the intercept *b0* or a regression coefficient *b1*. If multiple predictor variables are fitted in a model all regression coefficients as notated as *b1* to distinguish it from the intercept *b0*. The example in R shows the structure of the data frame.

```{r data}
data(example1)
head(example1)

```
In the example above the column **est** (estimate) indicates the estimated model parameters and the column **se** the standard error of the estimate. The column **group** can be an organism group or specific species/taxon (or anything you wish to group by), the column **predictor** the specific predictor variable, the parameter whether the estimate is the intercept (*b0*) or a regression coefficient (*b1*) and the column **link** contains the link function. An additional and recommended option is to include the sample size **n** for adjusting for 'small-sample-effects' (Peters et al., 2006; Moreno et al., 2009).

## 3.2 Basic model structure for meta-analysis

The meta function can include a random effect, by setting the argument RE=TRUE (default is TRUE). The  structure is then 
$$\{\beta_{i}, ..., \beta_{n}\}= \beta_{\text{pooled}} + u_i$$
It has the option of placing a single or multiple random effect as a vector or matrix using the argument 'random' the structure then becomes then 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i +r_i$$
for a single random effect. It can adjust for the relation between the $se$ and model parameters using the the squared standard error $se^2$ often refered to as Precision-Effect Estimate with Standard Errors or short PEESE (method=1, Stanley and Doucouliagos, 2014). The structure is then 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i + \alpha_{i} \cdot se^2$$ 
or inverse of the sample size $1/n$ (method=2, the latter option is performed below) with the structure 
$$\{\beta_{i}, ..., \beta_{n}\} = \beta_{\text{pooled}} + u_i + \alpha_{i} \cdot \left(\frac{1}{n}\right)$$ 
Of course if bias is considered neglect non can be performed (method=0). I still would like to include a third 4th option to utilize Robust Bayesian Model Averaging (RoBMA: Maier et al. 2023). But this sometimes adjust extremely when including $se^2$ and therefore I left this option open for now.

```{r meta1}
mod1 <- meta(estimate=example1$est,         #Model estimate
             stderr=example1$se,            #Standard error of the model estimate
             parameter=example1$parameter,  #Model parameter (b0 or b1)
             predictor=example1$predictor,  #Predictor variable  (independent variable)
             link_function=example1$link,   #Link function
             grouping=example1$group,       #Group
             Nsamp=example1$n,              #Sample size (optional, for adjustment 2=Peters (1/n)),
             method=2)                      #Adjustment method (0=none, 1=Egger's (1/se), 2=Peters (1/n))
```

The meta-function can return a warning that the MCMC-chains are not properly mixing. This can be an issue due to various reasons. Where this warning originates from can be assessed by looking at the 'raw' JAGS model output (`mod1$model$JAGS_model`). This could show that a parameter of interested 'mu[.]' Has a a large Rhat or small effective sample size. Most of these issues can be resolved by thinning the chains, increasing the number of chains or setting more informed or stronger priors. Moreover, if the issue is not an issue of the estimated 'mu' parameter, it could be decided to ignore it. These choices are ultimately up to the user. An option to prevent warnings would be to set the warning level for Eff_warn lower i.e., Eff_warn = 500.

## 3.3 Standard meta-analysis

Often a meta-analysis is performed over standardized-effect-sizes (SES). While I do not condone this practice in any way, I do not see the use of always using this practice has much benefit for field ecology, applied ecology, generalizations or learning about the impacts of real world application of ecological relations (Baguley 2009; Tukey  1969). Therefore a short introduction meta-analysis and also displaying how the bias correction methods perform. I compare it with my standard favorite metafor package in R.

```{r, bias adjustment and metafor}
#First run and check standard output for metafor
library(metafor)
data("example3")

#Run metafor
standard_metafor <- metafor::rma(yi=example3$est, sei=example3$se)

#Run EcoPostView
standard_meta    <- meta(estimate=example3$est, stderr=example3$se)

#Results metafor
print(standard_metafor)

#Results EcoPostView
print(standard_meta$Summary)

```

Both the metafor and EcoPostView show similar means -0.22 and standard errors. Hence not informing priors is often meaningless. However, we can now also check the bias via the residuals.

```{r plot metafor and EcoPostView, fig.width=7.8, fig.height=4}
par(mfrow=c(1,2))
plot(1/example3$se, resid(standard_metafor), 
     xlab = "1/se", ylab="Residuals", main="metafor")
abline(a=0, b=0, col="red", lty=2)

plot(1/example3$se, standard_meta$Residuals, 
     xlab = "1/se", ylab="Posterior mean residuals", main="EcoPostView")
abline(a=0, b=0, col="red", lty=2)
```

Here we clearly see a diagonal pattern almost similar for both functions. The posterior means are however pulled closer (shrinkage) to the mean for estimates with weaker standard errors. This bias can also be asses using the rescheck function from EcoPostView.

```{r plot rescheck function, fig.width=7.8, warning=FALSE, fig.height=4}
#Use the residual check function within EcoPostView
res_bias <- rescheck(standard_meta)

print(res_bias$bias_se)
```

This bias is very obviously caused by leaving out or selecting 'significant' results. Often by manually dropping 'non-significant' variables, forward-backward AIC, BIC selection methods to find the 'best' model (Gelman and Loken, 2013). This leading to large overestimation of the parameter ('effect-size'). This bias can be corrected using method 1 of Stanley and Doucouliagos (2014).

```{r adjusting bias via Eggers approach, fig.width=7.8, fig.height=4, warning=FALSE}
#Run EcoPostView (increased the chain thinning interval and number of iterations to improve mixing)
adjusted_meta    <- meta(estimate=example3$est, stderr=example3$se, method = 1, 
                         n_thin = 5, 
                         n_iter = 30000)

print(adjusted_meta$Summary)

res_bias2 <- rescheck(adjusted_meta)
print(res_bias2$bias_se)
```

In the method above the bias has been adjusted (not removed!). The pooled estimate is now much lower This means the relations between the standard error can be assessed by looking at the residuals. This is often performed with funnel plots but can as addition also be asses by simply checking the residuals. The adjustment should only be applied if patterns are extremely clear it can often lead to strong adjustments even if no clear bias is present. However, it is extremely effective if such bias is present in the data. In the future I hope to add multiple methods to assess the strength of the bias.

The bias displayed in this example are clearly extreme in such cases it could be beneficial to explore the bias further to asses if there is clearly a 'publication gap'. This publication gab can be highlighted by looking at the z-distribution derived from the p-value. While normally the p-value is derived from the z-value. When there is a clear gap visible (as in Zwet and Cator 2021) then we should be able to model the absolute z-value this from a mixture of two half normal distributions. Here one model is truncate at 0 and the other at 1.96. Since the likelihood of a mixture is difficult the obtain an Approximate-Bayesian-Computation algorithm was used based on rejection sampling (ABC-rejection). This method is further highlighted in Csilléry et al. (2010) and Hartig et al. (2011) and the model given here is formally described in the theoretical section.

```{r exploring bias with ABC-rjection, fig.width=7.8, fig.height=4, warning=FALSE}
#From the dataset calculate the p-value from the effect-sizes and standard errors
pvalues <- ptoz(estimate=example3$est, stderr=example3$se)

#run the ABC-rejection model
result_abc  <- abctoz(p=pvalues$data$p, nsim = 250000)

#Extract the information from the results based on a selected threshold
extract_abc <- extrabc(result_abc, dist_threshold = 0.052)

#Plot the histogram of the z-values with the simulated density lines of the posterior
plot(extract_abc$hist)
```
Based on the distribution of z-values we can clearly observe that z-values |z|>1.96 are more common published than those lower values. This would be reasonable if there was no gap observed and this strong boundary at >1.96. However, the z-values can be reasonable modeled and the model seems to fit decently (R-squared >0.6-0.7 is considered). Furthermore, the density lines decent and the  nicely overlay with the histogram. The number of observations from the censored model are 0.76 (76%). This does not mean that 76% is censored as the focus of this model to see if it can strongly fit to the data. This fit, if the bias is as sever as provided via the residuals, provides another hint that indication a selection procedure. To make sure the results are reasonable one can make sure that the fit stays sufficient and the accepted number of simulation is acceptable (heuristically > 100).

## 3.4 Setting priors

A benefit of the the Bayesian approach is that we can included prior information. Thereby explicitly shifting the weight of the outcome to more plausible values of the pooled model parameter. To set only a single prior for each relation and parameter a specific structure is needed. By default the model parameters are set as Gaussian/normally distributed with a mean ($\mu$, prior_mu) of 0 and standard error ($se$, prior_mu_se) of 0.5. The prior distribution is at the moment restricted to the normal distribution. The prior for the standard deviation ($\sigma$) is uniform where only the maximum value (prior_sigma_max) can be set, which at default is 5. 

As explained in section 2.1 I heuristically like to work with the elasticity- or semi-elasticity coefficient. However, THIS IS NOT A NECESSITY! an therefore means priors change depending on what you like to use and known. Hence, not thinking about your prior - even if not a lot of information is available, but at least inform it - is often not doing a Bayesian analysis at all. 

That being said you can choose an prior values for the mean and standard deviation. To get a nice table overview of the structure need for the priors can be obtained by setting the 'get_prior_only' to 'TRUE'. This will return a dataframe, with a column **level**, priors for the **mu** ($\mu$) and for the standard error ($se$). The numbers can adjusted to the specific topic based on prior information available. How to included this dataframe will be explained in more detail in section 3.5.

``` {r meta2 example priors}
only_priors <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,       
                    Nsamp=example1$n,            
                    method=2,
                    get_prior_only=TRUE) #Only show the structure of the priors

print(only_priors)
```

## 3.5 Setting multiple priors for Bayesian Model Averaging

A benefit of the the Bayesian approach is that we can included multiple ($k$) priors to perform Bayesian Model Averaging (BMA;  Hoeting et al., 1999; Hinne et al., 2020). Thereby explicitly adding multiple possible scenarios that could have generated the observable data and averaging over these plausible explanation. To set multiple priors we need a similar dataset as shown above.

Often prior weights are added, which indicate how heavy a prior weighs in the model. These prior weights could any number between 0 (0%) and 1 (100%) so that it preferably adds up to 100%. To avoid the already increasing complex model on could set these to equal weights. For example, if there are three priors then each weighs 1/3. However to avoid fumbling around with this one could also decide that these prior weights are uncertain and treat them a stochastic by letting them arise from a Dirichlet distribution $weight \sim Dir(\alpha_i)$ where $\alpha_i=1$ if the number ($i$) of prior distributions is >1. Meaning the Dirichlet distribution is 'uniform'.

In the example below the priors have varying values for the parameters $mu$ and $se$, and a broader prior $N(\mu=0, se=10)$ for the intercept is reasonable.

``` {r meta3 BMA}
data("example2")
print(example2)

mod2 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu=example2[c(2,4,6)],          #prior for the mean
                    prior_mu_se=example2[c(3,5,7)],       #prior for the standard error of the mean
                    Nsamp=example1$n,            
                    method=2,
                    n_thin=10,                            #thinning the chains
                    n_chain=4)                            #changing the number of chains from 2 to 4

#Display the summarized results
mod2$Summary
```
The results of the meta-analysis are summarized in a table describing Maximum A Priori values (MAP), the $\mu$, $se$ and the high density intervals at default (90%). Additionally the $I2$ heterogeneity is given.

The prior for the between study variance known as $\tau^2$ is set as uniform with ranging from 0 till 5 by default. This has as an advantage that it will be result in wider intervals and be conservative for smaller sample sizes. A disadvantage is that it will be conservative. It can under particular conditions be very beneficial while under other conditions where more precision is required be a downside. To give the user the option the argument 'prior_var_fam' can be set to 'exp' (exponential distribution) instead of 'unif' (uniform distribution). Under 'unif' the maximum value of the uniform distribution is set as Uniform(0, prior_study_var) and when set to 'exp' the mean of the exponential distribution is used as Exponential(1/prior_study_var). In general cases with a large number of predictors and response as in Kaijser et al. ... I would advice the uniform prior if no convergence issues arise. If a more specific topic is choose like one or two predictors and one response and smaller datasets <10 observations the exponential distribution (e.q., with a mean of 1000) might suite well. These advice are heuristic in that they can be supported by personal experience and examples but are not to be taken for granted. Best is to try it out and perform a sensitivity analaysis (check).

## 3.6 rescheck-function and bias

After analyzing the the meta-data also a check for bias should be performed. This can be via various methods. I believe this this should be a sensitivity analysis using various methods: Display of the z-distribution, Egger's test, Peters tests and/or funnel plots. Hence, there is always bias, but in the data you have it might be small.

A simple option is to visually asses the relation of the residuals relative to $1/se$ and if sample sizes is provided in the data $1/n$. If a clear diagonal pattern is visual this means that there is a strong relation of the $\beta$ to $se$ which can indicate the selection of only larger effects with broader intervals, p-hacking or noise in the data. A relation with $n$ often occurs when small sample sizes an noise result in so called 'small-study-effects'. There are different ways to asses the residuals relation relative to $1/se$ or $1/n$: in total, per group or per predictor. Below the residuals per group in relation to $1/se$.

```{r, residual check, fig.width=7.1, fig.height=4, warning=FALSE}
res_mod2 <- rescheck(mod2)

print(res_mod2$bias_se_group)
```
And the residuals per predictor in relation to $1/se$.
```{r, residuals predictor, fig.width=7.1, fig.height=4, warning=FALSE}
print(res_mod2$bias_se_predictor)
```
The dotted red line should more-or-less-ish overlay the solid blue line having an intercept and slope of 0. Small sample sizes can strongly influence the slope of this line. When bias is present then a different correction method can be selected or stronger priors can set to reduce this.

## 3.7 senscheck-function and prior sensitivity

Sensitivity checks can make up an important part of assessing the results. For simpler an more informing models it might not be necessary. Hence, we simply cannot assume the same results would be obtained in the next study under any circumstances. Strong conclusions based on intervals covering or not covering 0 are therefore rather arbitrary. Observing the posterior distribution with estimate and its error and a reasonable visualization of the patterns the estimates uncover with its variance is sufficient. Especially the nature of ecological data being extremely noise and exploratory. This means that also the posterior of each study can be drastically different. Nonetheless, it can be helpful to asses the sensitivity as a form of reality check.

The sensitivity check asses the shift between a model with full prior implementations (mod1) versus another model (mod0). This is simply performed by dividing the posterior of $Log(P(Mod1|Data, Info)/P(Mod0|Data, Info))$ assuming all model hyper parameters are held constant except for the priors. This means we calculate the posterior odds ratio between two models. Another possibility is to asses how much the prior information of mod1 versus mod0 adds to a positive or negative shifts away from 0. This means we check $Log(P(Mod1>0|Data, Info)/P(Mod0>0|Data, Info))$ which is transformed back to a percentage between 0 and 1. Where 0.5 no change of the prior density compared to 0, and 0 a completely negative and 1 completely positive. I am not the biggest fan of this way of assessing sensitivity, as this again treats the posterior as a form of dichotomous hypothesis test. It is available but I left it out of the examples below.

In the previous example mod2 was created, here treated as mod1. We can create a model with hardly any prior information (mod0) setting the $\mu$ of all priors to 0 and prior for the $se$ to 100.

```{r, sensitivity check, fig.width=7.1, fig.height=4, warning=FALSE}
#Create a model with minimal prior information
mod0 <- meta(estimate=example1$est,        
                    stderr=example1$se,            
                    parameter=example1$parameter,  
                    predictor=example1$predictor,  
                    link_function=example1$link,   
                    grouping=example1$group,
                    prior_mu=0,                           #prior for the mean
                    prior_mu_se=100,                      #prior for the standard error of the mean
                    Nsamp=example1$n,            
                    method=2,
                    n_thin=10,
                    n_chain=4)

#Perform the sensitivity check
sens_check <- senscheck(mod2, mod0)

#Plot the posterior odds
print(sens_check$posterior_odds)

```

The vertical black line indicates the value at which no difference between between mod1 and mod0 would be observed as $0=Log(P(Mod1|Data, Info)/P(Mod0|Data, Info))$ The most obvious results jump out where fish and oxygen for log-linear model and for salinity and sediment for the logit-linear models the priors pull the posterior to more negative values. It is possible to asses the strength by taking the inverse logit of the MAP $logit^{-1}(MAP)$ expressing it as the shift in the probability caused by the prior. Where smaller then 0.5 indicates negative shift, bigger than 0.5 a positive shift and 0.5 none.

```{r, sensitivity quantif, warning=FALSE}
#Select only predictor and link function
inv_df <- sens_check$table[c(3:4)]

#Calculate the probability
inv_df$prob <-plogis(sens_check$table$mu[sens_check$table$group=="Fish"])

#Print the table
print(inv_df)
```

The table shows for sediment and logit in relation to fish, the posterior shift from mod0 to mod1 is influenced by 0.5-0.38=0.12 or 12%. Hence, our prior information provides some information that the relation between fine sediment and lotic fish species is negative that is not coming from the data alone. This is not a 'negative' thing, but what we do in reality. It shows that our prior information is often more 'directed' and meta-data that can be obtained from literature.

This was the most difficult part where the data needs to be gathered and extracted, (G)LMs need to be fitted, priors need to formulated, bias needs to be assessed and the whole model needs to be optimized to get a stable results. To display or predict with the models requires less time.

# 4. Display

## 4.1 pdplot-function

To display the posterior results one could display the results with a point and interval range. However, this interval displaying a strong boundary on a continues set of possibilities does not completely capture  the idea of the posterior probability distribution. Another option would be to display the Posterior Density Distribution (PDD) combined with the point and interval estimate. Hence, the PDD displays the possible values of the prior information where the pooled estimated might have originated from $f(\beta_{\text{pooled}} \mid Meta-data, Info)$ it is the inverse of the likelihood which informs us of the values of the meta-data given we selected a particular pooled estimate $f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\})$. This can be  performed with the pdplot function.

```{r pdplot}
pdd <- pdplot(mod2, 
              label_size=4,      #setting the label size larger
              point_size=2)      #large point
```

The object contains the figures generated for both the log

```{r pddplot log, fig.width=8, fig.height=4}
#For the models with the log-link
print(pdd$posterior_density$log)             
```

and logit functions.

```{r pddplot logit, fig.width=8, fig.height=4}
#For the models with the logit-link
print(pdd$posterior_density$logit)
```

And, a summary belonging to the figures.

```{r summary figures}
#summary belonging to the figures
print(pdd$summary)
```

For larger datasets with multiple groups and predictor variables the order of the PDDs can be changed by using the arguments 'order_predictor' and 'order_group"' needing a character string with the names of the desired order.

## 4.2 hop-function (Hypothetical Outcome Plots)

Hypothetical Outcome Plots (HOPs) are a powerful method to visualize the possibilities the expected value could take on given a change in the predictor variable (Kale et al. 2019).
HOPs lines display the marginal change of the expected value given a change in the predictor variable holding all other variables constant.
$$
g(E(y_{i} \mid x_{ij)})) = \beta_{pooled,j=0,m} + \beta_{pooled,j=1,m} \cdot x_{i,j=1}+ \sum_{j=1}^{j} \beta_{pooled,j} \cdot \hat{x}_{j}
$$
This hypothetical prediction is generated using the posterior estimates $f(\beta_{pooled}|Data, Info)$. Hence, this parameter describes the influence on the response give one unit change in the predictor. To display this change a set of sequential values for $x_{i,j=1}$ can be generated as $x_{i,j=1}=\{i, ..., n\}$ realistic for the observed gradient. The function $f(\beta_{pooled,j}|Data, Info)$ contains most plausible values for $\beta_{pooled,j}$. By generating a value $m$ from this function $\beta_{pooled,j,m}\sim f(\beta_{pooled,j}|Data, Info)$ and repeating it $m$ times a bunch of hypothetical outcomes can be generated. To arrive at this display, all other estimated parameters are held constant at the sum the other estimates $\sum_{j=1}^{j} \beta_{pooled,j}$. The HOPs lines are therefore simulation of (marginal) possibilities from the posterior. There is a slight difference with classical hops because in the case of the meta-analysis $\hat{x}=1$. This still displays the marginal change of $y_i$ given $x_{i,j}$ but only under the conditions where all predictors are $\hat{x}_j=1$ 
$$
g(E(y_{i} \mid x_{ij)})) = \beta_{pooled,j=0,m} + \beta_{pooled,j=1,m} \cdot x_{i,j=1}+ \sum_{j=1}^{j} \beta_{pooled,j} \cdot 1
$$
Note that the function works under the idea of log transformed variables. This means that argument xlim (the limits of the figures) as given below are exp(-4.6)=0.01 and exp(0)=1 for the fraction of fine sediment. The y-axis is on the response scale. I hope soon to make it possible to use an argument that makes it possible to set the xlim on the response scale. Below an example of the outcomes of the response of invertebrate taxonomic richness along the gradient of fine sediment. 

```{r HOPs log salinity fig1, fig.width=7.1, fig.height=4,  warning=FALSE}
log_sal1 <- hop(mod2,                                  #Object from the meta function
    group="Invertebrates",                             #Select group to out of Invertebrates and Fish
    predictor = "Sediment",                            #Select group to out of Salinity and Oxygen
    xlab= "Fine sediment (%)",                         #Give x-axis a name
    link_function = "log",                             #Which link function out of log and logit
    ylab="Invertebrate taxonomic richness",            #Give y-axis a name
    xlim=c(-4.6, 0),                                   #Give y-axis a name
    ylim=c(0, 80),                                     #Set  limits y-axis
    hop_lwd = 0.3)                                     #Set width of the hop lines

#Display the HOPs
print(log_sal1)
```

It is also possible to scale to display the exponentiate values on the x-axis using the argument 'exp_axis'. Note that one might suggest the intercept is to high. This could be correct, because this is the average intercept over all studies keeping the other variables constant at 1. To change the position of the intercept the argument 'shift_b0' can be used. 

```{r HOPs log salinity fig2, fig.width=7.1, fig.height=4,  warning=FALSE,}
log_sal2 <- hop(mod2,                                   
    group="Invertebrates",                             
    predictor = "Sediment",                            
    xlab="Fine sediment (%)",   
    link_function = "log",                            
    ylab="Invertebrate taxonomic richness",            
    xlim=c(-4.6, 0),                                      
    ylim=c(0, 50),                                     
    hop_lwd = 0.3,                                    
    exp_axis = T,                                     #Exponentiate the x-axis notations
    round_x_axis = 2,                                 #Round the notation to full integers 
    shift_b0 = -1)                                    #Shift the intercept by -1     

#Display the HOPs
print(log_sal2)
```

## 4.3 hop function (and Partial Dependency Plots)

There are multiple predictors is the dataset. Therefore it possible to display the MAP predicted value given a change in one of the two predictors. This can be performed by display in Partial Dependency Plots.

```{r HOPs log salinity fig3, fig.width=7.1, fig.height=4,  warning=FALSE,}
log_sal3 <- hop(mod2,                                              
    group="Invertebrates",                            
    predictor = c("Sediment", "Oxygen"),                     #Select both Sediment and Oxygen
    xlab= "Fine sediment fraction",                          #Give x-axis a name
    ylab= expression(Oxygen ~ mg ~ L^-1),                    #Give y-axis a name
    gradient_title = "MAP Invertebrate \ntaxonomic richness",#Give the y-axis gradient a name
    pdp_resolution = 100,                                    #Set resolution of the grid
    link_function = "log",
    exp_axis = T, 
    round_x_axis = 2,
    round_y_axis = 0,
    xlim=c(-4.61, -0.92),
    ylim=c(1.61, 2.77)) 

#Display the PDP
print(log_sal3)
```

# 5. Prediction

This part is still under construction.

# 6. Theory and background

In most cases the theory and background is often not provided in many cookbooks, but makes it impossible to interpreted, criticize our results. This section introduces some theory. However, if you are already familiar with it or find it too technical, feel free to skip it. Since this R-package primarily uses Bayesian inference, an introduction to Bayes Theorem and methods used may be of interest. While it is not necessary needed for the utilization of the R-package itself.

## 6.1. Classical statistics and estimation

All statistics focuses on estimating the parameter of interest $\theta$ which in most (G)LMs is denoted as the parameter $\mu$ or $\beta$. For consistency I will use the parameter of interest $\mu$. The population parameter of interest $\mu$ is fixed and unknown. To investigate potential values for $\mu$ we take measurements/samples. The observed measurements or samples ($x$) are realizations of an unknown random variable $X$, where both x and X represents sets $x = \{x_i, \dots, x_n \mid x \in \mathbb{R}\}$ and $X = \{X_i, \dots, X_N \mid X \in \mathbb{R}\}$. Here, $x$ is a realization and $X$ denotes the random variable. It is then assumed that $x$ is originating as randomly identically and independent distributed random variable (iid) from X as in $x\sim X$ or $x \stackrel{\text{iid}}{\sim} N(\mu, \sigma^2)$. Since we do not have $X$ but only a set of realizations we need an estimator for it which it the sample mean $\hat{x}(x)$. Hence, the mean of the random variable $X$ would be $\mu(X)=\frac{X_i,\cdots,X_N}{N}$ and if x are indeed iid $\hat{x}(x)=\frac{x_i,\cdots,x_n}{n}$ would be an estimator for $\mu$. Therefore, the sample mean has certain properties $\mathbb{E}[\hat{x}]$ and $Var(\hat{x})=\frac{\sigma^2}{n}$. Then the weak law of large numbers suggest that the probability of deviation from the population parameter decreases when sample size $n$ increases $\lim_{n\to\infty} P\left(|\hat{X}_n-\mu|\geq \epsilon\right)=0$. This means that according to the central limit theorem $Z_n=\frac{\hat{x}-mu}{\sigma/\sqrt(n)}$ converges to the standard normal under $mu=0$. The probability of observing $Z$ under a long-run of repetitions $P(-1.96\lesssim Z \lesssim 1.96)=0.95$. Similair, the probability of the intervals of $\overline{x}$ to cover $\mu$ in a long-run of repeated experiments at 95% is $1  - c = P(\overline{x} > \mu - 1.96 \cdot \frac{\sigma}{\sqrt{n}}) ~and~  P(\overline{x} < \mu - 1.96 \cdot \frac{\sigma}{\sqrt{n}})$. I visually explain this in this shiny app https://snwikaij.shinyapps.io/shiny/. Furthermore, it is clear that statistics does do nothing with causality in the first place and the focus on error-control and causality starts by satisfying theoretical conditions needed the arrive at believes in these concepts. Hence, error-control and causality start a-priori (Fisher 1949, Pearl 2009, Mayo 2018). Yet, such a focus and framework would, for example be extremely useful if objectivity over repetitions are the goal and favorable. While classical statistics focuses on fixed but unknown parameters, error-control and objectivity of information, Bayesian methods extend this perspective by introducing prior information and viewing parameters as random variables. This shift opens the door to more flexible and informative inference, as explained in the next section.

## 6.2 Bayes Theorem and probablistic estimation

Informally Bayes Theorem would be notated $\text{Posterior}\, \text{probability} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Evidence}}$. More formally Bayes theorem is often notated with A and B where P indicates probability and '|' given or conditional on. $P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$.Other expression such as $P(\theta|Data, Info) = \frac{P(Data|\theta) \cdot P(\theta|info)}{P(Data)}$ are to highlight that the posterior describes the information of that conditional on the prior `info` that is given in there.

The derivation of Bayes theorem relies on the axioms probability theory.
**Premise 1)**
$$
P(A | B) = \frac{P(A \cap B)}{P(B)}
$$
similarly
$$
P(B |A) = \frac{P(B \cap A)}{P(B)}
$$
**Premise 2)**
Also, the joint probability, expressed as a set-theoretic relationship on $z$, indicates that element of both sets are the same.
$$
z = \{x : x \in A \cap B : x \in B \cap A\}
$$
thus
$$
P(A \cap B) = P(B \cap A)
$$
**Premise 3)**
In accordance with the previous
$$
P(A| B) \cdot P(A) = P(A \cap B)
$$
and
$$
P(B | A) \cdot P(B) = P(B \cap A)
$$
**Conclusion)**
Therefore
$$
P(A | B) \cdot P(A) = P(B | A) \cdot P(B)
$$
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$
As introduced, in statistics and estimation is about finding out the value for $\theta$ which is assumed be $\mu$. Where in the frequentist framework this is considered fixed and unknown, this is in the Bayesian framework considered to be random 'and approximately' known. Of course also in the Bayesian framework samples $x$ are taken. Assume that we already know something about $\mu$ then it is possible to restrict to exclude unreasonable values or for the information we have on $\mu$ to more acceptable values.
$$
P(\mu|Data) = \frac{P(Data|\mu) \cdot P(\mu)}{P(Data)}
$$
For a simple mean and variance an analytical approach can be used to derive the posterior given the likelihood and prior via the following equations.

$$\mu_{posterior} =\frac{\frac{\mu_{prior} }{\sigma_{prior}^2} + \frac{\hat{x}_{data} }{\sigma_{data}^2}}{
\frac{1}{\sigma_{prior}^2} + \frac{1}{\sigma_{data}^2}}
\\
\sigma_{posterior}=\sqrt{\frac{1}{\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{data}^2}}}$$

**Proof:**
**Premise 1**
Bayes rule can be simplified to
$$P(\mu|Data) \propto P(Data|\mu) \cdot P(\mu)
\\
N(\mu_{posterior}, \sigma_{posterior}^2)=N(\mu_{sample}, \sigma_{sample}^2)\cdot N(\mu_{prior}, \sigma_{prior}^2)
$$
**Premise 2**
The PDF for the normal distribution is
$$f(x)=\frac{1}{2\cdot \sqrt{\sigma \pi}}\cdot exp(-\frac{1}{2}(\frac{x-\mu}{\sigma})^2)$$
**Premise 3**
$$Prior: P(\mu_{prior})=\frac{1}{2\cdot \sqrt{\sigma_{prior} \pi}}\cdot exp(-\frac{1}{2}(\frac{\theta-\mu_{prior}}{\sigma_{prior}})^2)
\\
Likelihood: P(\mu_{prior})=\frac{1}{2\cdot \sqrt{\sigma_{sample} \pi}}\cdot exp(-\frac{1}{2}(\frac{\mu_{sample}-\theta}{\sigma_{sample}})^2)
$$
**Premise 4**
Both $\frac{1}{2\cdot \sqrt{\sigma_{prior} \pi}}$ and $\frac{1}{2\cdot \sqrt{\sigma_{sample} \pi}}$ are scalars and can be left out of the equation.

**Premise 5**
Since both exponent have the same base we can add the exponent $$(a^2+b^2=a^{2+2})$$ 
resulting in
$$exp(-\frac{1}{2}\cdot[(\frac{\theta-\mu_{prior}}{\sigma_{prior}})^2+(\frac{\mu_{sample}-\theta}{\sigma_{sample}})^2]$$
After which brackets can be moved
$$exp(-\frac{1}{2}\cdot[\frac{(\theta-\mu_{prior})^2}{\sigma_{prior}^2}+\frac{(\mu_{sample}-\theta)^2}{\sigma_{sample}^2}])$$
**Premise 6**
Expanding the brackets terms
$$(a^2+b^2)=(a-b)\cdot(a-b)=a^2-ab-ab+b^2=a^2-2ab+b^2$$
This means 
$$(\theta-\mu_{prior})^2=\theta^2-2\theta\mu_{prior}+\mu_{prior}^2$$ 
and
$$(\mu_{sample}-\theta)^2=\mu_{sample}^2-2\mu_{sample}\theta+\mu_{sample}^2$$ 
which can be replaced in premise 5
$$exp(-\frac{1}{2}\cdot[\frac{\theta^2-2\theta\mu_{prior}+\mu_{prior}^2}{\sigma_{prior}^2}+\frac{\mu_{sample}^2-2\mu_{sample}\theta+\mu_{sample}^2}{\sigma_{sample}^2}])$$
**Premise 7**
Separating each term by dividing by $\sigma_{prior}^2$ and $\sigma_{sample}^2$
$$exp(-\frac{1}{2}\cdot\frac{\theta^2}{\sigma_{prior}^2}+\frac{-2\theta\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{prior}^2}{\sigma_{prior}^2}+
\frac{\mu_{sample}^2}{\sigma_{sample}^2}+\frac{-2\mu_{sample}\theta}{\sigma_{sample}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2})$$

**Premise 8**
Group each term by the nominator
$$\frac{\theta^2}{\sigma_{prior}^2}+\frac{-2\theta\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{prior}^2}{\sigma_{prior}^2}+
\frac{\mu_{sample}^2}{\sigma_{sample}^2}+\frac{-2\mu_{sample}\theta}{\sigma_{sample}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2}=
\\
\theta^2(\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2})+
-2\theta(\frac{\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{sample}}{\sigma_{sample}^2})
+(\frac{\mu_{prior}^2}{\sigma_{prior}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2})
$$
Since the last group is not dependent on $\theta$ it is not in our focus
$$
exp(-\frac{1}{2}\cdot[\frac{\theta^2}{\sigma_{prior}^2}+\frac{-2\theta\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{prior}^2}{\sigma_{prior}^2}+
\frac{\mu_{sample}^2}{\sigma_{sample}^2}+\frac{-2\mu_{sample}\theta}{\sigma_{sample}^2}+\frac{\mu_{sample}^2}{\sigma_{sample}^2}=
\\
exp(-\frac{1}{2}\cdot\theta^2(\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2})+
-2\theta(\frac{\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{sample}}{\sigma_{sample}^2})
+not\ dependent\ on\ \theta])
$$
**Premise 9**
The goal is to derive $P(\mu|Data)$ from $P(\mu|Data) \propto P(Data|\mu) \cdot P(\mu)$ An the general exponential form of the normal distribution is given in Premise 2 and the premises 6, 7 and 9 lead to
$$\frac{1}{2}\cdot \theta^2 (\frac{1}{\sigma^2})+\theta(\frac{\mu}{\sigma^2})+C=
\\
\frac{1}{2}\cdot\theta^2A+\theta B+C$$
the general exponential form for the normal distribution is always $\frac{1}{2}\cdot\theta^2A+\theta B+C$ meaning that $A=\frac{1}{\sigma^2}$ and $B=\frac{\mu}{\sigma^2}$ and to obtain the standard deviation $A$ needs to be re-arranged to $\sigma = \sqrt{\frac{1}{A}}$ and to obtain the mean $\mu=\frac{B}{A}=\frac{\frac{\mu}{\sigma^2}}{\frac{1}{\sigma^2}}$

**Conclusion**
In Premise 8
$$
exp(-\frac{1}{2}\cdot\theta^2(\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2})+
-2\theta(\frac{\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{sample}}{\sigma_{sample}^2})+C)
$$
In Premise 9
$$
\sigma = \sqrt{\frac{1}{A}}, A=\frac{1}{\sigma^2}\\
\mu=\frac{B}{A}=\frac{\frac{\mu}{\sigma^2}}{\frac{1}{\sigma^2}}
$$
Which implies that
$$
\sigma_{posterior}=\sqrt{\frac{1}{\frac{1}{\sigma_{prior}^2}+\frac{1}{\sigma_{sample}^2}}}\\
\mu_{posterior}=\frac{\mu_{prior}}{\sigma_{prior}^2}+\frac{\mu_{sample}}{\sigma_{sample}^2}
$$
Another way to obtain the posterior mean would be via
$$\mu_{posterior}=\frac{\frac{\mu_{prior}}{\sigma_{prior}^2}+\hat{x}_{data}*\frac{n}{\sigma_{data}^2}}
{\frac{1}{\sigma_{prior}^2}+\frac{n}{\sigma_{data}^2}}$$

As might be clear this is less computational heavy than MCMC methods. For more then two parameter such an analytical approach becomes more combersome. And, if conjugacy is not satisfied no closed form solution is available. In this regards, Laplacian approximation is also computational easy. Yet, the equation clearly formulate the idea what happens in Bayes theorem. A simple example below highlights how this this can be useful.

## 6.3 Salinity example

For example, we want to know the probable relation between insects and salinity over a whole gradient from 0 till 5000 $\mu S \cdot cm ^{-1}$. To do so we generate a simple log-linear model $log(insect ~ diversity)=\beta_1*conductivity~+~\beta_0$. We know that insect diversity declines with an increasing salinity therefore we could exclude  positive values and shift the weight to more negative values. In the frequentist framework this is not performed and there is worked under the principle of indifference. This means all values are apriori equally likely. This is from an consistent error-control perspective preferred, but from a probabilistic perspective unreasonable. Hence, it would be beneficial to leverage this information into the model.
$$
P(\beta|Data, info) = \frac{P(Data|\beta) \cdot P(\beta|info)}{P(Data)}
$$
Under different context we can leverage more information over the likelihood using Bayesian Model Averaging (BMA) that does not use a single prior, but multiple priors.

## 6.4 Introduction to Bayesian Model Averaging (BMA)

Instead of $P$ the function '$f$' are used this to highlight that the probability is a mapping function. A mapping function being a 'rule' that maps $x$ to $y$ and so $y=f(x)$.
$$
f(\beta \mid Data, Info) = 
\frac{f(Data \mid \beta) \cdot f(\beta \mid Info)}
{\int f(Data \mid \beta) \cdot f(\beta \mid Info)}
$$
The integral in the denominator is used to scale the posterior probability to one. This expression is sometimes simplified to
$$f(\beta \mid Data, Info) = f(Data \mid \beta) \propto f(\beta \mid Info)$$
Where the $\propto$ symbol indicates 'proportional to' highlighting the idea of exchangeability. Therefore, the posterior is nothing more than a function that describes the probability $y$ as a function of $\beta$ conditional on $Data$ and $Info$ ($y=f(\beta \mid Data, Info)$). This cannot be solely conditional on the $Data$ as the $Data$ is not uncertain our information/believe is uncertain about a none existing object $\beta$ (unless Platonism is true). 

In the previous part a single prior model was used. Bayesian Model Averaging (BMA) has the advantages that it allows multiple ($k$) functions to be utilized as prior. I specifically choose the use of $f$ so multiple priors as $f_k$ in the equation below can be seen nothing more as multiple functions (or models). This in my opinion makes it easier to see that there is only optimized between multiple functions. It sound weird to say to optimize between probabilities.
Hence, multiple possible scenarios that could have been responsible for $\beta$ can be introduced as below.
$$
f(\beta \mid Data,Info) = \frac{f(Data \mid \beta) \cdot f_k(\beta \mid Info)}{\int \left( \sum_{k=1}^{k} f(Data \mid \beta) \cdot f_k(\beta \mid Info) \right)}
$$
Now it should be clear that each $\beta$ contained within $g(E(y \mid x_{ij})) = \sum_{j=1}^{v} \beta_j \cdot x_{ij}$ is being restricted by the prior models. While in frequentism it is unrestricted and 'complete indifference' towards the  possibility of $\beta$. All these methods can be used in a meta-analysis.

## 6.5 Meta-analysis

A standard meta-analysis uses a measure of location (mean) and scale (precision) to estimate a pooled value based on all parameters. For a fixed meta-analysis the pooled parameter is derived via the following equation.
$$\theta_{pooled} = \frac{\sum_{i=1}^{k}(\theta_i\cdot w_i)}{\sum_{k=1}^kw_i}$$
$\theta_i$ is the extracted effect-size for a study $i$. The $w_i$ is the weight per study $i$ for allk $k$ studies, derived from the precision $1/se_i^2$ via the equation below.
$$w_i = \frac{1}{se_i^2}$$
The standard error for the pooled effect-size can then be derived via the formula given below.

$$se(\theta_{pooled})=\frac{1}{\sqrt\sum_{i=1}^{k}(w_i)}$$
For a random-effect meta-analysis the variance between studies is separately modeled. In the metafor package REML or (Restricted Maximum Likelihood) is used to estimate this between study variance. However it is also possible using the DerSimonian and Laird method.
$$
\tau^2=max(0, \frac{Q-(k-1)}{\sum_{i=1}^{k}\frac{1}{w_i}-\frac{\sum_{i=1}^{k}1/w_i^2}{\sum_{i=1}^{k}1/w_i}})\
\\
w^*_i=\frac{1}{(\frac{1}{w_i}+\tau^2)}
\\
\theta_{pooled} = \frac{\sum_{i=1}^{k}(\theta_i\cdot w^*_i)}{\sum_{i=1}^{k}(w^*_i)}
\\
se(\theta_{pooled})=\frac{1}{\sqrt(\sum_{i=1}^{k}w^*_i)}
$$
If we now go back to how we analytically derived the posterior we can devise a function that can analytically perform a fixed effect meta-analysis with ease. I have placed this in a function called 'abmeta'. In in simple cases it approximates the results of metafor and the meta function inf EcoPostView relatively well. Of course the variance component slightly differs with that from metafor and the 'meta' function due to the different method of estimation.

## 6.6 BMA and meta-analysis

In a meta-analysis we do not talk about $\beta$ but about a set of estimates $\beta=\{\beta_{i}, ..., \beta_{n}\}$ meaning that $f(Meta-data\mid\{\beta_{i}, ..., \beta_{n}\})$. Hereby the flexibility allows that these estimates are either likelihood estimates ($\hat{\beta}$) or  posterior estimates ($\beta$). and we end up with an expression that should capture the inference to an underlying pooled model parameter.
$$
f(\beta_{poolded} \mid Meta-data,Info) = \frac{f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\}) \cdot f_k(\beta_{pooled} \mid Info)}{\int \left( \sum_{k=1}^{m} f(Meta-data \mid \{\beta_{i}, ..., \beta_{n}\}) \cdot f_k(\beta_{pooled} \mid Info) \right)}
$$

## 6.7 Sequential updating

Bayesian sequential updating refers to the practice of re-using the derived posterior of a previous model as the prior for the new model. For this the assumption of conditional independence between the the datasets is assumed. The parameter of interest is $\theta$ based on a dataset $Data_1$ and we derive the posterior. 
$$P(\theta|Data_1) = \frac{P(Data_1|\theta) \cdot P(\theta)}{P(Data_1)}$$
The next would be 
$$P(\theta|Data_1, Data_2) = \frac{P(Data_2|\theta) \cdot P(\theta|Data_1)}{P(Data_2)}$$ 
till $$P(\theta|Data_n) = \frac{P(Data_n|\theta) \cdot P(\theta|Data_1,\cdots,Data_{n-1})}{P(Data_n)}$$

For example, we would like to know what $\mu$ from a population of interest. Our example population has $\mu=0.5$, $\sigma=5$ and each study would have an error of $\alpha = 40\%$ when when we assume $\alpha=5\%$ (meaning that our heterogeneity is larger than expected). Our first prior starts with $N(0, 5)$ after which the posterior of previous is sequentially re-used visually represented in Fig. 2a below. Where more studies increase the precision of the estimated posterior. 

If the focus lies on objectivity and the error control over the different studies and assume iid then the curve between studies would follow that of Fig. 2b below. 

In a less formal way is the Bayesian framework more focused on transfer of information an precision. On the other hand the frequentist framework is more interested in objectivity, consistency and error among studies.

```{r sequential updating,  fig.width=7.1, fig.height=4, echo=F, warning=FALSE}
postvals <- function(mu_data, sigma_data, mu_prior, sigma_prior){
  
  post_mu <- (mu_prior / sigma_prior^2 + mu_data /sigma_data^2) / (1 / sigma_prior^2 + 1 / sigma_data^2)
  post_sigma <- sqrt(1/(1 / sigma_prior^2 + 1 / sigma_data^2))
  
  c(mu=post_mu, sigma=post_sigma)}

nsim <- 100
cint <- 0.05
errint <- 0.4
theta  <- 0.5
postset <- array(NA, dim=c(nsim, 5))
freqset <- array(NA, dim=c(nsim, 5))

z_adj <- abs(qnorm(errint/2))/abs(qnorm(cint/2))

set.seed(3)
for(i in 1:nrow(postset)){
xsamp <- rnorm(25, theta, 5)
if(i==1){
post  <- postvals(mean(xsamp), sd(xsamp)/sqrt(length(xsamp)), 0, 5)
}else{
post  <- postvals(mean(xsamp), sd(xsamp)/sqrt(length(xsamp)), post[1], post[2])}
postset[i,] <- c(i, post, post[1]-post[2]*abs(qnorm(cint/2))*z_adj, post[1]+post[2]*abs(qnorm(cint/2))*z_adj)

fse <- sd(xsamp)/sqrt(length(xsamp))
freqset[i,] <- c(i, mean(xsamp), fse,  mean(xsamp)-fse*abs(qnorm(cint/2))*z_adj,  mean(xsamp)+fse*abs(qnorm(cint/2))*z_adj)}

postset <- setNames(as.data.frame(postset), c("iter", "mu", "se", "ll", "ul"))

p1 <- ggplot(postset, aes(iter, mu))+
  geom_line()+ylab("µ")+xlab("Study")+
  geom_hline(yintercept = 0.5, col="tomato3", lty=2, lwd=0.2)+
  geom_ribbon(aes(ymin = ll, ymax = ul), fill = "grey70", alpha=0.2)+
  theme_classic()

freqset <- setNames(as.data.frame(freqset), c("iter", "mu", "se", "ll", "ul"))

p2 <- ggplot(freqset, aes(iter, mu))+
  geom_line()+ylab("")+xlab("Study")+
  geom_hline(yintercept = 0.5, col="tomato3", lty=2, lwd=0.2)+
  geom_ribbon(aes(ymin = ll, ymax = ul), fill = "grey70", alpha=0.2)+
  theme_classic()

sequpdate <- cowplot::plot_grid(p1, p2, ncol=2)
print(sequpdate)
rm(sequpdate)
```
*Figure 2: Sequential updating with credibility intervals on the left panel and a long-run of means with confidence intervals on the right The left panel.*


## 6.8 A short reflection on uncertainty

I do not believe statistics reflects uncertainty about events; rather, it reflects the information in the data under a particular model or the uncertainty about our belief in a parameter ($\theta, \beta, \mu$, etc.). The later concept is often vague and confusing because, if one assumes the parameter does not exist independently of the mind, then what exactly is uncertain - our belief? The claim to 'objective probability' is already compromised by the assumption that the parameter is objective. However, if the parameter does not exist outside the mind, the meaning of 'objective' in this context becomes questionable.

When people refer to objectivity, they often mean that the data itself is the most 'objective' part of the process. However, if some conditions are not met, such as (1) the data is not randomly sampled from a population of interest, (2) the model is not pre-selected in advance, (3) a sufficiently large sample size is not chosen based on the model, and (4) confounding variables are present, then even the data cannot be considered truly objective unless these limitations are explicitly acknowledged. Moreover, model selection procedures further contaminate the objectivity of the data, meaning that the estimated model parameters no longer fully reflect the objectivity of the data which is often implied in our conclusions (Gelman and Loken, 2013; Tong, 2019).

In Bayesian updating, the prior reflects the extent to which we want to sacrifice over the objectivity of the likelihood by using information which cannot be formalized into the likelihood. This is captured by the relationship $f(\theta \mid Data, Info) = f(Data \mid \theta) \propto f(\theta \mid Info)$

The posterior, therefore, is merely the weighted combination of the prior and likelihood. It represents the relationship (e.g., $0.25$ as $0.5 \cdot 0.5$) between the prior and the likelihood. There is no invalidity in a logical argument such as:(Premise 1.) All unicorns are orange. (Premise 2.) I have a unicorn. (Conclusion) Therefore, my unicorn is orange.

While this argument may be unsound — because unicorns do not exist — the reasoning itself is not flawed. The issue lies with the premises, not the structure of the argument. Hence, Uncertainty does not exist in the 'real' world; it resides solely in our minds. We cannot be 'wrong' or 'correct' about $f(\beta \mid \text{Data, Info})$ because it does not exist as a tangible entity. Even if it did, its existence would have no impact on reality because uncertainty is unrelated to the way reality operates. In the real world, events either occur or they do not. If my unicorn does not exist, I will never see it, and it was never orange in the first place.

We should also avoid treating models as a definitive representation of reality. Models are tools that convey information and serve as pragmatic instruments. The the model itself is not the result, the strength of the results relies on the argument, and how well the premises within the argument are clarified and supported by the model.

# References

Baguley, Thom. 2009. “Standardized or Simple Effect Size: What Should Be Reported?” British Journal of Psychology 100(3): 603–17. doi: 10.1348/000712608X377117.

Cramer, J. S. 1991. The Logit Model: An Introduction for Economists. London: Edward Arnold.

Csilléry, Katalin, Michael G.B. Blum, Oscar E. Gaggiotti, and Olivier François. 2010. “Approximate Bayesian Computation (ABC) in Practice.” Trends in Ecology & Evolution 25(7): 410–18. doi:10.1016/j.tree.2010.04.001.

Fisher, R. A. (1949). The Design of Experiments (5th ed.). Oliver and Boyd.

Gelman, Andrew, and Eric Loken. 2013. “The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‘FIshing Expedition’ or ‘p-Hacking’ and the Research Hypothesis Was Posited Ahead of Time.” 348(3): 1–17. doi: 10.1007/978-3-658-12153-2_7.

Hartig, Florian, Justin M. Calabrese, Björn Reineking, Thorsten Wiegand, and Andreas Huth. 2011. “Statistical Inference for Stochastic Simulation Models - Theory and Application: Inference for Stochastic Simulation Models.” Ecology Letters 14(8): 816–27. doi:10.1111/j.1461-0248.2011.01640.x.

Hinne, Max, Quentin F. Gronau, Don Van Den Bergh, and Eric-Jan Wagenmakers. 2020. “A Conceptual Introduction to Bayesian Model Averaging.” Advances in Methods and Practices in Psychological Science 3(2):200–215. doi: 10.1177/2515245919898657.

Hoeting, Jennifer A., David Madigan, Adrian E. Raftery, and Chris T. Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science 14(4):382–417. doi: 10.1214/ss/1009212519.

Kale, Alex, Francis Nguyen, Matthew Kay, and Jessica Hullman. 2019. “Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data.” IEEE Transactions on Visualization and Computer Graphics 25(1): 892–902. doi: 10.1109/TVCG.2018.2864909.

Maier, Maximilian, František Bartoš, and Eric-Jan Wagenmakers. 2023. “Robust Bayesian Meta-Analysis: Addressing Publication Bias with Model-Averaging.” Psychological Methods 28(1): 107–22. doi: 10.1037/met0000405.

Mayo, D. G. (2018). Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars. Cambridge University Press.

Moreno, Santiago G., Alex J. Sutton, Ae Ades, Tom D. Stanley, Keith R. Abrams, Jaime L. Peters, and Nicola J. Cooper. 2009. “Assessment of Regression-Based Methods to Adjust for Publication Bias through a Comprehensive Simulation Study.” BMC Medical Research Methodology 9(1):2. doi: 10.1186/1471-2288-9-2.

Pearl, J. (2009). Causality. Cambridge university press.

Peters, Jaime L., Alex J. Sutton, David R. Sones, Keith R. Abrams, and Lesley Rushton. 2006. “Comparison of Two Methods to Detect Publication Bias in Meta-Analysis.” JAMA 295(6):676. doi: 10.1001/jama.295.6.676.

Stanley, T. D., and Hristos Doucouliagos. 2014. “Meta-Regression Approximations to Reduce Publication Selection Bias.” Research Synthesis Methods 5(1):60–78. doi: 10.1002/jrsm.1095.

Tukey, John W. 1969. “Analysing Data: Sanctification or Detective Work?” American Psychologist 24(2): 83–91. doi: 10.1037/h0027108.

Tong, Christopher. 2019. “Statistical Inference Enables Bad Science; Statistical Thinking Enables Good Science.” The American Statistician 73(sup1): 246–61. doi: 10.1080/00031305.2018.1518264.

Van Zwet, E.W., Cator, E.A., 2021. "The significance filter, the winner’s curse and the need to shrink." Stat. Neerlandica 75, 437–452. doi: 10.1111/stan.12241

Woolridge, Jeffery M. 2001. Econometric Analysis of Cross Section and Panel Data. Cambridge, Massachusetts, London, England: The MIT press.


# an Ecological Posterior View<br />
This R package contains functions developed for my own work and personal interest. Considering the amount of time, please refer to the subsequent use article for publication. Most (if not all) functions use JAGS, which requires installation first (https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Windows/). The package focuses on visualizing the posterior and is largely tailored to how aquatic ecologists view "things". Any questions, positive feedback, or suggested improvements that benefit the functionality of the R package are welcome.

Currently, there seems to be a "trend" towards being either a dogmatic Bayesian or Frequentist. I completely distance myself from these perspectives, as the flaws of one are the benefits of the other, and vice versa. Such dogmatic stances are, in a larger sense, rather unfortunate and largely due to misinterpretations, nihilism, and dogmatism. It is perfectly reasonable to adopt the frequentist view, but one must use its language and understanding (I attempted to explain it somewhat here: https://snwikaij.shinyapps.io/shiny/). The frequentist cannot make probability statements about hypotheses, parameters, or models ("effects"). Choices of words such as rejecting, accepting, by chance, or the probability of θ falling within x% of the credibility bounds are Bayesian and are often referred to as abduction (or retroduction). Even Fisher was quite clear about this, but his words seem somewhat lost (https://github.com/snwikaij/Data/tree/main/Statistical_inference_and_induction).

That being said, the Bayesian attaches probabilities to hypotheses, parameters, and models via the prior. Then, the Bayesian can state that P(Model | Data, Info) or use Bayes Factor (BF) to compare which model most likely generated the data: BF10 = (P(Model1 | Data, Info) / P(Model0 | Data, Info)) / (P(Model1 | Info) / P(Model2 | Info)). In any case, there are rather useful and pragmatic implications for using Bayesian statistics in ecology. It can be used to add credibility to the outcomes of data based on prior knowledge.

Objective language, which I believe Sander Pierce terms induction, also relies on prior assumptions. However, these assumptions are not introduced into the model as the Bayesian does via the prior. The researcher must create a proper "random" sampling strategy aimed at obtaining samples that are (approximately) independent and identically distributed. When observing the data and the statistical likelihood P(Data | Model), it sees these as an infinite set of "likelihoods", meaning studies: x̄1 = {xi, ... xn} / n, x̄2 = {xi, ... xn} / n, ..., x̄∞ from an infinite set of random variables, the "true" mean: X̄ = {X, ..., X} / N where N is infinitely large.

In simplicity, 1/2 = 0.5, 1/3 = 0.33, 1/4 = 0.25, then 1/∞ = 0. This is important because if we assume that with more samples, we get closer to the "true" mean (X̄) we follow a Bayesian view. Assume we have a large vase with 100 glass marbles, 50 red (p) and 50 (q). Taking 90 samples would give us a substantial amount of "confirmation" about what p/q would be. But if the population is infinite, we cannot say that a large sample would give us more information on X̄. It does give us information indeed, but only about the data given a model as P(Data | Model) or the data relative to a fixed point P(Data > x | Model) (the latter the p-value). Hence, frequentist statistics have important considerations when one aims to stay objective, needing to apply infinitely many studies to conclude. Of course, approximately n(?) would suffice pragmatically to state something about our empirical error and how all these studies might serve as information against some null model (Model 0). Furthermore, it can be used as a clear signal-to-noise indication, incredibly useful in randomized trial studies and laboratory studies. If we do not know much (new drugs or otherwise) then this would be a viable framework to follow. Yet, the drawback is that a priori conditions need to be generated for the results to be interpreted with "confidence".

This readme is too short to address all these topics, and I will not delve further into depth either.

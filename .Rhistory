score_sentence_trigram <- function(sentence, model) {
# 1. Tokenize sentence into words
toks <- unlist(str_split(str_to_lower(sentence), "\\s+"))
# 2. If less than 3 words, return a small log-prob
if(length(toks) < 3) return(log(1e-6))
# 3. Create two-word histories (w1) and next word (w2)
w1 <- paste(toks[-c(length(toks)-1, length(toks))],
toks[-c(1, length(toks))], sep = " ")
w2 <- toks[-c(1,2)]
pairs <- tibble(w1 = w1, w2 = w2)
# 4. Join with the trigram model
pairs_join <- left_join(pairs, model, by = c("w1", "w2"))
# 5. Assign small probability for unseen trigrams
pairs_join$prob[is.na(pairs_join$prob)] <- 1e-6
# 6. Sum log probabilities
logprob <- sum(log(pairs_join$prob))
return(logprob)
}
# Classes
# Classes
classes <- names(trigrams_list)
# Sentences
sentences <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx", sheet = 2)
sentences_vec <- sentences$Sentence
# Score each sentence for each class
class_list <- list()
for(cls in classes) {
model <- trigrams_list[[cls]]
class_list[[cls]] <- sapply(sentences_vec, function(s) score_sentence_trigram(s, model))
}
# Combine into a dataframe
score_df <- data.frame(Sentence = sentences_vec)
for(cls in classes) {
score_df[[cls]] <- class_list[[cls]]
}
score_df
score_df$Predicted <- classes[apply(score_df[,-1], 1, which.max)]
score_df$true      <- sentences$Category
conf_mat <- table(score_df$true, score_df$Predicted)
caret::confusionMatrix(conf_mat)
conf_mat
df <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx")
split_df       <- split(df, df$Sentence)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
score_sentence <- function(sentence, model) {
toks <- unlist(str_split(str_to_lower(sentence), "\\s+"))
pairs <- tibble(w1 = toks[-length(toks)], w2 = toks[-1])
pairs_join <- left_join(pairs, model, by = c("w1","w2"))
pairs_join$prob[is.na(pairs_join$prob)] <- 1e-6
logprob <- sum(log(pairs_join$prob))
return(logprob)
}
classes        <- names(bigrams_list)
sentences      <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx", sheet = 2)
sentences_vec  <- sentences$Sentence  # extract the Sentence column
class_list <- list()
for (cls in classes) {
model <- bigrams_list[[cls]]
class_list[[cls]] <- sapply(sentences_vec, function(s) score_sentence(s, model))
}
# Combine into a dataframe
score_df <- data.frame(Sentence = sentences_vec)
for (cls in classes) {
score_df[[cls]] <- class_list[[cls]]
}
score_df
sentences      <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx", sheet = 1)
sentences_vec  <- sentences$Sentence  # extract the Sentence column
class_list <- list()
for (cls in classes) {
model <- bigrams_list[[cls]]
class_list[[cls]] <- sapply(sentences_vec, function(s) score_sentence(s, model))
}
df <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx")
ngram_train <- function(y, x, data, test)
split_df       <- split(df, df$Sentence)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
split(df, df$Sentence)
split_df       <- split(df, df$Sentence)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
score_sentence <- function(sentence, model) {
toks <- unlist(str_split(str_to_lower(sentence), "\\s+"))
pairs <- tibble(w1 = toks[-length(toks)], w2 = toks[-1])
pairs_join <- left_join(pairs, model, by = c("w1","w2"))
pairs_join$prob[is.na(pairs_join$prob)] <- 1e-6
logprob <- sum(log(pairs_join$prob))
return(logprob)
}
classes        <- names(bigrams_list)
sentences      <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx", sheet = 1)
sentences_vec  <- sentences$Sentence  # extract the Sentence column
class_list <- list()
for (cls in classes) {
model <- bigrams_list[[cls]]
class_list[[cls]] <- sapply(sentences_vec, function(s) score_sentence(s, model))
}
sentences      <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx", sheet = 2)
sentences_vec  <- sentences$Sentence  # extract the Sentence column
class_list <- list()
for (cls in classes) {
model <- bigrams_list[[cls]]
class_list[[cls]] <- sapply(sentences_vec, function(s) score_sentence(s, model))
}
# Combine into a dataframe
score_df <- data.frame(Sentence = sentences_vec)
for (cls in classes) {
score_df[[cls]] <- class_list[[cls]]
}
score_df
score_df$Predicted <- classes[apply(score_df[,-1], 1, which.max)]
score_df$true      <- sentences$Category
conf_mat <- table(score_df$true, score_df$Predicted)
caret::confusionMatrix(conf_mat)
conf_mat
conf_mat <- table(score_df$true, score_df$Predicted)
conf_mat
gc()
gc()
df <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx")
ngram_train <- function(y, x, data, test)
split_df       <- split(df, df$Category)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
score_sentence <- function(sentence, model) {
toks <- unlist(str_split(str_to_lower(sentence), "\\s+"))
pairs <- tibble(w1 = toks[-length(toks)], w2 = toks[-1])
pairs_join <- left_join(pairs, model, by = c("w1","w2"))
pairs_join$prob[is.na(pairs_join$prob)] <- 1e-6
logprob <- sum(log(pairs_join$prob))
return(logprob)
}
classes        <- names(bigrams_list)
split_df       <- split(df, df$Category)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
score_sentence <- function(sentence, model) {
toks <- unlist(str_split(str_to_lower(sentence), "\\s+"))
pairs <- tibble(w1 = toks[-length(toks)], w2 = toks[-1])
pairs_join <- left_join(pairs, model, by = c("w1","w2"))
pairs_join$prob[is.na(pairs_join$prob)] <- 1e-6
logprob <- sum(log(pairs_join$prob))
return(logprob)
}
classes        <- names(bigrams_list)
sentences      <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx", sheet = 2)
sentences_vec  <- sentences$Sentence  # extract the Sentence column
class_list <- list()
for (cls in classes) {
model <- bigrams_list[[cls]]
class_list[[cls]] <- sapply(sentences_vec, function(s) score_sentence(s, model))
}
# Combine into a dataframe
score_df <- data.frame(Sentence = sentences_vec)
for (cls in classes) {
score_df[[cls]] <- class_list[[cls]]
}
score_df
score_df$Predicted <- classes[apply(score_df[,-1], 1, which.max)]
score_df$true      <- sentences$Category
conf_mat <- table(score_df$true, score_df$Predicted)
caret::confusionMatrix(conf_mat)
conf_mat
caret::confusionMatrix(conf_mat)
sentences1 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test2.pdf")
sentences2 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test.pdf")
pre_clean <- function(file_path) {
# Read PDF and collapse pages
text <- pdf_text(file_path)
all_text <- paste(text, collapse = " ")
# Fix hyphenation at line breaks
all_text <- gsub("([a-zA-Z])-\\s+([a-zA-Z])", "\\1\\2", all_text)
# Split sentences
sentences <- unlist(tokenize_sentences(all_text))
sentences <- trimws(sentences)
sentences <- sentences[sentences != ""]
# Lowercase, remove extra whitespace, line breaks, short sentences
sentences <- tolower(sentences)
sentences <- gsub("\\s+", " ", sentences)
sentences <- gsub("[\r\n\t]", " ", sentences)
sentences <- trimws(sentences)
sentences <- sentences[nchar(sentences) > 10]
# Remove junk sentences
sentences <- sentences[!grepl("doi|http|www|volume|fig\\.|table|supplementary|copyright", sentences)]
return(sentences)
}
sentences1 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test2.pdf")
sentences2 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test.pdf")
pre_clean <- function(file_path) {
# Read PDF and collapse pages
text <- pdf_text(file_path)
all_text <- paste(text, collapse = " ")
# Fix hyphenation at line breaks
all_text <- gsub("([a-zA-Z])-\\s+([a-zA-Z])", "\\1\\2", all_text)
# Split sentences
sentences <- unlist(tokenize_sentences(all_text))
sentences <- trimws(sentences)
sentences <- sentences[sentences != ""]
# Lowercase, remove extra whitespace, line breaks, short sentences
sentences <- tolower(sentences)
sentences <- gsub("\\s+", " ", sentences)
sentences <- gsub("[\r\n\t]", " ", sentences)
sentences <- trimws(sentences)
sentences <- sentences[nchar(sentences) > 10]
# Remove junk sentences
sentences <- sentences[!grepl("doi|http|www|volume|fig\\.|table|supplementary|copyright", sentences)]
return(sentences)
}
sentences1 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test2.pdf")
sentences2 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test.pdf")
df_drop1 <- data.frame(y = "drop", x = sentences1[259:length(sentences1)], stringsAsFactors = FALSE)
df_keep1 <- data.frame(y = "keep", x = sentences1[5:258], stringsAsFactors = FALSE)
df_drop2 <- data.frame(y = "drop", x = sentences2[55:length(sentences2)], stringsAsFactors = FALSE)
df_keep2 <- data.frame(y = "keep", x = sentences2[1:54], stringsAsFactors = FALSE)
View(df_keep1)
df_all <- rbind(df_drop1, df_keep1, df_drop2, df_keep2)
View(df_all)
corpus <- VCorpus(VectorSource(df_all$x))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))  # optional
dtm <- DocumentTermMatrix(corpus)
dtm_bin <- as.data.frame(as.matrix(dtm))
dtm_bin[dtm_bin > 0] <- 1   # convert counts to binary presence/absence
train_data <- data.frame(y = as.factor(df_all$y), dtm_bin)
View(train_data)
View(df_drop1)
# --- 5. Train random forest ---
set.seed(123)
rf_model <- randomForest(y ~ ., data = train_data, ntree = 50)
# --- 6. Inspect model ---
print(rf_model)
sentences1 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test2.pdf")
sentences2 <- pre_clean("C:/Users/admin/OneDrive/Bureaublad/test.pdf")
sentences1
df1 <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx")
ngram_train <- function(y, x, data, test)
split_df       <- split(df1, df$Category)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
score_sentence <- function(sentence, model) {
toks <- unlist(str_split(str_to_lower(sentence), "\\s+"))
pairs <- tibble(w1 = toks[-length(toks)], w2 = toks[-1])
pairs_join <- left_join(pairs, model, by = c("w1","w2"))
pairs_join$prob[is.na(pairs_join$prob)] <- 1e-6
logprob <- sum(log(pairs_join$prob))
return(logprob)
}
split_df       <- split(df1, df$Category)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
split_df       <- split(df1, df1$Category)
bigrams_list   <- lapply(split_df, function(df) {
# 1. Tokenize into bigrams
bigrams <- unnest_tokens(
df,
output = "bigram",
input = "Sentence",
token = "ngrams",
n = 2
)
# 2. Count bigrams
bigram_counts <- count(bigrams, bigram)
bigram_counts <- bigram_counts[!is.na(bigram_counts$bigram), ]
# 3. Separate into w1 and w2
bigrams_sep <- separate(bigram_counts, bigram, c("w1", "w2"), sep = " ")
# 4. Compute group stats
group_stats <- summarise(
group_by(bigrams_sep, w1),
total_w1 = sum(n),
total_comb_w2 = n_distinct(w2),
.groups = "drop"
)
# 5. Join stats and compute probabilities
bigrams_final <- left_join(bigrams_sep, group_stats, by = "w1")
bigrams_final$prob <- (bigrams_final$n + 1) / (bigrams_final$total_w1 + bigrams_final$total_comb_w2)
# 6. Return the final bigram table
return(bigrams_final)
})
score_sentence <- function(sentence, model) {
toks <- unlist(str_split(str_to_lower(sentence), "\\s+"))
pairs <- tibble(w1 = toks[-length(toks)], w2 = toks[-1])
pairs_join <- left_join(pairs, model, by = c("w1","w2"))
pairs_join$prob[is.na(pairs_join$prob)] <- 1e-6
logprob <- sum(log(pairs_join$prob))
return(logprob)
}
classes        <- names(bigrams_list)
sentences      <- read_excel("C:/Users/admin/OneDrive/Bureaublad/Language.xlsx", sheet = 2)
pre_clean("C:/Users/admin/Downloads/science.abg1780.pdf")
pre_cleaned_data <- pre_clean("C:/Users/admin/Downloads/science.abg1780.pdf")
sentences_vec <- pre_clean("C:/Users/admin/Downloads/science.abg1780.pdf")
sentences_vec <- pre_clean("C:/Users/admin/Downloads/science.abg1780.pdf")
corpus <- VCorpus(VectorSource(sentences_vec))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))  # optional
dtm_pred <- DocumentTermMatrix(corpus)
dtm_bin_pred <- as.data.frame(as.matrix(dtm_pred))
dtm_bin_pred[dtm_bin_pred > 0] <- 1   # convert counts to binary presence/absence
pred_data <- data.frame(y = as.factor(sentences_vec), dtm_bin_pred)
pred_data <- data.frame(dtm_bin_pred)
View(pred_data)
predict(rf_model, pred_data)
colnames(dtm_bin) %in% colnames(dtm_bin_pred)
colnames(dtm_bin)[colnames(dtm_bin) %in% colnames(dtm_bin_pred)]
colnames(dtm_bin)[!colnames(dtm_bin) %in% colnames(dtm_bin_pred)]
colnames(dtm_bin)[colnames(dtm_bin) %in% colnames(dtm_bin_pred)]
dtm_bin_pred[colnames(dtm_bin)[colnames(dtm_bin) %in% colnames(dtm_bin_pred)],]
pred_data <- dtm_bin_pred[colnames(dtm_bin)[colnames(dtm_bin) %in% colnames(dtm_bin_pred)],]
predict(rf_model, pred_data)
dtm_pred <- DocumentTermMatrix(corpus)
dtm_bin_pred <- as.data.frame(as.matrix(dtm_pred))
dtm_bin_pred[dtm_bin_pred > 0] <- 1   # convert counts to binary presence/absence
sentences_vec <- pre_clean("C:/Users/admin/Downloads/science.abg1780.pdf")
corpus <- VCorpus(VectorSource(sentences_vec))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))  # optional
dtm_pred <- DocumentTermMatrix(corpus)
dtm_bin_pred <- as.data.frame(as.matrix(dtm_pred))
dtm_bin_pred[dtm_bin_pred > 0] <- 1   # convert counts to binary presence/absence
pred_data <- dtm_bin_pred[colnames(dtm_bin)[colnames(dtm_bin) %in% colnames(dtm_bin_pred)],]
!colnames(pred_data) %in% colnames(dtm_bin_pred)
pred_data[!colnames(pred_data) %in% colnames(dtm_bin_pred)]
colnames(pred_data)[!colnames(pred_data) %in% colnames(dtm_bin_pred)]
colnames(pred_data)[!colnames(dtm_bin_pred) %in% colnames(pred_data)]
citation()
citation("ggplot2")
